[ { "title": "右值引用", "url": "/posts/right/", "categories": "notes", "tags": "C++", "date": "2023-03-24 00:00:00 +0800", "snippet": "按 C++ 标准的发展来梳理出右值引用的概念。但会先复习一下引用的概念。引用先复习一下 C++ 中引用的概念： A reference defines an alternative name for an object 引用就是另外一个已经存在的对象的名字的别名 int ival = 1024;// refVal refers to (is another name for) ivalint &amp;amp;refVal = ival;// 错: a reference must be initializedint &amp;amp;refVal2;// assigns 2 to the object to which refVal refers, i.e., to ivalrefVal = 2;// same as ii = ivalint ii = refVal;// refVal3 实际上引用的是 ival（refVal 所指向的对象）int &amp;amp;refVal3 = refVal;// 实际上把 refVal 所指向的对象 ival 的值赋值给 iint i = refVal;引用在其初始化的时候被绑定到它对应的「初始对象」上；而且引用一旦被绑定就不能再被绑定到其他对象上得到 3 个推论： 把某个值赋值给引用，实际上是赋值给引用指向的真正的对象 当从引用获取一个值的时候，实际上获取的的引用指向的真正的对象 当使用引用来初始化时，实际上是用引用指向的真正的对象来做初始化const 和 引用const int ci = 1024;// ok: both reference and underlying object are const const int &amp;amp;r1 = ci;// r1 是 ci 的别名，所以 ci 是 const 的话，也不能通过 r1 修改其值// 错: r1 is a reference to constr1 = 42;// 企图用非 const 的引用来修改 const 的对象也是不行的// 错: non const reference to a const objectint &amp;amp;r2 = ci;// 但反过来是可以的// we can bind a const int&amp;amp; to a plain object// 不过这样做的话，虽然不能通过 r1 来修改对象；但是目标对象 i 本身还是可以修改的int i = 42;const int &amp;amp;r1 = i;// const &amp;amp; 可以引用一个 rvalue（重要）const int &amp;amp;r3 = i * 42;引用作为函数参数void f(int a) { a = a + 5; std::cout &amp;lt;&amp;lt; a &amp;lt;&amp;lt; std::endl;}int b = 5;// 不用引用作为参数的话，是按值传参。相当于做了一次 int a = bf(a);void fr(int &amp;amp;c) { c = c + 5; std::cout &amp;lt;&amp;lt; c &amp;lt;&amp;lt; std::endl;}// 使用引用作为参数的话，实现了按参数传参的效果。相当于做了一次 int &amp;amp;c = d// 在函数内部，引用 c 指向的是对象 d，修改 c 就是修改 dint d = 6;fr(d);再看一个 const &amp;amp; 作为参数的例子：// compare the length of two strings// 这样做的话，不会把值 copy 到函数内部，提升效率；同时又不会修改入参指向的对象的值（const &amp;amp;）bool isShorter(const string &amp;amp;s1, const string &amp;amp;s2) { return s1.size() &amp;lt; s2.size();}引用作为返回值返回值是引用，就是说当函数的调用者得到这个「返回值」以后，就拿到了指向某个「对象」的引用。这样的话： 既然拿到的是引用，在返回时没有发生「值」的 copy，效率高 另外，如果这个引用不是 const 的话，函数的调用者，可以通过这个引用修改该引用指向的「对象」例如：和 C 语言不同（C 语言的函数调用不能是「左值」），如果返回值是引用的话，该函数的调用可以是「左值」。参考下面例子：char &amp;amp;get_val(std::string &amp;amp;str, std::string::size_type ix) { // get_val assumes the given index is valid return str[ix]; }int main(void) { std::string t(&quot;a value&quot;); std::cout &amp;lt;&amp;lt; t &amp;lt;&amp;lt; std::endl; // changes t[0] to A get_val(t, 0) = &#39;A&#39;; std::cout &amp;lt;&amp;lt; t &amp;lt;&amp;lt; std::endl; return 0;}但注意不要返回临时变量的引用：// 问题: 返回一个指向局部对象的引用Matrix&amp;amp; add( Matrix &amp;amp;m1, Matrix &amp;amp;m2 ) { Matrix result; if ( m1.isZero() ) return m2; if ( m2.isZero() ) return m1; // 将两个 Matrix 对象的内容相加 // 喔! 返回之后 结果指向一个有问题的位置 return result;}操作符重载函数赋值号的「操作符重载函数」必须是成员函数： 赋值号的左边是当前对象（this 参数指向的对象） 赋值号的右边是「操作符重载函数」的入参 「操作符重载函数」的返回值一般来说都是赋值号左边对象的引用class Foo {public: // assignment operator // 1) 入参使用 const Foo&amp;amp; 不会改变赋值号右边的对象 // 2) Assignment operators ordinarily should return a reference to their left-hand operand. // 这是因为当 a = b = c 这种连续赋值操作的时候返回值是「左边对象的引用」效率高一点。 // 但只是 a = b 这种赋值操作时，无所谓是「左边对象的引用」还是直接返回「对象本身」 Foo&amp;amp; operator=(const Foo&amp;amp; v);};下面是一个调用的例子：Foo f;Foo g;// 这里的赋值会调用「赋值操作符重载函数」。// 注意：就这个赋值操作来说，并不会使用到「赋值操作符重载函数」的返回值（虽然一般建议「赋值操作符重载函数」的返回值是一个「左边对象的引用」）g = f;一个例子：// equivalent to the synthesized copy-assignment operatorSales_data&amp;amp; Sales_data::operator=(const Sales_data &amp;amp;rhs) { bookNo = rhs.bookNo; // calls the string::operator= units_sold = rhs.units_sold; // uses the built-in int assignment revenue = rhs.revenue; // uses the built-in double assignment return *this; // return a reference to this object}最后，先简单提一下左右值（这里先不考虑后来引入的 xvalue 的概念）： Every expression in C++ is either an rvalue (pronounced “are-value”) or an lvalue (pronounced “ell-value”)。可以粗略的理解成，当使用一个对象的值的时候，是把这个对象当作 rvalue 来使用；而当把一个对象用作一个 identity （可以根据 identity 来按在内存的位置来使用对象）的时候，是把这个对象当作 lvalue 来使用： lvalue：An expression that yields an object or function. A non const lvalue that denotes an object may be the left-hand operand of assignment. lvalue 多被用于持有某种状态 rvalue：Expression that yields a value but not the associated location, if any, of that value. rvalue 一般就是「字面量」或者「临时对象」 就引用来说，在 C++ 11 之前，引用只可以引用 lvalue，否则必须是 const &amp;amp; 才能引用 rvalue：int i = 42;// 引用一个 lvalueint &amp;amp;r = i;// const &amp;amp; 可以引用一个 rvalue（重要）const int &amp;amp;r3 = i * 42;C++98C++98 为了解决「临时对象拷贝」效率的问题，对「const 引用」指向的临时对象的析构时机做了规定（临时对象的析构可以延迟）。参考下面的例子：void f(std::vector&amp;lt;int&amp;gt; v) { for (int n: v) { std::cout &amp;lt;&amp;lt; n &amp;lt;&amp;lt; endl; }}int main(void) { std::vector&amp;lt;int&amp;gt; v{1, 2, 3}; // 这里的函数 f 的入参是一个「临时对象」， // 然后 f 会对该「临时对象」做 copy，copy 之后，原先的「临时对象」会被析构 // 「临时对象」被 copy 到函数内部的变量 v 上，然后当函数执行结束时，函数内部的变量 v 又会被析构掉 f(std::vector&amp;lt;int&amp;gt; {4, 5, 6};}在上面的例子中，当调用函数 f 时，会发生 2 次对 vector 的构造，2 次对 vector 对象的析构。既然按之前提到的 const &amp;amp; 可以引用到普通的「非 const 左值」，也可以引用到右值（所谓的「万能引用」）：// we can bind a const int&amp;amp; to a plain object// 不过这样做的话，虽然不能通过 r1 来修改对象；但是目标对象 i 本身还是可以修改的int i = 42;const int &amp;amp;r1 = i;// const &amp;amp; 可以引用一个 rvalueconst int &amp;amp;r3 = i * 42;为了解决这个效率问题，C++98 对「const 引用」（也就是「万能引用」）指向的临时对象的析构时机做了规定：// 函数 f 的入参是一个 const &amp;amp;void f(std::vector&amp;lt;int&amp;gt; const &amp;amp;v) { for (int n: v) { std::cout &amp;lt;&amp;lt; n &amp;lt;&amp;lt; endl; }}int main(void) { std::vector&amp;lt;int&amp;gt; v{1, 2, 3}; // const &amp;amp; 作为函数入参，可以是一个「非临时对象」 f(v); // const &amp;amp; 作为函数入参，也可以是一个「临时对象」。 // 把「临时对象」作为地址传入给函数 f // 然后由「编译器」来保证对 const &amp;amp;，当 f 结束以后（分号以后），才对「临时对象」做析构。而在函数 f 调用过程中，「临时对象」一直有效 f(std::vector&amp;lt;int&amp;gt; {4, 5, 6};}同时，为了解决「临时对象返回值」失效的问题，C++98 又规定了「临时对象返回值」会到该对象的作用域结束时才会对这个对象做析构。参考下面的例子：struct S{ ~S() {std::cout &amp;lt;&amp;lt; &quot;destruct it&quot; &amp;lt;&amp;lt; std::endl;}}S f(){ S s; return s;}int main(void){ { // 针对函数 f 的返回值使用 const&amp;amp; 可以解决「临时对象返回值」的效率问题 // 这样函数 f 内部返回的「临时对象」可以在返回以后继续使用 // 但注意，C++98 规定这里的 s（const &amp;amp;）不会在这个「分号」后被析构 // 而是会到 s 的作用域结束后才会被析构 const S &amp;amp;s = f(); std::cout &amp;lt;&amp;lt; &quot;inner scope&quot; &amp;lt;&amp;lt; std::endl; } std::cout &amp;lt;&amp;lt; &quot;outer scope&quot; &amp;lt;&amp;lt; std::endl;}对 C++98 左右值的总结： 通过对「右值」对象做 const &amp;amp;，使得「右值临时对象」可以被延迟析构，从而可以复用这个已经构建好的「临时对象」，提升效率C++11C++98 虽然通过规定 const &amp;amp; 可以延迟对「右值」临时对象的延迟析构来提升效率，但最后还是会被析构。考虑另外一种可以提升效率的场景：既然这个临时对象已经构造好了，拥有了一些分配好的资源；如果开发者希望该对象被析构以后，能够继续复用这个对象已经拥有的分配好的资源，从而提升效率？所以 C++11 更进一步，引入了「移动」相关的概念，提供给开发者来解决这个问题。右值引用在 C++ 11 之前，引用只可以引用 lvalue，否则必须是 const &amp;amp; 才能引用 rvalue：int i = 42;// 引用一个 lvalueint &amp;amp;r = i;// const &amp;amp; 可以引用一个 rvalueconst int &amp;amp;r3 = i * 42;C++ 11 引入了一种新的引用：右值引用。把右值引用绑定到某个对象，就说明这个对象是一个「临时对象」，其含义是： 右值引用指向的对象是「临时的」，也就是随后就会被析构 不存在其他的方式来使用这个「临时对象」 右值引用不能被绑定到 lvalue// 报错：右值引用不能被绑定到 lvalueint &amp;amp;&amp;amp;rr = i;所以，右值引用其实就是一种标志：被右值引用指向的对象可以被移动出来复用，避免不必要的拷贝，提升效率。移动赋值C++ 11 之前「重载赋值操作符」的话，是「拷贝赋值」，也就是会把「赋值号」右边的「对象」拷贝给「赋值号」左边的对象。在有的场景，这种拷贝其实是一种浪费：「赋值号」右边的「对象」其实未来不再会被使用了，实际上赋值的时候可以采用移动的方式，把未来不会再被使用的「对象」的资源移动给「赋值号」左边的「对象」。先看一个例子：struct S{ // 重载赋值操作符 S &amp;amp;operator=(const S &amp;amp;s) { std::cout &amp;lt;&amp;lt; &quot;const &amp;amp;&quot; &amp;lt;&amp;lt; std::endl; // 实现赋值逻辑（略） return *this; }}int main(void){ S s; S ss; // 下面 2 个赋值都会使用重载的赋值操作，同时都会使用 const &amp;amp; 作为入参（C++98） ss = s; // 1）先是 S() 为返回值构造一个临时对象 // 2）然后会匹配到参数为 const S &amp;amp; 的「赋值操作符重载函数」（const &amp;amp; 是「万能引用」），入参直接就是 S() 中的临时对象（因为按 C++ 98 的规定，因为是 const S &amp;amp;，返回的时候不会析构这个临时对象） // 3）「赋值操作符重载函数」之后，才会析构 S() 中构造出来的临时对象 ss = S();}上面这个例子中，按 C++98 的规定，当「赋值操作符重载函数」完成之后，S() 中构造出来的临时对象会被析构掉。但如果开发人员为了提升效率，希望这个临时对象被析构以后，其拥有的资源可以继续被复用（比如可以通过 ss 这个 identifier 继续使用）。就需要用到 C++11 右值引用相关规定了：struct S{ // 重载赋值操作符 S &amp;amp;operator=(const S &amp;amp;s) { std::cout &amp;lt;&amp;lt; &quot;const &amp;amp;&quot; &amp;lt;&amp;lt; std::endl; // 实现赋值逻辑（略） return *this; } // C++11 标准规定了「右值引用」：&amp;amp;&amp;amp;；标明这个赋值操作「右边」的对象是一个可以被复用的「临时对象」，把直接把该对象的资源移动给赋值操作「左右」的对象，从而提升效率。 S &amp;amp;operator=(S &amp;amp;&amp;amp;s) { std::cout &amp;lt;&amp;lt; &quot;&amp;amp;&amp;amp;&quot; &amp;lt;&amp;lt; std::endl; if (this != &amp;amp;s) { free(); // free existing elements = s.elements; // take over resources elements froms s s.elements = nullptr; } return *this; }}int main(void){ S ss; // C++11 的编译器能判断出 S() 是一个未来不会被使用的「临时对象」，所以会优先使用 &amp;amp;&amp;amp; 为入参的赋值操作，而开发人员已经把这个赋值操作实现成了移动资源，而不是拷贝资源，从而实现了效率的提升 ss = S();}总结 「开发者」负责实现多个版本的函数，分别满足对资源的「拷贝」和「移动」 「拷贝」版本的入参是：普通的「引用」、或「const 引用」 「移动」版本的入参是「右值引用」 如果「开发者」没有提供「右值引用」的版本，只提供了「const 引用」的版本，「编译器」多半会匹配到「const 引用」的版本（万能引用） 如果「开发者」提供了「右值引用」的版本，那么「编译器」能够根据当前入参是左值还是右值，判断出当前需要使用那个版本的函数： 如果入参是「左值」，就会匹配「拷贝」版本的函数 如果入参是「右值」，就会匹配到「移动」版本的函数 另外，如果当前值是「左值」，「编译器」就不会匹配到入参是「右值引用」的「移动」版本。但如果此时「开发者」希望能匹配到入参是「右值引用」的「移动」版本，可以通过使用 std::move 显示的指定调用「右值引用」的版本：S s;S ss;// 略// 通过使用 std::move 显示的指定调用「右值引用」的版本ss = std::move(s);附C++ 新标准中也提出了 xvalue 的概念。这种新的 value 的分类，个人理解其实是为了迎合前面章节总结出的 move 概念，而抽象出来定义。所以放到附录里面，看看就好了，和前面章节的内容并不冲突：For all the values, there were only two independent properties: “has identity” – i.e. an address, a pointer, the user can determine whether two copies are identical, etc. (defined as glvalue) “can be moved from” – i.e. we are allowed to leave to source of a “copy” in some indeterminate, but valid state. (defined as rvalue)There are four possible composition: iM: has identity and cannot be moved from (defined as lvalue) im: has identity and can be moved from (defined as xvalue) Im: does not have identity and can be moved from (defined as prvalue) IM: doesn’t have identity and cannot be moved (C++ 中目前没有这种) expression / \\ glvalue rvalue / \\ / \\lvalue xvalue prvalue prvalue，纯右值。典型的 prvalue 有：纯数学值（整数 7，单个字符 a），、引用（&amp;amp;a）、有返回值的函数调用（T f()） xvalue，临时对象。即 expiring value（即将消亡的 value），也就是前面章节提到的可以被 move 的 value。xvalue 从哪里来的？比如： prvalue 在某些语句中会被实体化（例如前面章节中的例子：S s = func()），从而创建出一个「临时对象」，也就产出了一个 xvalue " }, { "title": "chatbot", "url": "/posts/chatbot/", "categories": "notes", "tags": "chatbot", "date": "2022-12-07 00:00:00 +0800", "snippet": "对话型应用对话型应用也既是 chatbot 的中文翻译。首先，对话型应用是智能应用，是当今最热门的「机器学习」（AI） 技术的典型应用。所以，chatbot（对话型应用）是「智能应用」。何谓「智能应用」？如下图所示，通过在训练阶段获得的模型来处理真实的数据：flowchart LRA[[Training data]] --&amp;gt;B(ML argorithm) --&amp;gt;C[[Model]]D[[Real data]] --&amp;gt;E(Model)--&amp;gt;F[[Result]]具体来说是一种基于 SL 的「智能应用」： SL（Supervised learning）：在一组数据集（输入数据+输出数据，也就是训练数据）之上建立的数学模型。之后可以使用建立的「数学模型」对新输入的数据产出结果数据 例如：「分类模型」（垃圾邮件识别就是其典型应用） 对话型应用和机器学习既然对话型应用是「智能应用」，那么先介绍一下「机器学习」和 chatbot 相关的内容。对话型应用主要使用的是「机器学习」子方向 NLP 方面的模型。 具体可以参考 NLP 相关内容。实施「机器学习」的过程分 3 个步骤：training、inference、evaluation。整个实施过程就是模型不断调优，不断迭代；所以工程上要提供全链路的支持。如何评估「分类」算法？主要是体现在一些概念上： ACC（Accuracy）：算法得到正确结果总数 / 数据集总数；也就是：（TP+TN）/（数据集总数） Recall：算法得到的正确的正例 / 数据集中应该的正例；也就是 TP / TP + FN Precision：算法得到的正确的正例 / 算法算出来的所有正例；也就是 TP / TP + FP F1 score：Recall 和 Precision 的「调和平均」；综合 recall 和 precision 对算法评价 overfitting：算法过于简单、选用的网络架构不适用于当前应用等…… underfitting：泛化（Generalization）能力不足最后，再提一下在对话型应用中比较重要的 Transfer learning。为什么重要： TL 可以用相对来说不大的训练数据得到好的训练效果 TL 的速度也比较快在正式介绍 chatbot 更多细节之前，先用一个小节简要介绍一下 NLP 的相关内容。NLPNLP 是「机器学习」（ML，或 AI）的一个子方向，重点关注计算机和人的互动。NLP 技术的发展分为 2 个阶段：传统方法和现代方法。传统的 NLP 方法面临 2 大难于解决的问题： how we represent textual information during the computing process how we can build models for text第一个问题，传统的 NLP 使用的都是比较初级的方法：one-hot encoding 表示 word 和 phrase；用 BoW（bag of words）表示句子和段落。第二个问题，传统建模方法普遍重度依赖人工来构造特征： TF-IDF：用于表示 word 的重要性 topic modeling：用于获取文档主题的相关信息 也需要人工把很多语言学相关的信息构造成特征在用上面的方法得到各种特征之后，传统的方法用传统的「机器学习」算法来建立模型；例如得到以下的分类模型：SGD、NN、SVC、RF、AdaBoost……传统的方法优点是训练起来比较快，不要求要有大数据集。缺点是复杂的场景效果不好，而且需要花大量的时间在人工构造特征和模型调参上。而现代方法能比较好的解决这 2 个问题。现代的 NLP 方法： word2vec（CBOW）解决了第一个问题，第一步就是把 words 先转化为 words embeddings。 在解决了第一个问题之后，第二个问题也有了标准的解法：DNNs 模型（把 DL 应用到 NLP 领域，替代了传统的 ML 方法）word2vec 基础上的改进（基于当前的上下文来提供 words embeddings）： LSTM（RNN） GPT/BERT（Transform，attention mechanisms）总结：实际场景中，有多种方法可供候选。对话型应用按功能分类从功能上，对话型应用可以归为 3 种类型，或者是这 3 种类型的混合体： 任务型 问答型 闲聊型需要在做需求调研时收集多的用例来印证用户的需求属于哪一种。但问题在于，不同的「业务方」对于对话型应用的认知并不一致，「业务方」期望通过对话型应用获得什么价值也并不一致（甚至有可能「业务方」自己也不能精确描述出自己想要什么；甚至有可能「业务方」的需求根本不需要用对话型应用就能解决）。所以，所有工作的前提要在产品上和「业务方」进行多轮沟通，之后才有可能获得正确的答案。这里我们重点介绍「任务型」；同时也会附带一些「问答型」相关的内容；但基本不会介绍「闲聊型」。对话型应用架构chatbot 应用在架构上一般由 NLU、DM、NLG 3 大部分组成： NLU：解释用户输入 DM：对话状态管理 NLG：对话响应生成接下来重点介绍这 3 大部分。NLUNLU 模块在接收到用户输入的内容（文本）后，要完成 2 大任务： 意图识别 fill slot（填槽）从 NLP 的角度来看，NLU 模块的「意图识别」（或「意图分类」）是典型的「文本分类」任务；而填槽是典型的 ER（实体识别）任务。NLU 模块一般工作步骤为（现在只是简介，后面还会细说）： 预处理。例如：分句，token 化，分词，coreference resolution 等等…… feature engineering。例如传统的 number_of_tokens, symbols_ in_between, and bag_of_words_in_between 「分类」和 ER。可以使用传统的 ML 方法和现代的 DL 方法： 使用传统方法的话。传统的 ML 分类方法有：linear classification，support-vector machines（SVMs）。而传统的 ER 方法有：sequential labeling models（例如 hidden Markov model (HMM) 和 CRF） 使用现代 DL 方法的话。「分类」和 ER 都可以 use word2vec to do UL on a large corpus to embed hidden features of words into word embeddings and input them into DNN models such as convolutional neural networks (CNNs) or RNNs 总之，既可以用模型也可以用规则来做 NLU。模型的好处是能获得更高的 recall，从而能覆盖到更多样的用户输入；规则的好处是利用精度更高规则生成的特征可以训练出更好的模型。先给一个架构图：![[屏幕快照 2022-11-13 20.24.10.png]]具体细节如下： 「意图分类」的方法主要有：TextCNN、TextRNN、Transformer（以及这些方法的变种） ER 任务的方法主要有：CRF、BiLSTM-CRF、Transformer（以及这些方法的变种）NLU 是 pipeline 结构，流水线由几个组件组成的： 语言模型组件 分词组件 特征组件；无论是「意图识别」，还是「实体提取」都需要先有 features。而生成 features 可以采用多种方法的组合，例如： 传统 NLP 方法 现代的 word2vec 方法 基于人工定义的的规则生成 features（高精度）。规则可以是「字典」或者「正则表达式」 实体提取组件；也可以是多种方法： 传统的分类模型（SVMs） 也可能是前面提到的现代分类模型 还可能是基于规则的方法 意图分类组件。可能的方法有： 传统的 ER 模型（HMM） 现代 ER 模型， 基于规则的方法 另外，对话型应用也会接入一个「知识库」，在做「意图分类」时，也会判断（这个判断也是由算法模型+规则综合决定）是否需要从「知识库」里面查询有用的信息给用户（FAQ），如果判断出是要返回「知识库」的结果，DM 会调用知识库查询，然后返回 NLU 结果产出组件：产出 JSON 格式的 NLU 结果，其中包括「意图」和「槽位）NLU 的 pipeline 是顺序执行的；流水线上的组件可插拔（不同的业务场景使用不同类型的组件来工作）；但注意，流水线上的有些组件是互斥的（用了一个，就不能用另一个）；有些组件可以并行执行。NLU 实战下面给一些 NLU 实战例子。下面配置分别给出了训练所需要的数据（YAML 格式） 「意图」训练数据 「同义词」列表 「字典」（可枚举类型）定义。利用「字典」可以获得更多的 features，模型从而可以得到更加精准的结果 正则表达式定义。利用「正则表达式」可以按照模式匹配规则生成更加精准的 features 来改进 ER 和意图识别的效果 总之，「字典」和「正则表达式」都是规则。这些规则有 2 个用途： 1）可以获得更多的 features，利用这些 features 可以提高 ER 和意图识别的效果。但注意，某个规则返回的 features 只会作为 model 的候选，是否要真正的把这个 features 作为 ER 和意图识别的结果，最终还是取决于 model 根据具体的上下文来做决断 2）直接利用这些规则来提取实体nlu: - intent: greet examples: | - hey - hello - hi - hello there - good morning - good evening - morning - hey there - let&#39;s go - hey dude - good afternoon - intent: goodbye examples: | - ByeBye - bye - intent: weather example: | - What&#39;s the weather like [tomorrow]{&quot;entity&quot;: &quot;date&quot;} in [New York]{&quot;entity&quot;: &quot;city&quot;, &quot;value&quot;: &quot;New York City&quot;}? - intent: medicine examples: | - What medicine should I take if I have a [cold](disease) - I am [constipated](disease), what medicine should I take? - synonym: bike examples: | - bicycle - mountain bike - road bike - folding bike - lookup: city examples: | - New York - Chicago - San Francisco - Huston - regex: help examples: | - \\bhelp\\b下面是 2 个 NLU 结果的例子（JSON 格式）：{ &quot;text&quot;: &quot;show me chinese restaurants&quot;, &quot;intent&quot;: &quot;restaurant_search&quot;, &quot;entities&quot;: [ { &quot;start&quot;: 8, &quot;end&quot;: 15, &quot;value&quot;: &quot;chinese&quot;, &quot;entity&quot;: &quot;cuisine&quot;, &quot;extractor&quot;: &quot;CRFEntityExtractor&quot;, &quot;confidence&quot;: 0.854, &quot;processors&quot;: [] } ]}{ &quot;intent&quot;: { &quot;name&quot;: &quot;greet&quot;, &quot;confidence&quot;: 0.9968444108963013 }, &quot;entities&quot;: [], &quot;intent_ranking&quot;: [ { &quot;name&quot;: &quot;greet&quot;, &quot;confidence&quot;: 0.9968444108963013 }, { &quot;name&quot;: &quot;mood_great&quot;, &quot;confidence&quot;: 0.00005138086999068037 } ], &quot;text&quot;: &quot;hello&quot;}DMDialogue Management（以后简称 DM）负责记录对话上下文，并相应的对下一步的行动做出选择。DM 依据对话先前的轮次，决定当前的动作。DM 对于「任务型」对话特别重要，因为「任务型」对话往往需要多轮对话才能完成一个任务。DM 的任务就是管控对话流程： 当 DM 判断出所有的条件都已经满足时，调用业务接口，或进行查询，获得业务上要求的结果 另外，DM 也可以接入一个知识库（或知识图谱），当用户的意图是 FAQ 时，DM 会查询知识库获取 Answer 反之，当 DM 判断出条件还缺失时，返回提示给用户所以，DM 需要维护对话 session，或者说 context（其中包含了该多轮对话的所有历史信息）。具体来说，DM 负责 4 个工作： 对话状态跟踪：根据前一轮次的对话状态和前一轮次的系统动作，更新当前轮次的对话状态 对话策略（dialogue policy）：负责根据当前对话状态，相应的产出对话的下一步行动 对话动作（dialogue action）：基于对话策略的决定，完成真正的任务动作（一般是取调用业务接口来完成） 对话结果输出：把系统的操作结果以对用户友好的方式返回配置 DM因为 DM 如何运行依赖于业务方如何配置 DM。所以，在深入 DM 如果运行之前，先看一下如何配置 DM。DM 的配置是按 domain 为单位的。关于 domain 这里我们有 2 个前提： 一个 chatbot 中只有一个 domain 在一个 domain 内，在对话过程中，有可能在该 domain 下的不同 intents 之间切换 当然，在哪些 intents 中切换，甚至不出现切换，是根据「具体业务」来决定的 定义 chatbot 的 DM（单 domain 维度）需要提供 6 种信息 intents、entities、slots、responses、actions、forms。这 6 种信息简要说明如下（这里先简要介绍，后面会深入它们的细节）： intents：该 domain 内，所有可能的 intents（用户想要完成的意图）。intents 由 NLU 识别；这里要注意的是，虽然一个 domain 中有多个 intents 存在，但需要特别指出： 多轮对话中，可能 DM 并不一定会在所有的这些 intents 中切换。可以在哪些 intents 中切换，需要 DM 来决定 考虑一种极端情况：一旦 intent 确定后，就不能再切换到其他 intent 中去 entities：该 domain 内所有可能的 entities，也就是用户想要提供的关键实体信息。entities 由 NLU 识别 slots：在对话过程中，chatbot 需要跟踪和记录的信息 和 entities 不同，slots 不是由 NLU 填写，而是由 DM 负责填写 slots 除了「名字」之外，还需要指明其「类型」和「取值范围」；就「取值范围」来说，有「可枚举」和「不可枚举」2 种 可以通过配置，把 NLU 阶段获得的 entities 填到 slots 中 可以通过配置，把 action 的输出填到 slots 中 actions：chatbot 可以实施的动作；actions 就是 DM 的产出物：每一轮，DM 都要产出一个 action 作为结果。action 的更多相关信息，可以参考：对话流程配置 forms：定义必须收集的 slots 信息，以及如何填这些 slot 的规则。更多 forms 的配置可以参考：对话流程配置 responses：chatbot 响应模板；对话过程中，响应模板的实例化往往由 NLG 模块（可以是一个外部的 NLG 服务）来负责；而 DM 只负责把响应返回给用户接下来的几个小节会分别介绍「对话流程配置」，以及 DM 的这 6 种关键信息的细节。对话流程配置在定义好以上信息之后，配置 DM 的重点配置对话流程配置，也就是要提供对话 stories。需要配置「用户」和 chatbot 之间的对话流程（也就是对话的 steps）。 The story is a high-level semantic way of recording conversations. It records not only the expressions from users, but also the correct state change within the system.简单点说， chatbot（对应一个 domain）的 DM 的对话流程配置就是 stores 的列表 每个 story 定义的是针对一种用户输入，chatbot 需要实施的动作 而针对一种用户输入时，chatbot 实施的动作可能是多个步骤。所以，每个 story 对应一个 steps 列表。 也就是说，我们把每个 story 中要做的一系列步骤配置到 steps 列表配置 story steps 是配置 DM 的关键。steps 要配置针对每次用户的输入，bot 需要做的动作。具体如下： 用户输入已经被识别为 intent + entities。所以要配置根据 intent + entities DM 需要实施的动作的列表 配置各种 actions 的细节；以及对话的状态如何改变例一配置 Story 需要配置「用户」和 chatbot 之间的对话流程（也就是对话的 steps）。给一个例子：stories: - story: This is the description of one story steps: - intent: greet - action: action_ask_how_can_help - slot_was_set: - asked_for_help: true - intent: inform entities: - location: &quot;New York&quot; - price: &quot;cheap&quot; - action: action_on_it - action: action_ask_cuisine - intent: inform entities: - cuisine: &quot;Italian&quot; - action: restaurant_form - active_loop: restaurant_form上面例子的说明： stories 由多个 story 组成 每个 story 由 2 个字段组成：story 和 steps。story 字段是该 story 的文字描述；steps 表达人机之间的「对话流程」 所谓的「对话流程」就是每次「用户」发送一个消息给「chatbot」，然后「chatbot」做相应的「动作」，再等待下一次「用户」的消息，如此反复，就形成了一个「对话流程」 我们重点描述一下上面例子中的 steps 的定义： 用 intent + entities 字段描述「用户」发送过来的消息。可以参考上面例子中的： - intent: inform entities: - location: &quot;New York&quot; - price: &quot;cheap&quot; 然后针对这个「用户」发送过来的消息，给出「chatbot」相应的「动作」，以及这个「动作」返回的「事件」。参考上面的例子，对于用户的输入，「chatbot」可能执行 2 个动作： - action: action_on_it - action: action_ask_cuisine 「chatbot」在执行了「动作」之后，也会改变「对话」的「状态」。「对话状态」的改变也就是「事件」，「事件」是「动作」的结果 所以，在定义「动作」时，也需要定义其对应的「事件」（「对话状态」会怎么改变） 典型的「对话事件」有：「slot events」和「loop events」 slot events 会改变 slot 的状态。例如： - slot_was_set: - asked_for_help: true loop events 可以被设置为「生效」或「无效」状态。下面是一个「生效」的例子： - active_loop: restaurant_form例二stories: - story: Process starts steps: - intent: greet - action: action_ask_user_question - checkpoint: check_asked_question - story: Handle user&#39;s confirmation steps: - checkpoint: check_asked_question - intent: affirm - action: action_handle_affirmation - checkpoint: check_flow_finished - story: Handle user&#39;s denial steps: - checkpoint: check_asked_question - intent: deny - action: action_handle_denial - checkpoint: check_flow_finished - story: Process ends steps: - checkpoint: check_flow_finished - intent: goodbye - action: utter_goodbye这个例子中，使用 checkpoint 字段实现了 story 之间的跳转： 「Process starts」这个 story 可以跳转到另外 2 个 story： 当用户回复「肯定」时，跳转到「Handle user’s confirmation」story 当用户回复「否定」时，跳转到「Handle user’s denial」story 最后，再都跳转到「Process ends」story使用 checkpoint 字段可以简化 stories 的配置，但也不能过度使用，否则会造成配置过于复杂，难于维护。actions专门说一下 actions 的细节（actions 的输入和输出）。 每个 action 的输入包括：用户输入、对话状态、对话历史 每个 action 的输出可以包含以下 4 种产出物中的一种或多种： 1）用户响应 2）forms（收集必须的 slots 信息） 3）对话状态修改；这里说的对话状态可以被修改为：「等待用户输入而不是由 DM 来判断 action」、「清空session，重启对话」、「对话开始」、「退回到上一轮对话状态」、「NLU 识别失败时，转到请用户再说一次的状态」…… 4）外部接口调用 也专门说一下 forms 的细节： forms 定义必须收集的 slots 信息，以及如何填这些 slot 的规则。例如，可以是把 NLU 阶段得到的实体 map 到 slot 除此之外，还要定义什么场景下「激活 form 开始工作」。例如，可以设置为一旦某个 intent 下，自动激活某个 form最后讨论 intents 如何切换的细节： 可以由人工来定义 intents 直接的切换规则。例如，在一个 step 结束时，可以指定一个规则，在这个 step 完成之后，切换到另外一个 story 去，而这个新的 story 可能是另外一个 intent。 但人工定义 intents 切换规则的一个问题是，随着规则越来越多，切换规则也就越来越复杂，后期难以维护。slotsslots 用于记录对话过程中的关键信息，一般是 KV 对的形式。例如，一个天气查询的对话任务，对话过程中，「chatbot」必须收集 2 个关键信息，也就是 2 个 slots：地点、时间。同时，slots 需要有「值」和「类型」（类型规定了这个值的取值范围及其支持的运算类型）。常用的 slots 类型有：text、bool、float、list、any……slots 的值可以影响到「对话」的「运行流程」。slots 的值可以从用户的输入获得（例如 intents 或 entities）；也可以从 actions 执行的结果获得（例如调用外部服务获取天气信息）。formform 定义了该 intent 所需要的所有 slots。简单例子：forms: weather_form: address: - entity: address type: from_entity date-time: - entity: date-time type: from_entity该 form 指明了需要 2 个必须的 slots：address 和 date-time。但如何把 NLU 阶段收集到的信息（intents + 实体）填入到 form 指明的 slots 呢？我们可以在对话流程配置中定义填入规则（也就是所谓的「slot mapping functions」）。上面例子中的「slot mapping functions」就是：from_entity，也就是直接把 NLU 阶段识别到的「同名实体」填入 slots。在配置好 form 之后，还需要在对话流程配置中指明什么时候启用该 form，开始 slots 的收集工作（默认 form 是不工作的，不然 DM 执行起来后就开始收集 slots 的工作）。fallbackchatbot 不能处理的「用户输入」始终存在，所以我们专门用一节来说明一下当遇到不能处理的「用户输入」时，chatbot 要怎么工作，也就是所谓的 fallback。有 2 类 fallback：NLU Fallback，和 Policy Fallback。NLU fallback：NLU 不能清晰的辨别出用户的 intent： 如果识别出 intent 的 threshold 被设置为 0.6，那么如果小于 0.6，就会触发 nlu_fallback 如果存在 2 个预估 intent 的 threshold 的差距小于 0.1，也会触发 nlu_fallback当触发了 nlu_fallback 时，可能采取的 action 是：请用户重说一次。Policy fallback：在 DM 阶段不能预测出任何 action；或者有多个可能的 actions 难于抉择出使用哪一个。针对 NLU Fallback，我们可以对 chatbot 的 NLU 进行配置，例如：- FallbackClassifier threshold: 0.6 ambiguity_threshold: 0.1主要就是设置了 2 个 threshold：NLU 识别出 intents 的阈值为 0.6；识别出不止一个 intents 的模糊阈值为：0.1。一旦触发了这 2 个 thresholds 的阈值，NLU 就会把当前 intent 识别为一个特殊 intent：Fallback。然后在根据事先配置好的，和 Fallback 意图对应的 action，chatbot 就会执行对应的 Fallback 动作，例如：rules: - rule: Ask user to speak again steps: - intent: nlu_fallback - action: utter_please_rephrase也就是当 fallback 时，utter_please_rephrase 这个 action 被触发。而 Policy Fallback 就是在对话流程配置时，直接指定对话执行到某一步时，触发 fallback。这种场景较简单，就不细说了。执行 DM在了解了 DM 的配置后，再说明一下 DM 运行时的细节。DM 运行时的核心是 Policy 模块。我们会把 DM 配置好的各个 stores 转换成「对话状态」，然后得到这些「对话状态」的特征。而 Policy 模块工作时按照「对话状态」的特征来预测对话的下一个 action。常用的 Policy 实现有： TEDPolicy：Transformer Embedding Dialogue MemoizationPolicy：把配置存在「字典」中，然后通过查字典的方式来采取相应的 action AugmentedMemoizationPolicy：MemoizationPolicy 的基础上加上随机步骤，使得 DM 运行起来合理 RulePolicy：DM 完全基于各种跳转配置来运行工程上，以上的 Policy 可能会被同时使用，但会根据优先级计算出真正采用哪个 actions。各个 policies 的优先级为：RulePolicy &amp;gt; MemoizationPolicy &amp;gt; TEDPolicyRulePolicy例一：rules:- rule: mapping from some_intent to some_action steps: - intent: some_intent - action: some_action也就是一旦 NLU 识别出特定的 intent 就触发指定的 action。NLGNLG 负责返回给用户响应内容，一般来说是按文本的形式返回。NLG 目前项目中的做法是按照业务规则把要返回给用户信息提入到实现配置好的 template 作为响应，回复给用户。现在也在探索不用规则，而是通过训练得到模型来决定返回什么样的响应给用户。FAQ本节关注 FAQ（也就是「问答型」对话）相关的内容。「任务型对话流程」和「问答型对话流程」是有可能混合在一个对话 session 中的。也就是说，在对话过程中，可能会从「任务型」对话切换到「问答型」对话。所以我们需要在 NLU 阶段做识别，识别出是否要进入「问答型」对话。对应的，需要给 NLU 提供一组 QA 对作为「问答型」对话的训练数据。给一个「问答型」对话的一些 QA 的训练数据的例子： - intent: faq/notes examples: | - What are the precautions for applying for the company&#39;s campus recruitment position? - intent: faq/work_location examples: | - Where is the main work location of fresh graduates admitted to campus recruitment? - intent: faq/max_job_request examples: | - How many positions can I apply for at most? - intent: faq/audit examples: | - Description of each stage of the audit - intent: faq/write_exam_participate examples: | - How to take the written test? - intent: faq/write_exam_location examples: | - How to arrange the location of the written test? - intent: faq/write_exam_again examples: | - Is there only one written test? I didn&#39;t take part in the written test on the day. Is there any chance for another written test?知识库在「问答型对话」中，用户可能需要查询目标，及其属性，这个时候，就可能需要引入 知识库（Knowledge Base，简称 KB）。和普通的「问答型」对话类似，需要配置 NLU，当用户提问时，能触发对 KB 的查询。为了接入一个 KB，需要在 DM 中新增一种特殊的 action，KB action：用户处理引用决议，查询对象及其属性。需要在 NLU 中提供触发 KB actions 的机制。另外，DM 也需要记住上一轮发送给用户的消息（例如：Bot 先发消息给用户请选择以下选项，然后用户选了 1 选项）。使用 KB 要做 2 种配置： 配置 1：建立 KB。所谓的 KB，就是指把复杂的数据存储，并组织起来，特别是数据间的关系。 另外，KB 的返回需要被润色，使得对用户更加友好。 配置 2：设置 NLU 触发查询 KB 需要提供 NLU 意图识别需要的「训练数据」 一般来说，有 2 类用户请求会触发查询 KB： 用户请求返回一个某种对象的列表，同时，也可能附加某种条件，过滤返回的结果 用户请求获取具体某个对象的属性下面给一些相关的例子。创建一个 KB 需要的配置，一般是结构化的信息数据：{ &quot;song&quot;: [ { &quot;id&quot;: 0, &quot;name&quot;: &quot;Billie Jean&quot;, &quot;singer&quot;: &quot;Michael Jackson&quot;, &quot;album&quot;: &quot;Thriller&quot;, &quot;style&quot;: &quot;Rock&quot; }, { &quot;id&quot;: 1, &quot;name&quot;: &quot;Like a Rolling Stone&quot;, &quot;singer&quot;: &quot;Bob Dylan&quot;, &quot;album&quot;: &quot;Highway 61 Revisited&quot;, &quot;style&quot;: &quot;Rock&quot; } ], &quot;singer&quot;: [ { &quot;id&quot;: 0, &quot;name&quot;: &quot;Bob Dylan&quot;, &quot;gender&quot;: &quot;male&quot;, &quot;birthday&quot;: &quot;1941/05/24&quot; }, { &quot;id&quot;: 1, &quot;name&quot;: &quot;Michael Jackson&quot;, &quot;gender&quot;: &quot;male&quot;, &quot;birthday&quot;: &quot;1958/08/29&quot; } ]}NLU 要做对应的配置，使得用户能触发针对 KB 的问答。例如： - intent: query_knowledge_base examples: | - What nice [song](object_type) is there? - Is there any [singer](object_type) that sings well? - List me some [songs](object_type) - List me some [singers](object_type) - List me some [songs](object_type) of [Sting](singer) - List me [songs](object_type) of [Bob Dylan](singer) - List me [songs](object_type) sung by [Bob Dylan](singer) - [Michael Jackson] What does (singer) have [song] (object_type) - [Michael Jackson](singer) What is [song](object_type) - [That song] (mention) belongs to what [album](attribute) - [Who] (attribute) sang [that song just now](mention) - Who is the [singer](attribute) of [the previous song](mention) - What [genre](attribute) does [that song](mention) belong to? - What is [style](attribute) of [last](mention)? - What [album](attribute) does [first](mention) belong to? - [Album](attribute) of [First](mention) - [The first](mention) is [who](attribute) sang? - [The last one](mention) is [which](attribute) sang? - [Which singer](attribute) sang [Like a Rolling Stone](song)? - What [album](attribute) does [Billie Jean](song) belong to? - [Album](attribute) of [Billie Jean](song)? 注意以上「训练数据」中的 3 中特殊实体：object_type、mention、attribute object_type：用户想要查询的「实体对象」 mention：用户表达中的指代（reference resolution）。比如：this、that、the first…… attribute：用户想要查询到的「实体对象」的具体「属性实体」 这 3 种实体及其 slots 定义如下：entities: - object_type - mention - attribute - object-type - song - singer - genderslots: attribute: type: any gender: type: any knowledge_base_last_object: type: any knowledge_base_last_object_type: type: any knowledge_base_listed_objects: type: any knowledge_base_objects: type: any mention: type: any object_type: type: any singer: type: any song: type: any上面的例子中，第 1 种和第 3 种比较简单：用户查询 object_type 和查询 object_type 的 attribute。第 2 种比较复杂，例如：Bot:I find the following songs:1: Billie Jean2: The Shape of My Heart3: Like a Rolling StoneUser:Which album does the first one belong to?然后可以对「序号」的指代关系进行配置，例如：{ &quot;1&quot;: lambda l: l[0], &quot;2&quot;: lambda l: l[1], &quot;3&quot;: lambda l: l[2], &quot;4&quot;: lambda l: l[3], &quot;5&quot;: lambda l: l[4], &quot;6&quot;: lambda l: l[5], &quot;7&quot;: lambda l: l[6], &quot;8&quot;: lambda l: l[7], &quot;9&quot;: lambda l: l[8], &quot;10&quot;: lambda l: l[9], &quot;ANY&quot;: lambda l: random.choice(l), &quot;LAST&quot;: lambda l: l[-1],}针对 reference resolution，需要定义给各种「指代关系词」定义一个「同义词」列表。例如「第一」这个「指代关键词」： - synonym: &#39;1&#39; examples: | - First - first - First song" }, { "title": "Github Markdown", "url": "/posts/git-ref/", "categories": "notes", "tags": "Markdown, Github", "date": "2022-10-24 00:00:00 +0800", "snippet": " Here’s a tip – if you’re having trouble with your anchor not working due to misspellings or odd characters, simply hover over your heading on Github, then hover or click the link icon that appears to the left. You can then right click to copy the link location, left click it to go to the URL and view it in your address bar, or simply stay hovered over it and view it in the status bar of your browser.测试内部锚链接（anchor）能正常工作的各种场景。English Title纯英文标题。中文Title中文+英文，但无空格。中文 Title中文+英文，但有空格。Title中文英文+中文，但无空格。5 中文数字+中文，有空格。6 中文 Title数字+中文+英文，有空格。7. 中文 Title?数字+中文+英文，有空格，有英文标点。测试下面是对各种情况的测试。纯英文标题：English Title[English Title](#english-title)英文+中文，但无空格：中文Title[中文Title](#中文title)中文+英文，但有空格：中文 Title[中文 Title](#中文-title)英文+中文，但无空格：Title中文[Title中文](#title中文)数字+中文，有空格：5 中文[5 中文](#5-中文)数字+中文+英文，有空格：6 中文 Title[6 中文 Title](#6-中文-title)数字+中文+英文，有空格，有英文标点：7. 中文 Title?[7. 中文 Title?](#7-中文-title)" }, { "title": "DDD Notes", "url": "/posts/ddd/", "categories": "notes", "tags": "DDD", "date": "2022-10-07 00:00:00 +0800", "snippet": " Part 1: DDD 总览 DDD 要解决的问题 问题域 问题空间 解空间 Strategic Patterns Tactical Patterns 领域 子域 逻辑概念和物理概念 通用语言 领域模型 限界上下文 模式 Part 2: DDD 原则与实践 DDD Intro 对问题域进行提取 识别核心域 Model Driven Design 领域模型实现模式 Bounded Context 上下文映射 应用架构 DDD 的常见问题 DDD 的实施 Part 3: 战略模式 集成限界上下文 通过消息系统集成 Part 4: 战术模式 模型模式 生命周期模式 显露模式 Part 1: DDD 总览在深入 DDD 细节之前，先从对 DDD 的基本思想进行概括。DDD 要解决的问题存在的问题：复杂问题域的挑战。 反模式：「大泥球」模式。领域复杂性和技术复杂性被混合在一起，导致了软件维护困难 举个例子：一个 BBS 领域中，和 Post、Forum、Discussion 等概念相关联的是 Author；而不应该和「用户权限」相关联；如果把 BBS 领域中的概念和「用户权限」的相关概念关联起来，就可能造成后期难于维护 通用语言和「问题域知识」的缺乏使得软件不能和「业务逻辑」对应，导致了「大泥球」模式。也使得「分析模式」和「代码」之间转换的代价极其高昂 「分析模式」是一种表现形式，使得非技术人员能够理解软件的逻辑和结构 如果不围绕着问题域来设计和和组织软件的开发，会导致复杂度快速提升如何来解决复杂问题域？DDD 提出了战略模式和战术模式。在 DDD 中，整体的建模和设计过程大致为： 挖掘「User Stories」（即「User Cases」收集） 建立 UL 战略模式设计 战术模式设计 但以上过程并不是线性的过程，而可能是「螺旋上升」不断优化的过程。在具体介绍 DDD 的细节之前，先简要引入各种 DDD 的概念。问题域问题域：Problem Domain。什么是问题域？问题域指的是当前正在构建的软件的主要范围（Subject Area）。DDD 强调的是在构建大型复杂软件时，必须把对问题域的关注放在最高的位置。例如，在构架一个医疗相关的软件时，不是要去了解怎么成为一个医生，而是要专注于该领域的术语，不同的利益方看待病人的不同方式，以及医生需要收集哪些信息，并且怎么处理这些信息。DDD 的目的就是用来应对 2 大挑战： 理解问题域 创建出长期可维护的解决方案来解决问题域中的问题DDD 利用多种战略模式和战术模式来解决这 2 大挑战。所有的战略模式和战术模式都是用来管理问题空间和解空间中的复杂性。问题空间DDD 首先强调问题空间和解空间的概念。问题空间：Problem Space。在解决问题之前，要先专注在对业务问题域的理解。DDD 首先强调的是定义问题空间，在问题空间定义问题。问题空间事实上就是领域和子域的组合（也就是问题域）。 也就是先把精力集中在识别、提炼问题域（Problem Domain）上 把大的「领域」（Domain）分解为多个小的子域（Subdomain）解空间解空间：Solution Space。在理解了问题域后，在解空间解决问题（也就是用代码来解决问题）。解空间是一个或多个限界上下文，也就是一组特定的「软件模型」（DDD 里面可以采用领域模型作为「软件模型」）。 注：但「软件模型」也不仅仅只有领域模型，根据场景的不同，可以采用其他更简单的「软件模型」 先在问题空间，针对领域、子域进行提炼 识别出「核心子域」、「支撑子域」、「通用子域」 然后在解空间对限界上下文进行分析 盘点现有「软件资产」、创建哪些新「软件资产」、如何对「软件资产」集成 在解空间要重点关注「核心子域」的解决方案 Strategic Patterns战略模式：strategic patterns。 DDD 的战略模式：The strategic patterns of DDD distill the problem domain and shape the architecture of an application 翻译：DDD 的战略模式对问题域进行提炼，并依此塑造出应用的架构 战略模式包括以下工作模式： 在问题空间，从问题域中提炼出现实世界中最重要的部分 把领域分割成子域更容易管理 并识别出为「核心域」的子域 在解空间，给每个子域创建一个「软件模型」来解决领域问题 使「业务逻辑」和「软件代码」保持一致 对不同复杂度的子域使用不同的「设计模式」来实现「软件模型」 基于通用语言的协作 通用语言是保持「软件模型」和「分析模型」一致的保障 「模型」必须是建立在限界上下文（Bounded contexts）之中 理解限界上下文之间的关系：上下文映射Tactical Patterns战术模式：tactical patterns，也叫做 model building blocks（模型构建块）。DDD 中使用战术模式一个重要理由是：由于「核心域」中的「业务逻辑」十分复杂，为了在「核心域」中建立起正确的「领域模型」，需要借助战术模式来实现。 DDD 的战术模式：其实就是一系列的设计「模式」，这些「模式」用来帮助开发人员更有效的创建限界上下文中的「模型」（很多时候是针对代码层面） 战术模式要解决的是限界上下文内部的实现细节领域领域：Domain。什么是领域？广义上，领域就是当前组织在做的事情，以及这个组织做这些事情所处的世界。当我们为一个组织开发一个软件系统时，我们就工作在这个组织的领域当中。子域子域：Subdomain。 DDD 强调每个组织是由多个子域组成的；而不是一个组织对应单个领域。 企图把整个领域实现在单个系统内，事实上是相当难的：随着系统复杂度的不断提高，面对新的业务需求，单个系统最后无法维护 例如，一个「在线零售」业务的领域可能是由 5 个子域构成的：商品类目、发票、支付、物流、库存 DDD 提供了战略模式帮助开发人员把领域从逻辑上切分为多个子域 使用战略模式并不是要对现有系统重新划分进行重构，而是先从「逻辑」上把子域识别出来 先使用战略模式识别出「核心子域」 、「通用子域」、「支撑子域」 「核心域」才是公司的价值所在，公司只对「核心域」重点投入（人力，物力） 对于其他子域，无需投入大量精力，可以采用简单的实现方式，甚至直接购买现成产品 逻辑概念和物理概念领域和子域都是逻辑上的概念。 在画图的时候，对子域的边界使用虚线（逻辑概念）。逻辑概念相对应的是物理概念。 物理概念关注的重点是在真实系统中，在物理上如何把逻辑概念通过代码来实现。通用语言通用语言：Ubiquitous Language（UL）。 在通用语言（UL）还没有被提炼出来的时候，可以借助「图」，「术语表」等辅助工具来把通用语言提炼出来 但这些工具仅仅在开始的时候使用，真正能保持和「业务逻辑」一致的只有代码。所以最后要把「图」，「术语表」这些东东到后来都要抛弃掉 UL 是 DDD 的 2 大强大的工具（UL 和 限界上下文）之一 通用语言中的概念只有在确定的限界上下文才具备确定的含义；UL 和限界上下文是一对一的关系 Ubiquitous means “pervasive,” or “found everywhere,” as spoken among the team and expressed by the single domain model that the team develops 这里的「通用」意味着「无处不在」或「每处都能发现」，也就是会在团队内部（无论团队内什么角色：开发、测试、业务专家……）广泛使用，用于表达单个「领域模型」 领域模型领域模型：Domain Model。领域模型是解空间中的概念。 什么是领域模型？领域模型就是某个特定业务领域的「软件模型」；领域模型也就是把从这个业务提炼出来的通用语言（UL）表示为软件模型 领域模型一般通过「对象模型」来实现 这些对象同时包含数据和行为 这些数据和行为能准确的表达业务含义 领域模型存在于限界上下文之内，在限界上下文中对「模型」进行开发。也就是领域模型的所有「基础部件」（Building Blocks）都处于限界上下文之内 领域模型的概念（属性和操作）只在对应的限界上下文中有意义 通用语言之外的概念不应该被引入到领域模型中「领域模型」相关的更多实践细节请参考：领域模型实践。限界上下文限界上下文：Bounded Context。限界上下文是物理的概念。限界上下文是包含了领域模型的明确边界。 和领域、子域等逻辑概念相对应的是物理概念；也就是在真实系统中，在物理上如何把逻辑概念通过代码来实现。领域模型和限界上下文都是物理概念，都和真实系统以及代码实现相关。 总之，在逻辑上，通过战略模式把领域分为若干子域；然后在物理上，在限界上下文中完成对领域模型的开发。在限界上下文中的术语，只在当前限界上下文有效。「限界上下文」的更多细节在后面会有更多介绍。模式DDD 提供了 2 大类模式作为方法论： 战略模式：Strategic Pattern 战术模式：Tactical PatternDDD 的战略模式能使得产品的「核心部分」和「通用区域」分割开了，避免修改「核心部分」时影响到整个产品。产品的「核心部分」（极其复杂+频繁变动）必须是基于「模型」的。DDD 的战术模式（同时配合 Model-Driven Design）帮助开发人员针对 Domain，创建出代码级别的「模型」。「模型」是物理概念： 「模型」是分离复杂性，让「软件」能够持续进化的关键 「模型」也是在问题域上和其他开发人员以及领域专家沟通的关键下面的章节开始详细介绍 DDD 的各种细节。准备分 3 部分来介绍： DDD 基础理念，以及 DDD 的原则与实践 DDD 的战略模式（strategic pattern） DDD 的战术模式（tactical pattern）Part 2: DDD 原则与实践先介绍第一部分：DDD 的原则和实践。之后会再分别介绍其他 2 个部分：战略模式和战术模式。DDD IntroDDD 解决复杂问题域DDD 如何利用战略模式和战术模式来解决复杂问题域的问题？DDD 战略模式的主要工作模式如下： 识别出「核心域」，找到软件真正的价值所在，「核心域」才是真正值得投资的部分。避免「大泥球」模式的出现 在解空间中，为每个子域（Subdomain）构建一个「软件模型」来和「业务逻辑」对齐。不过由于不同的子域的重要性及复杂度不同，所付出的实现代价也就不同（例如，不复杂的子领域，甚至可以不用面向对象模型来构建） 既然「模型」是由「领域专家」和「开发人员」合作建立起来的，所以一套不断进化的通用语言（UL）是必须的。「分析模型」中的新概念会通过 UL 反馈到「代码」中，「代码」中的概念也会被复制到 UL 中 「模型」属于限界上下文（Bounded Context），限界上下文定义了「模型」的应用范围，同时确保了「模型」的完整性。这种围绕着模型的保护边界可以防止软件变成「大泥球」模式（「模型」被隔离在自己的边界内变化，避免把复杂度扩展到其他「上下文」） Context Map 用来表现处于不同「上下文」中的「模型」的关系DDD 实践原则一些 DDD 实践原则： 集中在「核心域」（Core Subdomain） 协作：需要「领域专家」和「开发人员」的协作，以及相互学习 通过探索和试验来创建「模型」：「分析模型」和「代码模型」的不断迭代和相互反馈 交流：必须创建 UL 来保障正确而高效的交流 理解「模型」的应用范围（「模型」处于「上下文边界」内） 「模型」保持不断进化DDD 的常见误区一些 DDD 常见误区： 战术模式是 DDD 的关键 DDD 是一种 Framework DDD 是银弹之后，我们开始介绍 DDD 的工作流程。对问题域进行提取DDD 的工作流程是先从对问题域的理解出发，也就是先对问题域进行提取。 从问题域提取出建设「模型」所需要的知识 知识提取必须和「业务专家」合作才能成功：「业务专家」+「开发团队」+「Use Cases」=&amp;gt; 满足用例的「模型」 「领域知识」可能比技术本身还要重要：开发人员要具备有使用简单术语向客户描绘「业务域」中复杂概念的能力 完成「知识提取」并达成共识的一个标志是产出通用语言 「利益方」（甲方）和「业务专家」是不同的角色 「利益方」更多的是描绘对系统的愿景、系统的输入输出；「开发团队」把这些信息收集起来形成「Use Cases」 「业务专家」和「开发团队」一起合作进行「知识提取」，并产出「模型」 一些有用的手段： 「手绘的草图」 CRC 卡（类职责合作卡）：类名及其蕴含的概念+类的职责+类之间的合作关系 避免过早的对模型中的概念命名 在和「业务专家」交流时可以使用 BDD（GWT，Given-When-Then）。也就是对一个特性进行描绘可以分解为：条件（Given）、场景中包含的事件（When）、输出结果（Then） 利用快速原型 Impact Mapping（理解为业务事件「影响力」）：利用业务事件的「影响力」来更好的理解业务 例如：利用「思维导图」的形式展示出「提高自行车销量 25%」这个业务事件的影响力 业务模型图 …… 识别核心域识别「核心域」： 首先要发掘问题的实质：要去了解需求背后的深意，理解需求方的真正目的 向需求方寻求「项目概述」，也就是确立该项目的真实原因和愿景：为什么要编写这款软件，而不是到直接购买；软件做成什么样子才算成功？ 在问题空间把问题域分割为子域。而这个过程是「逻辑」过程，和公司组织结构以及技术细节无关 然后区分出「核心域」、「通用域」、「支撑域」 把「核心域」作为产品，而不是项目。项目有终点，而产品是会被不断打磨，进化DDD 的理想状况限界上下文和子域一一对应；但在现实中可能并不是理想状况。需要先对现有状况进行盘点： 这种场景特别常见：少量物理上的「子系统」（也就是「上下文」）中包含了多个子域 也可能多个物理上的「子系统」（也就是「上下文」）在同一个子域当中 可以用图的形式表达出现有状况 。 其中，用「实线」划分出「物理」上现存的「子系统」（也就是「上下文」）； 同时，由于我们已经把问题域分割为子域，画图时，可以用「虚线」划分出「逻辑」上的子域。在对现有状况进行盘点后，接下来的任务就是向限界上下文和子域一一对应的目标前进。Model Driven Design在完成对问题域的理解，并识别出「核心域」之后，开始把关注点放到解空间。DDD 提倡使用 Model-Driven Design 来到达「分析模型」和「代码模型」的一致。Model-Driven Design 重度依赖通用语言（UL），利用 UL 把「分析模型」和「代码实现」绑定在一起，并在软件的生命周期中保持一致。Domain Model既然是 Model-Driven Design，先来看看什么是 Model（模型），也就是领域模型： DDD 中，Domain Model 是「一体两面」的形式存在，而且这「两面」要保持同步一致： 既是一种「分析模型」，以 UL + 草图的形式存在 也是一种「代码模型」，以「代码实现」的形式存在 虽然 Domain Model 最后的形态是「代码模型」，是解空间的概念；但一开始的时候，它是在「知识提取」阶段，在问题空间作为一种「分析模型」成形的（「开发团队」和「业务领域专家」协作产出） 而最后当 Domain Model 成为「代码模型」后，它就只是为了满足问题域中 User Cases 需求，而设计出来表示问题空间的视图 这里强调一下：「代码模型」只是「视图」，只是对问题域中的「现实」状况的抽象，是按「代码实现」的方式来表达，而不是「现实」状况本身；与之相对的，Domain 表现的则是问题域实实在在的「现实」状况 也就是说，不需要把「现实」中的所有事物和行为都提取为领域模型中的「名词」和「动词」；而是要抓住重点，把精力放在怎么抽象出领域模型来解决问题域的问题，满足「User Cases」 另外一个建模时要注意的问题就是不要轻易使用「抽象」。「抽象」必然会付出代价，所以需要确实能从「抽象」获得收益，才可以在设计时对某些概念进行「抽象」 仅仅对「变体」（变化的事物）采用抽象 UL 是问题空间和解空间的桥梁。Domain Model 和使用了相同 UL 的「分析模型」绑定在一起 Domain Model（领域模型） 是解空间的概念；对应的，领域是问题空间的概念 Domain Model 的用途是解决问题空间的「业务 User Cases]，表达领域中的复杂的「业务逻辑」，并不是对「现实」状况的表现 「代码模型」是领域模型的主要表现形式 DDD 强调「代码模型」（也就是实现）需要和「分析模型」（也就是设计）保持一致，这种一致需要靠通用语言来保证 「分析模型」也叫做「业务模型」。「分析模型」用于对问题域的理解 如果「开发团队」发现代码和「分析模式」存在不一致，需要立即和「领域专家」沟通解决：code is model；code is truth Model-Driven Design 和 DDD（领域驱动设计）的区别就是前者注重实现，后者注重语言、协作和知识 模型驱动设计实践所谓 Model-Driven Design 就是把「分析模型」绑定到「代码模型」，并确保 2 者不断进化且保持一致的设计过程。Model-Driven Design 和 DDD 的区别： DDD 专注 UL、协同、和「领域知识」的提取 Model-Driven Design 专注实现Model-Driven Design 和 DDD 互相补充。传统的开发模式往往会造成「分析模型」和「代码模型」偏离。而 DDD 强调的是「开发」、「测试」、「业务分析人员」、「架构师」、「业务专家」工作在同一个团队中，对问题空间进行讨论，协作提取「领域知识」：技术人员要懂业务，业务人员要懂「分析模型」以及怎么验证「代码模型」。「代码模型」和「分析模型」不断迭代保持同步。总之，DDD 强调的是协作：「团队建模」。通用语言实践 通用语言是 DDD 的基础（既然 DDD 强调「业务专家」和「开发团队」直接的沟通和协作） 从「实际例子」中来提取 UL。但注意： 不要在描述「实际例子」时使用「技术术语」 不要在描述「实际例子」时转到解空间，而是要专注在问题空间 尽量避免使用「过载」的词语，例如：service，manager，policy 等等 专注于「核心域」，首先在「核心域」使用「模型驱动」并建立 UL；不需要把太多精力放到「通用域」和「支撑域」 作为「技术开发人员」，可以尝试为自己正在工作的领域开发一套通用语言领域模型实现模式正如前面提到的，并不是所有的领域（或者说子域）的复杂度和重要性都是相同的，所以，在对复杂度和重要性不同各个子域进行建模时，我们可以采用不同的「领域模型的实现模式」。下面各个小节给出不同的「实现模式」，并给出它们的适用场景。领域层另外，也先明确一下领域层（Domain Layer）：是应用程序的核心，包含了领域模型的代码。领域层在领域模型实现中的作用：在代码构架中把领域模型和代码的其他部分隔离开，比如把领域模型和「代码基础设施」（事物管理、状态持久化等……）隔离开，从而实现对「复杂度」的隔离。领域模型模式 领域模型模式（Domain Model Pattern）是面向对象模型，适用于「业务逻辑」复杂的领域（Subdomain） 值得注意的是，「领域模型模式」的建模过程中不需要考虑数据的持久化，也就是说，不需要涉及到「数据库建模」 如果建立「领域模型」时，同时也考虑如何「数据库建模」反而会影响到对「领域」的抽象 领域模型可以被看成是当前领域的「概念层」。事物存在于该模型中，并与其他事物相互关联作用 「模型」就是数据+行为；建模是应该把注意力集中在「行为」上，而不要去关注「模型」的状态 特别注意的是：只有在对「核心域」建模时，才使用「领域模型模式」；对于「业务逻辑」不复杂的其他子域（例如：通用域，支撑域），不一定要使用「领域模型模式」，可能使用「贫血领域模型」或者「表模型模式」就够了事务脚本模式 「事务脚本」使用的是「过程式」的编程方式，而不是面向对象的编程方式 「事务脚本」的优点是上手比较简单；但当业务逻辑复杂度增大的时候，重叠的功能逐渐增多，「事务脚本」就会遇到管理困难的问题 当「业务逻辑」复杂度增高时，需要从「事务脚本」向领域模型的方向进行重构表模型模式 「表模型模式」把「对象模型」对应到「数据库模型」上： 单个「对象模型」表示数据库中的单个「表」或「视图」 「表模型模式」特别适用于「数据库驱动设计」（Database-Driven Design）方法 对于被「有边界的上下文」隔离出来的简单领域，并且该领域的数据形式很简单的话，可以考虑使用「表模型模式」；这样做的优点是更容易掌控 当「对象模型」和「数据库模型」开始出现分歧时，需要从「表模型」向领域模型方向进行重构Active Record 「Active Record 模式」是「表模型模式」的变体；它不是把「对象模型」对应到「表」，而是把「对象模型」对应到「表的行」 单个「对象模型」表示「表中单独一行」在某个时刻的状态 在使用「Active Record 模式」时，每个「业务对象」都负责其自身的持久化，及其相关的业务逻辑 「Active Record 模式」适用于「对象模型」和「数据模型」一一对应的场景贫血领域模型 「贫血领域模型」和领域模型的共同点是都使用了「领域对象」。但和领域模型不同，「贫血领域模型」中的「领域对象」不包含行为，仅仅包含了数据 「贫血领域模型」最大的缺点也就是其采用了类似「过程式」的程序风格（类似「事务脚本」模式） 「过程式」的程序风格最大的问题是违反了所谓的「Tell，Don’t Ask」原则。如果按照该原则，「对象」只会告诉客户端该「对象」能做什么，不能做什么；而不能把「对象」自己的状态暴露给客户端；不能让客户端去根据「对象」的状态来实施动作 「贫血领域模型」中的「模型对象」被剥夺了「业务逻辑」，仅仅存储了数据 「贫血领域模型」适用于「业务逻辑」比较简单的场景；或者是不擅长「面向对象」开发的「开发团队」贫血领域模型和函数式编程 在使用「函数式编程」时，「贫血领域模型」不再是「反模式」？ 尽管表面上看起来，「贫血领域模型」剔除掉了把「领域概念」表示成「对象」的能力，从而和领域模型强调的与「领域专家」进行对话有所冲突。但事实上，使用「函数编程」+「贫血领域模型」依然可以做到和「领域专家」进行有意义的对话？ 这部分内容还需要进一步的理解Bounded Context限界上下文的必要性限界上下文是 DDD 成功的关键。对当前的软件系统进行盘点，往往会发现现状可能是「模型」的边界不清晰，但 DDD 的目标就是要让每个子域对应一个限界上下文： 限界上下文也是「语言」上的界限：DDD 不是要提炼出整个领域内都适用的「术语」，DDD 的目标是获取到限制在限界上下文之内的「术语」 如果不清楚「模型」（Model）适用于哪个「上下文」，「模型」（Model）就会变得越来越模糊，「模型」（Model）的概念和逻辑也会越来越混乱 所以需要保护「模型」（Model）的「完整性」，并在代码中清晰的定义「模型」（Model）的职责的边界：Bounded Context（限界上下文） 把一个「模型」（Model）绑定在特定的「上下文」也就是限界上下文 限界上下文对管理解空间的复杂度至关重要，所以使用限界上下文是成功实施 DDD 的关键。限界上下文切分大模型如上面一个小节说的，一个「模型」（或「领域概念」）如果跨了多个「子域」，需要正确的把它切分到多个限界上下文中去 。也就是要定义「模型」的边界。一些切分大模型到多个限界上下文的原则： 依据领域中的「术语」在不同「上下文」所具有的不同「含义」，将「模型」切分到不同的限界上下文中 通过和「业务能力」对齐的方式来进行切分 不过要特别小心「业务能力」和限界上下文不是完美对应的场景 「开发团队」的组建也要和限界上下文对齐（而不是和公司的组织结果对齐） 同时，「开发团队」也要熟悉自己所在的限界上下文和其他限界上下文的关系 「开发团队」也要保持和「其他团队」的交流，分享自己团队当前工作内容和未来的工作计划，甚至和「其他团队」结对编程 总之： 限界上下文可以和「技术组件」对应起来（例如，「技术组件」可以是一个代码级别的项目工程）；但限界上下文的确定必须是由通用语言所驱动 设计的最终目的是使得每个限界上下文能和各自对应的子域形成一一对应的关系子域和限界上下文的区别 子域是问题空间的概念，子域用于在问题域打破复杂度 建立领域模型的目的是满足子域的「User Cases」。现实中，一个子域可能包含多个「模型」，单个「模型」也可能横跨多个子域（常见于涉及到遗留系统的场景） 限界上下文是解空间的概念；限界上下文是在应用软件内部对「模型」进行分割的具体技术实现 限界上下文是代码相关的：限界上下文从展示层，到领域逻辑层，到持久层，到数据存储层进行了代码级别的垂直切分 根据各个限界上下文的不同，可以采用不同的「架构模式」限界上下文的实现 限界上下文实现起来并不一定都要是一个单独的程序（微服务），其实可能是一套算法、或者是一个代码模块……各种可能都有 在「战略设计」阶段除了要划分出若干子域，也需要识别和划分出限界上下文 在画图时，对限界上下文使用实线（物理概念） 确认哪些领域模型的 Building Blocks 可以被加到当前的限界上下文的依据是通用语言。相反，如果我们依据「架构」和「开发任务」（即开发资源）等不正确的依据来判断需要加到限界上下文的 Building Blocks，会得到错误的「上下文边界」 限界上下文中并不仅仅只是包含模型。除了模型之外，它还包含： 如果「建模团队」也负责数据库 schema 的设计，那么数据库 schema 也术语这个限界上下文（但如果数据库 schema 由另外的团队负责，并已经存在，则不属于这个限界上下文） UI 界面也和数据库 schema 类似，有可以属于限界上下文 面向服务的组件也可以属于限界上下文 应用服务也可以属于限界上下文 我们倾向于使用多个小「模型」，而非单一大「模型」 DDD 鼓励把一个大的系统分割为多个子系统，尽量避免把单个「模型」用于整个系统 因为，如果使用单个「模型」，单个「模型」过于庞大，容纳了太多的「领域概念」，随着系统的不断增长，单个「模型」会更加复杂，变得难于管理 多个「开发团队」工作在单个「模型」，随着系统的增长，工作成本越来越高，效率越来越低 很多时候，同样的「名称」在不同的「上下文」其实代表了不同的含义，所以多个小「模型」比单一大 Model 更加有效 类似的，问题域中的单个「实体」也可能代表不同的「领域概念」 其他一些实践经验： 有时候要通过重构把两个原先设计不合理，交错在同个限界上下文的 2 个子域分到 2 个不同的限界上下文。这个时候可以采用两种「中间方案」： 一种是可以隔离出一个新的 Repository 层 另外一种是采用「隔离内核」的方法，我们倾向于这种方法 使用「隔离内核」的时机是当你有一个非常重要的限界上下文，但是其中模型的关键部分却被大量的起辅助作用的功能所掩盖 注意：使用「隔离内核」只是中间方案，「隔离内核」为下一步把两个交错在一起的领域模型彻底分开到 2 个不同的限界上下文打下了基础 要避免发生谷仓效应。例如：不要系统里面到处都是验证身份的代码，还是应该把验证身份的代码内聚到一个单独的身份验证限界上下文中 实现上，限界上下文可以和「技术组件」对应起来。例如，一个限界上下文可以对应一个 Project（Intellij IDEA 中的 Project）。而这个工程中，可以划分出不同的 packages（Java 项目），「领域模型」、「界面」、「应用服务」等系统模块就放在一个 package 里面。上下文映射 上下文映射（Context Map）：其职责是确保系统各个上下文之间的边界被明确定义，并且每个团队都理解他们之间的契合点 很多时候，Context Map 都不是文档，就是手绘的图。只需要简单的表示出各个限界上下文之间的集成关系：上下游关系，以及集成种类 Context Map 表示的是现实的状态，而不是未来的理想状态 上下文关系之间关系的种类： 防止损坏层：在集成其他子系统时，为了避免其他子系统中的模型破坏当前系统的模型，增加防止损坏层来进行隔离。该层主要是做转换工作 共享内核：在具有大量交错的独立限界上下文中，提取出一个供 2 个团队共享的模型是一种交接方法 该方案的风险在于：「共享模型」中的任何修改都可能影响到 2 个团队，需要测试充分后才能变更 开放宿主服务（Open Host Service）：当前模型的多个使用者可以共享一套模型转换逻辑，可以把这层转换逻辑提取出来形成「开放宿主服务」供多个使用者使用 Separate Ways（互不相干）：有时候系统之间的沟通互不相干也是一种选择，而是采用非系统的手工方式来沟通 Partnership：2 个团队协作工作，2 个系统的发布保持同步。2 个团队的成功与否绑定在一起 上下游关系：又分为 2 种「上下游」关系，「客户-供应商」关系和「遵从」关系（Conformist） 在绘制「上下文映射」图时，可以标明「上下游」，以及「上下文」之间是哪种关系应用架构 DDD 不限制具体使用哪种「应用架构」，DDD 强调的是：无论使用哪种「应用架构」都要有对「领域逻辑」进行隔离的能力 「应用架构」要能够分离「技术复杂度」和「领域复杂度」 「应用架构」要能够对「复杂的领域细节」进行抽象和封装：在粗粒度上实现「User Cases」 可以考虑使用「分层架构」。其中关键有 2 层： 「领域层」包含所有核心业务逻辑；「领域层」不和任何技术实现相关，也不需要了解任何持久化的具体实现 「应用层」表示各种外部要访问的用例；用「应用层」包裹住「领域层」 其他层次重要性比这 2 个核心层要低，只是起到支持作用 「领域层」不依赖任何其他层次；「应用层」只依赖领域层 例如，如果「领域层」中的「领域对象」需要持久化，实现的时候不能把「领域层」相关的代码和「持久化」的代码耦合在一起，而是从「应用层」来暴露一个用于存储「领域对象」的接口来实现（可以使用「依赖倒置」的方式来达到这个目的） 「应用层」表示应用程序的用例和行为；「应用层」把应用的逻辑代理给「领域层」和「基础设施层」来实现 「应用层」隐藏所有「领域层」的工作细节 「应用层」的工作可能会依赖除了「领域层」之外的其他层（例如：「基础设施层」），但代码实现上又不能依赖其他层，所以会采用「依赖倒置」的方式编码 从某种意义上来说，「应用层」就是「领域层」的「防止损坏层」，避免技术实现细节的变动影响到「领域逻辑」 「基础设施层」（Infrastructural Layer）包含整个应用程序工作的技术实现细节；「基础设施层」完全不涉及任何「业务逻辑」和「模型行为」 此外，「基础设施层」可以为日志，安全，通知，以及和其他限界上下文或「应用程序」的集成提供能力 「跨层通讯层」：跨层间通讯的时候，不要直接传输「领域对象」而是要遵从低层次的协议，用 DTO 对象进行传输 既然「领域层」不依赖任何其他层次，测试时，可以把「领域层」和其他层隔离开来，进行单独的测试 多个限界上下文不要通过「共享数据库」数据的方式来通讯，而是应该遵循「分层架构」，由「应用层」暴露接口来实现限界上下文之间的通讯 总结：使用 DDD，一般应用架构会有 2 个核心层次：「领域层」和「应用层」。在具有这 2 个核心层次的前提下，可能的应用架构有 2 种：「4 层架构」和「六边型架构」。 「4 层架构」。从下到上分别有：「基础设施层」、「领域层」、「应用层」、「接口层」 「六边型架构」：从内到外分别有：「领域层」、「应用层」、「基础设施层」、「对外适配层」 和「4 层架构」相比，在「六边型架构」中，「领域层」不依赖其它层次（使用「依赖倒置」），更加纯粹，开发团队可专注于「领域模型」。应用服务层最后在专门说明一下「应用服务」（Application Service），属于上面提到的「应用层」。 「应用服务」对外暴露该应用程序可用的业务能力，而这些业务能力是由系统需要满足的「业务用例」所定义的。同时，对外暴露的协议不能受调用方（client 方）的影响，只能是暴露「User Cases」，和界面展现无关 简单点说，就是「领域逻辑」和「User Cases」之间还存在「Gap」，需要应用服务层来填补这个「Gap」：安全、限流、日志、事务、权限、定时任务、外部事件处理等工作的实现 「应用服务」对「领域层」进行抽象，并对其复杂性进行封装 「应用服务」只包含「应用逻辑」，而不应该存在任何「领域逻辑」 「应用服务」需要跟上最新的技术潮流，并保持精简；但不会依赖任何程序框架或技术 「应用服务」的功能有点像 MVC 模式中「控制器」，都是「过程式」而非「面向对象」的代码 「应用服务」也可以用于验证该子域是否真的需要一个领域模型：如果「应用服务」的 API 过于简单（只是简单的增删改查），那么未必需要领域模型，用「事务脚本」之类的方案可能就够了再说明一下应用层的具体逻辑。应用层逻辑就是用来满足「业务 User Cases」的一个工作流，工作流中包含多个步骤。这些步骤有： 从持久层（Database）拿出「领域对象」 把用户的输入翻译成「领域层」能理解的「领域对象」 把用户请求转给「领域层」执行「业务逻辑」，拿到执行结果，并返回业务结果给用户 另外也包含需要调用「基础设施层」来工作的步骤（消息，日志，通知，认证等） 如果一个「业务功能」需要跨多个限界上下文才能完成，就不是单个应用服务层能完成的了，需要一个无状态的「过程管理器」（Process Manager）来协调多个限界上下文来完成DDD 的常见问题 过分强调战术模式：技术人员倾向于各种「技术模式」并埋头写代码；而事实上，一旦问题定义清楚，写代码真心不是大问题 对所有的限界上下文使用同样的「架构」；而事实上，对复杂度不高的限界上下文可以使用简单的架构 花了过多的精力和时间在把战术模式的实现得更完美上面了；而事实上，这些模式只是参考，不是规则，必须了解什么时候可以不完全按这些「模式」来实施；明白这些「模式」能解决什么问题，比掌握怎么用代码实现这些「模式」更重要 在使用了 DDD 的「简化版」的同时，又忽略了 DDD 的原则，只是把精力主要放在编码上。没有用到「协作」，「知识提取」，领域模型设计，提取 UL 等战略模式 忽略了 DDD 真正的价值所在：「协作」，「沟通」，「上下文」 低估了划分「上下文」的重要性，从而导致应用软件成为一个「大泥球」 UL 定义得不完善，从而导致了歧义和误解：「沟通」是解决问题域的基础，UL 不完善的话，就不能好的解决问题 缺乏「协作」和「沟通」，从而实现的方案过于技术化，并不能真正的解决问题（为了技术而技术） 「沟通」不充分，往往会造成对问题域一些潜在「领域逻辑」的忽视 不能接受不完美；事实上，要把「核心子域」划分出来，并把精力集中投入到「核心子域」上 把一些「简单问题」复杂化了： 简单，或者不重要的子域其实不需要实施 DDD 原则；也没有必要对所有的限界上下文使用「模型驱动设计」模式 不敢使用 CRUD 模式；在合适的场合，使用 CRUD 模式其实是简单而有效的 总之，在引入复杂之前，先评估一下付出的代价是否能匹配获得的好处 低估了实施 DDD 的成本。事实上，实施 DDD 的要求是相当高的： 对整个团队有非常高的要求 领域专家要愿意投入大量精力，并愿意协同工作 需要持续迭代对设计进行打磨 对简单问题使用 DDD 反而可能拖慢整体的进度 DDD 的实施 首先强调一点：不是无脑推广使用 DDD；而是在了解 DDD 的适用场景和 DDD 的优势后，再判断是否要使用 DDD 也要强调开发团队的学习；特别是对 DDD战略模式的学习和使用 同时，相对于向「甲方」推销 DDD 的概念，更重要的是，找到「业务专家」和他们随时随地的协同工作DDD 的实施过程： 「产品愿景」阶段：先要通过和「甲方」沟通，搞清楚「产品愿景」。可以通过一些问题来达到这一目标： 产品的业务目标（或者说，业务驱动力）是什么？ 产品能给业务带来什么价值？ 怎么确定这个产品一定能成功？如果成功的话，产品看起来会是什么样子？ 和现存的类似产品有什么本质不同？ 「需求收集」阶段：在找「甲方」收集需求的时候，可以使用 BDD 最后收集到的是 User-Cases、Input、Output 收集需求的过程中，也会形成 UL 「需求收集」阶段只专注于问题域，并不急于投入到「解决域」 如果需求过于庞大，可以考虑对问题空间进行提炼，也就是把问题抽象到更高的层次，并新建子域，然后再识别出「核心域」，「通用域」，「支撑域」 把主要精力投入到「核心域」；尽量把「核心域」做得小而美 「边界划分」阶段：在解空间进行建模之前，先要划分出限界上下文，并建立 Context Map 在「建模」之前，要先搞清楚系统的大环境的状况，当前产品在整个体系中如何存在 要识别出各个限界上下文，以及各个「上下文」和当前产品如何相互作用，也就是要建立「上下文映射」（Context Map） 「边界划分」（建立 Context Map）具体工作包括： 在问题域上，确认本团队负责的「模型」 画出各个「上下文」、对各个「上下文」命名、标明各个「上下文」的负责人 确认「上下文映射」映射时的各个「集成点」 确认「上下文映射」映射时需要交互的数据 标明「上下文」之间的相互关系的种类（梳理上下游） 持续收集当前产品所处的整个体系大环境的信息 另外，Context Map 就是「作战地图」；不需要太复杂，只要能快速准确的表达出映射关系就行了 「建模阶段」：建模时要牢记 DDD 的原则，只有在合适的场景才使用 DDD，否则就是浪费资源。建模时的一些建议： 问题域的不同部分有不同的重要性，只是对重要的部分实施 DDD。要选择好自己的主战场 和「领域专家」一起协作，进行「建模」 「建模」工作的粒度是选择一个「行为」，并围绕着该行为具体的场景；而不是对整个问题域进行建模 所谓的「行为」就是：Given-When-Then 「建模」工作要从最重要，最有价值的场景开始，而不是从最容易「建模」的场景开始；先要去解决整个「系统」最核心的部分 在「建模」阶段要持续对 UL 进行迭代。尽管 UL 是在「需求收集」阶段创建的，但需要持续迭代，消除歧义；当然，UL 进化的同时，代码也要同时迭代，保持和 UL 的一致 弃掉前 2 个模型；前 2 个模型往往都是在对问题域理解不够深刻时产出的，弃掉并不可惜 「模型」产出后，就是着手通过代码来实现该「模型」；但如果在实现过程中，发现「模型」不准确的话，需要反过来修订「模型」。总之，「模型」和「代码」要不断迭代，一起进化 领域模型建立以后要能在具体的业务场景中作为例子来验证。领域模型需遵守 2 原则： Be relevant：Able to answer questions in the problem domain and enforce rules and invariants Be expressive：Able to communicate to developers exactly why it’s doing what it’s doing 保持「模型」专注于单个问题，同时其代码实现也就应该是简单的（一旦发现代码陷入复杂就需要引起警惕） 如果存在「遗留代码」，不要把精力投入到「遗留代码」；而是给「遗留代码」划分一个清晰的边界，保护「遗留代码」不会影响到其他部分 尽早的做集成，持续的做集成。通过集成，可以尽早不断的对「模型」进行检验 「非技术性重构」：不是在技术层面做重构，而是通过对「领域」更深层次的认识，更新「领域知识」，对「模型」进行迭代，再实施对应的代码重构 当「模型」变得更庞大时，要考虑对解空间进行分解。相对于对代码精雕细琢，更值得去追求完美的边界划分 「建模」也是不断迭代，核心模型至少需要制作出 3 个版本才能最后达到目标。所以，代码实现也需要相应的比较灵活，要能适应「模型」的持续迭代 「建模」过程中要注意把深层次的「隐式规则」发掘出来，更新到领域模型中，并明确的实现在代码中，消除歧义 注意给新的概念命名，随时更新 UL Part 3: 战略模式从本节开始详细介绍 DDD 的战略模式（strategic pattern）。集成限界上下文在识别出限界上下文以后，就可以开始集成它们，让它们配合起来实现完整的 User Cases。也就是如何把多个限界上下文集成起来构建一个分布式系统。所以，实现上最大的挑战就是如何实现一个可靠的分布式系统（包括技术上的挑战，以及组织上的挑战），下面重点就会针对这些内容来介绍。在实现分布式系统时，可以考虑使用 SOA （Service Oriented Architecture）模式。另外，细节上也可以考虑使用响应式的事件驱动编程。实现分布式系统时，对 SLA （service level agreements）的定义也是必要的。下面几个小节会详细介绍。如何集成限界上下文先要确定软件服务之间应该采用哪种通讯方式。代码实现上，要注意保持限界上下文之间的「松耦合」。要注意使用 SRP 原则：不同的限界上下文专注各自的职责。可以把所有「上下文」的代码放在同个代码库中；也把不同的「上下文」的代码放在不同的代码库中。哪种好就用哪种，没有强制的规定。但使用单代码库时，需要考虑可能的一些问题： 如果不同的「上下文」的代码在同个代码库中，需要避免修改一个「上下文」代码的时候，不小心改动到其他「上下文」的代码 尽量避免使用「数据库集成」模式。使用「数据库集成」会使得各个「上下文」紧密的耦合在一起 不同「上下文」代码都在同个代码库中还会有一个问题：可能需要开多个分支并行工作，各个分支间的协调管理比较麻烦。发布上也不好协调，持续集成变得难以实施 要注意维护「上下文」的边界，这比代码的美观更重要（必要时可能需要把相似的代码复制到不同的「上下文」）在处理「遗留系统」时，可以采用一些常用的模式： Bubble Context 模式：新的代码被隔离在「气泡上下文」中，不直接和「遗留系统」接触。对应的，也需要给「遗留系统」创建一个 ACL 层（防腐层），ACL 层用于对「气泡」中的模型和「遗留系统」做双向翻译 Autonomous Bubble Context 模式：技术上实现一个更加「独立自主」的「气泡上下文」。这种「气泡上下文」往往采用「异步」的方式来和「遗留系统」进行通讯，当时，实现上也会更加复杂 Exposing Legacy Systems as Services 模式：把「遗留系统」封装成服务暴露出来。当多个「上下文」都需要使用同一个「遗留系统」时，封装成服务暴露出来可能更加有效；但实现上要考量的方面也比较多集成分布式限界上下文常用的分布式「上下文」的集成方式是 RPC，不过「纯文件共享」和「数据库集成」在有时候也适用。总之，要从「可扩展性」、「可用性」、「可靠性」（这里的「可靠性」指的是系统如何容错）等方面来考量如何进行集成。当然，如何选择也要综合考量「业务需求」和「成本」等因素。「纯文件共享」和「数据库集成」可以在简单的系统中使用，但当系统变得更加复杂时就不再适用了。RPC（或 REST）才是王道。但使用 RPC 时也要注意一些问题： 要考虑网络不可用等错误出现时的容错方案（技术上或业务上的容错） 使用 RPC 时，上下游系统的耦合是比较紧密的 分布式系统中，必须关注如何合理处理「分布式事务」（能忍受不一致，或者能实现「最终一致」就够了）响应式消息驱动也是一种选择。不过由于是新技术，实现上要付出的代价可能会更高一点。SOA 架构既然 SOA 的核心思想是基于「松耦合」给「业务」和「技术」提供好处。所以可以考虑使用 SOA 架构（或者说是「微服务架构」）结合「响应式编程」来实现集成的松耦合： SOA 中的服务是高度自治的，能够在不依赖其他服务的条件下运行，所以可以把 SOA 的原则应用到集成限界上下文 相对于「响应式编程」是在低层次上（代码上）实现「松耦合」的目的；SOA 是在高层次上（业务上）实现「松耦合」 把 SOA 的原则应用到 DDD 当中，就是考虑把限界上下文看成是 SOA 服务，这样限界上下文就有可能被分解为「业务组件」 把限界上下文所承担的每个「职责」封装成一个个的「业务组件」 各个「业务组件」对应代码中的一个「模块」 而各个「业务组件」是相互松耦合的（无 RPC 调用，无数据库共享），便于维护 注意，这里的 SOA 中的「服务」和「业务组件」都是逻辑概念，「逻辑概念」仅仅用于隔离，从而到达「松耦合」的目的 「业务组件」进一步分解为「组件」：「业务组件」需要处理各种「业务消息」，可以使用不同的「微服务」（或者「组件」）来处理不同的「业务消息」（利用响应式编程来处理消息） 以「微服务」（或者「组件」）作为部署单元，这样可以优化对硬件的使用率。而「微服务」（或者「组件」）之间通讯比较灵活，可以采用数据库共享的方式来通讯 下图是一个限界上下文中存在多个「业务组件」的例子： 「物流上下文」中存在「高优先级物流组件」和「普通优先级物流组件」2 个「业务组件」 「业务组件」间不会共享 DB，而是每个「业务组件」拥有独立的 DB 而每个「业务组件」又分成了 2 个「组件」：「发货组件」和「撤销组件」。C4Context title Decomposing the shipping bounded context into business components. Enterprise_Boundary(b0, &quot;Shipping Bounded Context&quot;) { Enterprise_Boundary(b1, &quot;Priority Shipping Component&quot;) { System(SystemA, &quot;Arrange Shipping Component&quot;) System(SystemB, &quot;Cancel Shipping Component&quot;) SystemDb(SystemDB1, &quot;Priority Shipping Database&quot;) } Enterprise_Boundary(b3, &quot;Standary Shipping Component&quot;) { System(SystemC, &quot;Arrange Shipping Component&quot;) System(SystemD, &quot;Cancel Shipping Component&quot;) SystemDb(SystemDB2, &quot;Standary Shipping Database&quot;) } }总结： SOA 架构是把限界上下文分解为「业务组件」；「业务组件」又分解为「微服务」通过消息系统集成利用消息系统的异步编程模式可以建立响应式应用。Message Bus：消息总线；系统中的所有「组件」（就是「微服务」）都通过「消息总线」来发送和接收消息。通过「消息总线」实现了去「中心化」。消息中间件要保证：消息的「可靠性」、消息的持久化。消息系统实战： 先收集「领域事件」，典型的「领域事件」例如：下单，付款，发货。 「领域事件」可以形成部分 UL；同时，把「领域事件」拼起来可以得到完整的 User Cases 和「领域专家」协同工作提取领域知识，并画出「组件图」 「组件图」不需要什么标准格式，也不需要太细节，只要能在沟通中使用「线」，「框」来描述「领域事件」和「过程」 「组件图」还需要表示出在限界上下文之间的「消息流」 「容器图」（Container Diagram）。利用「容器图」回答以下问题： 显示系统不同部分是如何分组的 不同部分是如何通讯的 系统的主要技术选型是什么 系统「架构」需要持续演进：新知识更新到系统中，新事件被添加到领域中，调整上下文边界，同时各种示意图也保持相应的调整 在各个「上下文」内部保存数据的本地副本是一个好的选择：付出一点存储的代价（存储的成本其实不高），降低系统的耦合 推荐各个不同的「业务组件」都拥有自己的私有 DB；同时，每个「业务组件」都通过公开的 API 开放自己的数据 「消息系统」的维护： 合理的设计「消息」格式；可以通过「消息版本」的方式来保持系统更新时的向后兼容 制定系统 SLA，并对其进行监控 系统要具备 Scaling Out 能力 如果要把 2 个使用了不同「消息中间件」的系统集成在一起，可以考虑使用「消息桥接」模式 「消息桥接」的实现可以是多做形式（例如采用数据库共享的方式来实现） Part 4: 战术模式从本节开始详细介绍 DDD 的战术模式（tactical pattern）。Building Block Patterns 为了让开发团队能有效的实现和「分析模型」对应的领域模型，提出了战术模式 战术模式是「创建领域模型」的 Building Block Patterns DDD Building Block Patterns 提供了一系列的模式，通过这些模式，我们可以更加有效的创建领域模型 所以战术模式（tactical patterns）也就是 Building Block Patterns 但要注意的是，战术模式不是教条的，其本身是在不断发展的。战术模式也只是提供指导，并不是规定。而且也不要只局限在几种固定的 Building Block Patterns，可以不断尝试各种新的模式 如何在代码实现中使用 UL 来表现领域概念才是战术模式的重点 每一种 DDD Building Block Patterns 表示领域中的一个概念（例如：Entity，Value Object……） 利用这些 Building Blocks 来构建丰富而有用的领域模型。常用的 Building Block Patterns 又分为 3 大类： 对领域建模的模式。代码中如何表达「模型元素」的模式，具体有 4 种： Entities Value Objects Domain Services Modules 生命周期模式。代码中创建对象和持久化对象（这些对象代表了领域的结构）的模式，具体有 3 种： Aggregates Factories Repositories 显露模式（Emerging Patterns）。「事件」相关的模式： Domain Events Event Sourcing 模型模式模型相关的模式有 4 种：Entities、Value Objects、Domain Services、Modules。EntitiesEntities 的关键特点是其具有「Identity」和「Continuity」。Entities 用于表示领域中可以按 identity 来定义的概念。「Continuity」也就是说 Entities 具有生命周期。在 Entities 的整个生命周期，它的 identity 是不变的，但同时，它的属性是可变的。Entities 一个显著的特点是当实现它的时候，会 override equality 方法。在 Entities 的实现上要注意： 考虑通过把 Value Objects 放到 Entities 中来实现的 Entities 的行为 也就是把「业务逻辑」放到 Value Objects 中后，再把 Value Objects 放到 Entities 中；而不是直接把「业务逻辑」放到 Entities 但这样也会带来一个问题：容易造成对象嵌套层次可能会比较深，所以对是否真的这样做要做权衡 Entities 要实现对 Entities 自身的约束（不变性）和校验（保证其「业务逻辑」正确） 「Tell Don’t Ask」：只对外暴露行为，而不是暴露内部的数据（或状态） 避免「建模」中的一个「误解」：DDD 是对现实世界建模 所以，Entities 不是描述现实世界的行为，而是要和所构建的「软件系统」的行为相关 分布式问题：如果是在一个分布式系统中，单个 Entities 中的属性可能来至于不同的限界上下文，所以当从「持久化」系统（DB）中「实例化」这个 Entities 时，需要考虑分布式问题。实现 Entities 时可以使用的一些常用 tips： 使用 Specifications：也就是把「业务规则」（Validation 或 Invariants）封装到小的、单职责的 class 中，这种 class 就是 Specification。例如，该 class 中可实现一个验证方法：bool IsSatisfiedBy(T Entity); 最好不要用一个 State class 来表达不同的「业务状态」；而是要给每个「业务状态」一个对应的 Entity 类 尽量不要使用 Getters 和 Setters 方法 实现 Entities 的方法时，尽量不要有 side effecting（也就是调用该方法时，会改变 Entities 的内部状态），而是要做到 effect‐freeValue ObjectsValue Objects 用于表示领域中只需要通过其「特征」来辨识的概念。也就是说，我们可以用 Value Objects 来描述领域中的「模型」；同时，Value Objects 可以不需要 identity。Value Objects 单独使用没有意义，必须是在某个上下文中使用，也就是配合其他对象使用（例如，作为 Entities 的属性）。Domain ServicesDomain Services 把不能放到 Entities 和 Value Objects 的「领域逻辑」封装起来。有 3 大特点： 需要是「领域逻辑」，也就是「业务逻辑」；如果不是「业务逻辑」就不能作为 Domain Services 和「业务逻辑」无关、偏向依赖「基础设施层」的服务应该是放到应用服务层 只和「行为」相关（无标识 id） 无状态 需要把多个 Entities 和「领域对象」协调在一起工作另外，在使用 Domain Services 的同时，又要避免使用「贫血模式」。和应用服务层的区别在于[应用服务层不代表「领域概念」，不表达「业务规则」；而 Domain Services 是用于表现「领域概念」的。Domain Services 有 2 大类： 运行 Entities 和 Value Objects 相关的行为。这类 Domain Services 是领域模型的一部分 作为 Contract，但不是领域模型的一部分，而是要依赖「基础设施层」来实现。也就是这部分「业务逻辑」是 UL 的一部分，但实现上和「领域模型」无关，而是和「基础设施层」相关 在这种场景 Domain Services 会实现在应用服务层，而不是「领域层」 其他一些注意事项： 如果 Entities 内部需要用到 Domain Services，可以考虑在构造 Entities 的时候，把 Domain Services 作为构造方法的参数传入 也可以选择使用「Dependency Injection」，而不是用构造函数的参数来实现 Entities 发布一个「领域事件」，然后 Domain Services 作为消费者来处理这个「领域事件」也是一种实现方法。这种方法的好处是不需要把 Domain Services 在构造时传入 Entities；坏处是相关的代码可能分布在多个地方，需要 tradeoff……ModulesModules 对应项目代码中的 namespaces 或 projects，用于封装各种相关的概念（Entities 和 Value Objects）到「模块」中。从而能够简化对大型领域的理解。生命周期模式生命周期模式有 3 种：Aggregates、Factories、Repositories。AggregatesEntities 和 Value Objects 相互合作会形成大的「领域对象」：「对象图结构」。DDD 使用 Aggregates 模式按照业务「复杂性」和「不变性」把大「对象图结构」切分为多个 Aggregates，并保证各个 Aggregates 的「一致性」，以及定义它们的「并行事务边界」。 要尽量的按「不变性」来分割 Aggregates，这样更容易保持「领域」的「一致性」。 「领域对象」之间的关联本质是为了支持「不变性」和「一致性」。确认 Aggregates 边界的一些特征： 一致性：识别出工作在一起的「领域对象」，同时这些「领域对象」必须保持一致来满足「用例」 保持「事务一致性」「对象图结构」是一个大的「领域模型」，如果不使用 Aggregates 模式的话，会难于管理。实现上，Aggregates 之间相互引用时，是通过其「根对象」的 Id 来引用，而不是通过「根对象」本身来引用。 Aggregates 「根对象」是对 Aggregates 进行操作的入口点。总结： Aggregates 有其边界 Aggregates 内部要保证「事务一致性」；所以，聚合不能跨限界上下文，这样避免了「分布式事务」 换句话说，Aggregates 中的 Entities 和 Value Objects 在活动中的变更保持一致 Aggregates 之间保证「最终一致性」即可FactoriesFactories 负责 Entities 和 Value Objects 的创建。封装「领域概念」创建的「复杂性」。构造领域对象的方法有 2 种： 工厂方法 抽象工厂方法：依据所提供的具体信息，工厂（Factories）就能判定要创建和返回哪个子类另外，可以考虑在「领域对象」中实现 Builder 模式来精简其构建代码。RepositoriesRepositories 模式用于抽象「模型」的「反持久化过程」和「持久化过程」，使得开发人员无需关注「持久化」相关的底层「基础设施」。换句话说，Repositories 确保了「持久化」的「抽象」（无需关注「持久化」的底层实现细节）。这里的「抽象」指的是：Repositories 利用 Facade 模式，把对领域中的 Aggregates 进行存储的工作 facade 到底层「基础设施层」机制来实现。也就是说在「领域模型」和「数据模型」（DB 中数据的存储模式）并不一定完全一致的情况下，Repositories 利用 Facade 模式隐藏掉「领域模型」和「数据模型」这 2 种模型之间的转换的复杂度。Repositories 也就是定义了「领域模型」和「数据模型」这 2 个模型的边界，从而实现了把「领域模型」和「数据模型」的存储解耦。显露模式Domain EventsDomain Events 就是领域中发生的，「领域专家」感兴趣的事情。Domain Events 并不是技术概念，和「事件驱动」什么的技术概念无关。Domain Events 一般从 Aggregates 产生，把在某个 Aggregates 上的变化发布出去，从而使得其他关注这个变化的 Aggregates 能够通过订阅的方式获取到。 Domain Events 的命名一般是：名词+动词的完成时，例如：OrderCanceledEvent（订单已取消事件） Domain Events 需要被持久化 Domain Events 对象包含的一些属性： 领域事件 ID 事件产生时间 Domain Events 技术上的发布可以采用的实现方式： API 调用 数据库流 消息队列 Events 实现上就是「不可变类」（数据对象） 这里说的 Domain Events 主要是指单个限界上下文中的事件，而且可能是同步的模式处理事件 处理 Events 可能是「调用领域逻辑」（和业务逻辑相关，可以是调用 Domain Services）；也可能是调用「应用程序逻辑」（和业务逻辑无关）事件风暴建模Domain Events 除了是一种战术模式，它也和一种重要的「领域建模方法」，事件风暴建模相关，这里重点介绍一下。事件风暴建模可以完成几类不同的任务： 获取整个领域的 Big Picture 明确业务处理流程 帮助开发人员完成软件设计事件风暴建模的应用场景： 评估已有业务线的健康度，并发现优化点 探索一个新业务模型的可行性 在已有项目里面发现具有最大利益的新服务 设计整洁可维护的软件，能支持快速推进的业务事件风暴建模的关键词： Domain Events Aggregates 「决策命令」：系统的「参与者」对「系统」发出的「指令」，从而引发「系统」状态的变化，也就是会触发 Domain Events 「角色」：触发「决策命令」的「角色」，可能是人，也可能是其他事件触发源 「读模型」：在「角色」发出「决策命令」之前可能需要先查询必要的「信息」，这个查询信息就是「读模型」 「策略」：对 Domain Events 采用何种「策略」进行响应 「外部系统」：整个过程中需要协作的「外部系统」 「问题/热点」：会对整个过程中发现的不确定点，或重要的点进行标注事件风暴建模关键步骤： 列出「领域事件」、选定「领域事件」 收集「关注点」和「问题」：「讨论点」、「问题点」、「假设」、「风险/关注点」 深入领域：确定「决策命令」、「角色」 找出 Aggregates：Aggregates 负责产生「事件」，也负责处理「事件」。Aggregates 连接了「决策命令」和「领域事件」 找出「读模型」 找出「策略」：响应「领域事件」、也会触发「领域事件」（触发「决策命令」）然后，沿着「时间线」确定下一个「领域事件」，再按同样的步骤对这个「领域事件」按步骤分析。Event SourcingEvent Sourcing 通过把「事件」按顺序存储起来的方式来记录 Entities 的状态，而不是直接对 Entities 的状态进行持久化。也就是说，所有活动的历史都会按时间线被记录下来，而不仅仅存储「领域对象」的当前状态。相对于只存储「领域对象」的当前状态，把「事件」按顺序存储起来的优势有： 获得分析完整历史数据的能力 时态查询（Temporal Queries）：可以获取到之前某个事件点「领域对象」的状态。例如，当系统出现问题时，能够更好的排查出问题的原因 投影（Projections）：把多个「事件流」投影（Projections）以后形成一个「事件流」以后，再在这个新的「事件流」上做数据分析查询等工作。例如：已经对每个用户的活动记录成了「事件流」，之后就可以做投影得到「年龄 20 岁以下，3月1号」的「用户事件」，然后对新的「事件流」进行分析 快照（Snapshots）：如果通过把「事件」回溯来重现某个时间点的状态，成本可能较高。所以可以考虑存储「快照」，在回溯状态时，可以直接加载「快照」 当使用 Event Sourcing 时，相应的，在设计 Aggregates 时也要让 Aggregates 和「事件」能配合起来：可以通过「事件流」来回溯 Aggregates 的状态。例如，Aggregates 中可包含几个属性和方法： Changes 属性：等待 Apply 的「事件」列表 Version 属性：一旦一个「事件」被 Apply，Version 号加 1 Apply 方法：Apply 事件（按照「业务规则」，不同的「事件」会有不同的 Apply 效果），Apply 之后，Aggregates 的状态也就会随之变化但对外暴露的「领域接口」应该只表达「领域概念」，外部接口无需关注 Aggregates 的实现是否和 Event Sourcing 相关，Aggregates 的内部实现细节不需要暴露给外部。另外就是，如上面提到的那样，Aggregates 要具有创建「快照」的能力。对应的，该 Aggregates 对应的 Repositories 需要支持 3 个操作： 创建流：以一组初始事件组成的流作为入参进行创建操作 Apply 到流：参考上面提到的「Apply」操作 加载到流：从事件存储中获取事件，并加载这些事件，从而恢复 Aggregates 到期望的状态（这里可能是一个性能损害操作，因此可以采用各种性能优化手段）" }, { "title": "Working on it", "url": "/posts/working-on-it/", "categories": "notes", "tags": "notes", "date": "2022-06-25 00:00:00 +0800", "snippet": "分 2 部分介绍： 「日常需求」的完整工作流程 「线上服务」的运维一. 流程前提： 效率相关（相关指标） 故障相关 定义什么是故障 定义故障等级（受影响用户数 + 时长） 设定目标 工作流程包括以下环节： 技术评审 变更管理： 代码分支管理 代码提交卡点：代码规范检测，安全审核，中间件版本检测，CR，单测通过。。。 数据库（或中间件）变更规范 系统变更管理 推进：开发环境 -&amp;gt; 联调环境 -&amp;gt; 测试环境 -&amp;gt; 预发环境 测试：自测 -&amp;gt; 联调 -&amp;gt; QA 测试 -&amp;gt; QA 验证通过 -&amp;gt; 是否压测？ 发布：灰度 -&amp;gt; 引流 -&amp;gt; 分批发布 回滚1. 技术评审 需求的开发负责人组织评审工作 形式可以多样，但涉及的相关人员都要确认（特别是 QA 人员的确认） 基于「技术评审」的结果确认所有工作的时间表（特别是 QA 人员的测试时间表）2. 变更管理 代码分支管理：主干开发模式 vs. 迭代分支模式 代码规范 &amp;amp; 安全审核 &amp;amp; 中间件版本检测 &amp;amp; 单测通过 CR 卡点时机？ 数据库变更规范 &amp;amp; 中间件变更规范 数据库/中间件/系统变更的回滚方案3. 环境 &amp;amp; 效率 建设稳定的开发/测试环境的目标是效率 自测环境 -&amp;gt; 开发效率 联调环境 -&amp;gt; 联调效率 测试环境 -&amp;gt; QA 测试效率 4. 测试流程 提测时间点 在哪个环境提测的讨论： 「测试环境」vs.「预发环境」（需要同时测试多个需求？） QA 人员发布「测试报告」 是否压测在「技术评审」时确定 5. 发布 「发布」和「运维」相互关联 「红线」 「发布方案」也是「技术评审」的一部分 依据「效率」&amp;amp;「故障」指标来制定对应的「发布流程规范」 设置各个发布阶段的卡点策略（灰度 -&amp;gt; 引流 -&amp;gt; 分批发布） 需要相关「运维」工具的支持 日志/监控/报警 限流/降级 回滚流程（应急手册 + 事先演练）6. 中间件的迭代 监控和统计 版本分布/接口分布（日志和统计接口） CR 策略 测试策略 回归测试/压力测试（自动化测试用例集合） 发布策略二. 运维1. 运维粒度 业务级别 / 服务级别 / 应用级别。。。 系统级别 / 单机级别 / 容器级别。。。2. 运维指标各个「运维粒度」对应的运维指标： 业务级别：业务指标 服务级别：QPS/耗时/失败率 应用级别：CPU / 负载 / 内存 / 错误数。。。 系统级别：上下游错误数 单机 / 容器级别：CPU / 负载 / 内存 / 错误数。。。但最重要的核心指标还是围绕服务级别：QPS/耗时/失败率3. 链路梳理 强弱依赖/系统瓶颈/系统容量 -&amp;gt; 压测 -&amp;gt; 风险评估 配合具体业务目标进行梳理 压测相关 压测的风险：也是故障的普遍来源之一 压测流量的区分 压测策略的选择（单机/接口级/应用级/单链路级/全链路级别） 4. 报警监控应急监控 -&amp;gt; 报警 -&amp;gt; 应急手册依赖各种运维工具： 监控相关 业务指标跟踪 链路跟踪 + Metric（基于中间件日志） 中间件日志/应用日志错误实时分析 系统指标监控（CPU，内存，负载，网络，JVM。。。） 报警相关 依据「故障」指标来对报警进行设置 核心指标：出现故障到相关人员上线处理的时长 应急相关（基于之前的「链路梳理」，「监控」，「报警」） 预案可能需要包括以下方面： 人员 &amp;amp; 沟通 限流 降级 回滚 应急演练（例如进行压测的同时，对各种预案进行演练） 三. 重点1. 代码规范检测Java 语言的话，可以使用一些静态代码分析工具，常用的有： 阿里编码规约 PMD SonarLint具体使用方法，一般有 2 种使用方法： 安装对应的 Intellij 插件。开发人员在编码和 CR 时，利用这些 IDE 插件确认代码能通过这些检测 利用对应的命令行工具（可能是 maven 插件的形式）对代码库中的代码自动扫描2. 技术评审「日常需求任务」的技术评审主要包括以下要素： 任务背景和任务内容的介绍 涉及到的系统的工作细节（例如：「时序图」） 涉及到的相关「接口」的文档 本次任务的实现方案（选择该方案的考量？计划怎么做？涉及哪些系统的改动？） 列举需要变更的各个系统的改动点（可以精确到代码级别） 各个改动的负责人也要在这个时候明确 埋点+统计方案和大概的验证方法（如果需要） 中间件的改动（如果需要。可能包括：DB，缓存，RPC。。。） 性能影响和压测方案（如果需要） 发布方案（如果需要） 风险评估及其应对方案（如果需要） 开发时间预估 特别是给出最终关键时间节点的「时间」 例如：「什么时候能确定提测时间」 3. 基础设施变更的回滚也就是类似「数据库变更」，「中间件版本变更」，「系统软件变更」等基础设施变更的回滚方法。先讲一下「数据库变更」的回滚： 所谓「数据库变更」最常见的场景其实就 3 种：新增表，加字段，改字段类型。这 3 种变更的回滚其实还好 「加索引」要谨慎（变更和回滚方案要和 DBA 确认） 其他不常见的场景，就需要事先和 DBA 制定详细的计划了（例如在「技术评审」阶段） 再讲一下「其他基础设施变更」的回滚，例如「中间件版本」，「系统软件」等设施的变更： 在做回滚之前，先要确认「交付物」是什么： 如果是当前流行的：镜像（Docker 镜像这种），Mesh，Serverless，比较简单，使用对应的成熟方案即可 如果还是传统的「二进制包」，就需要发布人员有一套稳定的「脚本工具」来实施对应的发布/回滚等工作 另外，回滚方案可能需要演练4. 中间件版本/接口管理方法如下： 中间件日志的目录（例如：/home/admin/logs）和应用服务的目录分开 一般中间件目录下会重点记录以下日志（这些日志会记录到不同的日志文件）： trace 信息 实施监控统计信息（看具体情况，可以到分钟级，30 秒级等。。。） 错误信息 然后可以通过这些日志掌握当前中间件运行的状况，基于这些状况可以实现监控，报警 甚至可以暴露 HTTP 接口来查询到中间件的运行状况有了以上这些单机的信息之后，就可以汇总得到中间件「可视化大盘」5. 监控指标这里重点介绍一下需要重点关注（监控+记录）的单机指标（有了单机指标后，汇总起来就能得到整体指标，并可以组合出各种报警）： 系统（物理机和容器）级别的指标： CPU 相关（用户级，系统级，等待，切换，steal。。。） 内存 相关 磁盘 相关 Load（负载） IO 相关：「网络流量」，「网络错误」，「磁盘 IO」，「磁盘错误」等 JVM 指标： GC 相关 堆相关 线程数 接口级别的指标（本身接口和依赖的上下游接口）： 实时流量（秒级，分钟级。。。） 实时失败数 实时失败率（「成功率」） 实时 RT（也就是延时）：平均，最大，百分比 应用级别的指标： 实时日志中的 error 数 实时业务指标 实时资损（如果有的话） 中间件级别的指标： 包括 DB，缓存，RPC，消息队列。。。 限流相关监控 测试用例定时执行（预发环境 + 线上环境）" }, { "title": "Build Systems", "url": "/posts/build-system/", "categories": "工具", "tags": "cmake, makefile, bazel, c", "date": "2022-05-06 00:00:00 +0800", "snippet": "一. CMake1. Modern CMakeModern CMake: 围绕着 targets 来管理应用的构建。同时，要成功构建 target，也需要配置 target 对应的 properties。 Target：target 就是被构建应用的各个组件的抽象。一个可执行文件是一个 target，一个库是一个 target 构建一个应用就是构建一系列 targets 的集合。而且这些 targets 可能互相依赖 Property：需要对被构建的 target 配置 properties，才能成功进行构建： target 是从哪些 source files 构建而来的 target 构建时的编译选项 target 需要链接哪些库 总之，Modern CMake 的目的就是创建一系列的 targets，同时定义这些 targets 必要的 properties。2. Properties ScopesTarget properties 可以被定义在 2 种 scopes 中：INTERFACE and PRIVATE： INTERFACE：暴露给使用该 target 的用户使用的 properties PRIVATE：只是在内部构建该 target 时使用的 properties另外，如果一个 property 同时是 INTERFACE 和 PRIVATE 的，那么这个 property 为 PUBLIC。同时，INTERFACE property 会被透传给 target 的使用者；相反的，PRIVATE 的 property 不会被透传给 target 的使用者。3. CMake 的职责管理 C/C++ 应用。其中包括对从构建到发布整个生命周期中的各个步骤进行管理： Compiling executables and libraries Managing dependencies Testing Installing Packaging Producing documentation Testing some more用户需要利用「CMake 脚本」来实现对应用的管理。内容为「CMake 脚本」的文件叫 listfile： CMakeLists.txt：在项目的顶层目录下，cmake 执行时从这个文件开始执行 以 cmake 为文件后缀的文件4. 最佳实践 把一个大的 Project 进行拆分；而且是拆分成不同的目录，每个目录为项目的一个组成部分。例如： 代码、测试、文档、外部依赖，脚本等 而每一个组成部分又可以进行进一步拆分，代码目录可以再拆分成库代码，执行程序代码等等 可以使用 CMake 提供的 add_subdirectory 命令实现项目的拆分 CMake 提供了一个 Unit Test 规范（CTest），只要按照 CTest 规范进行配置，可以集成各种 Unit Test 框架。从而实现 Unit Test 的自动化运行 推荐使用 GTest 框架，能够和 CMake 有很好的配合 C 语言的话，我现在使用 Tau 5. 更多细节我们给出了一个 CMake 项目的常用结构，可以直接参考（其中的 CMakeLists.txt 中有详细的注释说明）：https://github.com/darktea/cmake-full-project更多细节备忘： CMake 中可以使用 list；同时提供了 list 命令来操控 list。例如：# 把 &quot;${CMAKE_SOURCE_DIR}/cmake&quot; 添加（append）到 CMAKE_MODULE_PATH 路径中去# 其中：# CMAKE_SOURCE_DIR 是当前项目的顶级目录# CMAKE_MODULE_PATH 是 cmake module（cmake 模块）所在的目录list(APPEND CMAKE_MODULE_PATH &quot;${CMAKE_SOURCE_DIR}/cmake&quot;) CMake 的变量（variable）的作用域（scope）比较复杂；分为 2 种作用域： Function Scope Directory Scope 我们这里只简单的通过一个例子简单说明一下 Function 作用域：cmake_minimum_required(VERSION 3.20.0)project(Scope)function(Inner)# 进入 inner 函数后，变量 V 的值还是上一层函数的值 2 message(&quot; &amp;gt; Inner: ${V}&quot;)# 在函数中使用 set 命令会把 V 的值改为 3 set(V 3)# V 的当前值为 3 message(&quot; &amp;lt; Inner: ${V}&quot;)endfunction()function(Outer)# 进入函数后，全局变量 V 的值不变，还是 1 message(&quot; &amp;gt; Outer: ${V}&quot;)# 在函数中使用 set 命令会把 V 的值改为 2 set(V 2)# 调用下一级函数 inner Inner()# 从 inner 函数返回以后，V 的值恢复到 2 message(&quot; &amp;lt; Outer: ${V}&quot;)endfunction()# 设置全局变量 V 的值为 1set(V 1)message(&quot;&amp;gt; Global: ${V}&quot;)# 调用函数 OuterOuter()# 从函数 Outer 返回后，V 的值恢复到 1message(&quot;&amp;lt; Global: ${V}&quot;)运行这个例子可以得到结果：&amp;gt; Global: 1 &amp;gt; Outer: 1 &amp;gt; Inner: 2 &amp;lt; Inner: 3 &amp;lt; Outer: 2&amp;lt; Global: 1另外，如果上面例子的 inner 函数改为：function(Inner) message(&quot; &amp;gt; Inner: ${V}&quot;)# 使用 PARENT_SCOPE 修改上一层函数中 V 的值；# 但不会修改本函数和全局的 V 的值 set(V 3 PARENT_SCOPE) message(&quot; &amp;lt; Inner: ${V}&quot;)endfunction()运行结果如下（Outer 函数的打印为 Out: 3）：&amp;gt; Global: 1 &amp;gt; Outer: 1 &amp;gt; Inner: 2 &amp;lt; Inner: 2 &amp;lt; Outer: 3&amp;lt; Global: 16. Reference Modern CMake is like inheritance二. C 语言相关概念Value 从「可移植」方面来考量的话，就 C 语言中的数据来说，作为一个 programmer，应该尽可能的只关注 value 本身，而不是 value 的具体 representation C programs primarily reason about value and NOT about their representation. 这个道理就类似现实世界中，同一个数在不同的语言（例如用罗马数字来表示数字）中有不同的表达方式，但我们关注的是数本身，而不是表达方式 C 中，value 是抽象实体，抽象于特定的实现，和特定的运行之上 value 和 representation 之前的相互转换是编译器应该关注的事情 C 程序可以抽象成「有序抽象状态机」。「抽象状态机」的运行理想上是和平台无关的，「状态机」中和平台无关的相关概念包括 value、object、type： 所谓「有序抽象状态机」就是用来操作 value（在给定时刻，程序的变量，variable，拥有特定的 value） value：就是「值」的抽象概念，和具体的程序是无关的；而且 C 中的 value 都是「数值」（或者可以被转换为「数值」） 「状态机」状态的转换由 3 者决定： value、type、Binary RepresentationType type：C 语言中，type 是 value 的属性 所有的 value 的 type 都静态的决定（非运行时决定）；所谓的「静态类型」 在一个 value 上可以执行什么操作，由 value 的 type 决定 value 上操作的结果是什么会被 type 决定（但也不能完全被 type 决定，结果也受 Binary Representation 的影响） 编译器是否能做一些优化，也受到 type 的影响（例如，对 unsigned int 上的操作做优化时，可以不考虑 overflow） Object澄清几个概念： object：可以被看成一种 box 本身 identifier：用来标识 box 的 name type：如果 object 看成是 box 的话，那么 type 就是 box 的 specification value：如果 object 看成 box 的话，value 就是放在 box 中的 content 赋值语句可以理解为：把 value 赋值给 object（把 content 放进 box） C 语言中，可能是不需要引入「左值」、「右值」的概念；没有那么复杂，直接用 object、value 的概念就行了。 为什么？请参考下面的讨论……C 语言里面的「左右值」的讨论： 先不考虑 function 的场景，C 语言中的 lvalue 中的 “l” 代表的意思不是「左」的意思，是「地址可被定位」的意思： lvalue simply means an object that has an identifiable location in memory 「左值」就是 object 所谓「右值」，在 C 语言中，其实是「非左值」 一个 C 语言中「非左值」的例子：typedef struct S { int i;} S;S f() { S s; s.i = 123; return s;}int main() { S s; // 这里的 s 是「左值」 s = f(); // 这里的 f() 就是一个「非左值」}Binary Representation Binary Representation：type 的二进制表示。例如对一个 16 位的 unsigned int，其二进制表示就很简单：直接用 16 位 bits（b0, b1…b15）表示就行了。一般来说是具体平台无关，也是一种可抽象的概念 但由于各种平台的差异，C 的标准并不能完全控制所有的具体实现细节；也就是说，即使遵循了 C 标准，也不能完全保证同样的操作在所有平台上的结果完全一致。 Binary Representation 也会对在 value 上的操作的结果造成影响 但这里的 Binary Representation 还是一个抽象的概念，并没有决定 value 物理上具体怎么存储的。value 物理上怎么存储的是 Object Representation 相关的概念 举个例子： C 标准说明了 size_t 这个 type 是一个大于 0 的整数，据此可以推演出 size_t 这个 type 的各种操作；但同时，由于在不同的平台上，size_t 的 SIZE_MAX 并不一样，所以在 size_t 上的操作结果也是和平台相关的，并不能仅仅由 type 就决定其操作结果；和平台相关的部分需要由 Binary Representation 来决定。总之： 「抽象状态机」状态的转换由 3 者决定： value、type、Binary Representation其他一些 tips： 尽量使用 unsigned 类型，避免可能的 UB 行为 尽量使用 uint8_t, uint16_t, uint32_t……Object Representation Object Representation 是和平台相关的概念，不是抽象的，具体由编译器来决定总之， 「抽象状态机」状态的迁移只由 3 者决定：value，type，Object RepresentationDerived data types4 种非基本类型： 2 种组合类型 Array Structure 2 种非组合类型 Pointer Union Array Array 作为函数参数时，数组的长度信息会丢失，但可以使用 static 关键字进行说明。例如：// static 2 表示作为函数入参的数组的程度「大于等于」 2void swap_double(double a[static 2]) { double tmp = a[0]; a[0] = a[1]; a[1] = tmp;} void func(double a[static 7]); 中的 static 7 标明 func 的参数是一个至少包含 7 个元素的数组（或指针） 「字符串」是最后一个元素为 0 的 char 数组。下面分别给出 2 类例子，第一类是「字符串」，第二类不是「字符串」：// 下面 4 个是字符串（最后一个元素为 0）char jay0[] = ”jay”;char jay1[] = { ”jay” };char jay2[] = { &#39;j&#39;, &#39;a&#39;, &#39;y&#39;, 0, };char jay3[4] = { &#39;j&#39;, &#39;a&#39;, &#39;y&#39;, };// 下面 2 个不是字符串char jay4[3] = { &#39;j&#39;, &#39;a&#39;, &#39;y&#39;, };char jay5[3] = ”jay”; 对字符串函数（例如：strlen(s)）使用非字符串是 UB2 种数组：FLA 和 VLA（VLA 是到了 C99 才有的）VLA 的一些特点： VLA 没有初始化 VLA 不能在 function 之外定义（只能在函数内部定义使用）VLA 的一个声明： void func(size_t n, double a[n]);Structure 在声明 struct 时，使用 typedef，起到简化的作用（不需要每次都要加上 struct 关键字）：typedef struct toto toto;struct toto { // ...};// 不需要加上 structtoto one; opaque structure：就是不在头文件里面定义 struct。只在头文件里面声明 struct，同时头文件里面的函数的声明只会使用到这个 struct 的指针。例如：typedef struct toto toto;void toto_doit(toto*, unsigned); 但是，注意，不要 typedef 指向 struct 的指针// 不要这样做typedef struct toto_s* toto;void toto_doit(toto, unsigned);线程本节介绍 C11 标准里使用线程时会用到的几个基本库函数：#include &amp;lt;threads.h&amp;gt;typedef int (*thrd_start_t)(void*);/* * thrd_create 的 3 个入参说明如下： * thrd_t*：线程创建成功后的线程 id * thrd_start_t：线程执行的函数 * void* 需要传入给线程的数据 */int thrd_create(thrd_t*, thrd_start_t, void*);int thrd_join(thrd_t, int *);一般的使用方法： 在 main 函数中，使用 thrd_create 函数对不同的任务创建相应的线程 然后在 main 函数中使用 thrd_join 对这些线程做 join，等待这些线程的结束 同时，某个线程执行其对应的 thrd_start_t 函数，直到这个函数 return，该线程的工作结束一个简单的例子：/* 先准备好需要提供给线程的数据： Create an object that holds the game&#39;s data. */life L = LIFE_INITIALIZER;life_init(&amp;amp;L, n0, n1, M);/* Creates four threads that all operate on that same object and collects their IDs in ”thrd” */thrd_t thrd[4];thrd_create(&amp;amp;thrd[0], update_thread, &amp;amp;L);thrd_create(&amp;amp;thrd[1], draw_thread, &amp;amp;L);thrd_create(&amp;amp;thrd[2], input_thread, &amp;amp;L);thrd_create(&amp;amp;thrd[3], account_thread, &amp;amp;L);/* Waits for the update thread to terminate */thrd_join(thrd[0], 0);/* Tells everybody that the game is over */L.finished = true;ungetc(&#39;q&#39;, stdin);/* Waits for the other threads */thrd_join(thrd[1], 0);thrd_join(thrd[2], 0);thrd_join(thrd[3], 0);// 只有当这 4 个线程都结束时，main 函数才会结束使用线程的要点： 如果某个线程对一个「非原子」变量进行了写操作，那么其他线程同时对这个变量的「读写操作」会导致线程执行的 UB（未定义行为） 使用 _Atomic(T) 语法定义「原子」变量。具体用法参考文档，这里只给简单的例子： // 不能作用到数组上。Invalid: atomic cannot be applied to arrays._Atomic(double[45]) C;// Valid: atomic can be applied to array base._Atomic(double) D[45];Tips类型 C 语言中，有几种类型是不能直接做算术操作的。所谓的 narrow types：unsigned char / unsigned short / char / signed char / signed short / bool 在算术表达式中，narrow types 会被做一次 promote，成为 signed int 类型（而不是 unsigned int）；所以干脆最好不要在算术表达式中使用 narrow types 尽量使用 unsigned 类型；同时不要在算术表达式中把 unsigned 类型和 signed 类型混用 C 标准中的 stdint.h 头文件里面定义了固定 size 的整数。例如：uint32_t，int32_t 等。但到了具体平台上，并不是每种这些类型都存在#include &amp;lt;inttypes.h&amp;gt;#include &amp;lt;stdio.h&amp;gt;uint32_t u = 23;// 需要使用 PRIu32 来 printf 类型为 uint32_t 的值printf(&quot;%&quot; PRIu32 &quot;\\n&quot;, u); 非 scalar 类型的初始化必须使用 {}。例如：double A[] = { 7.8, };double B[3] = { 2 * A[0], 7, 33, };double C[] = { [0] = 6, [3] = 1, }; 上面例子中数组 C 的初始化是 Designated Initializers（「指定初始化」，C99 标准支持）。尽量要使用 Designated Initializers 可以获得更好的 robust 另外，当不知道如何对类型 T 做初始化时，推荐使用 Default Initializers，{ 0 }。只要不是 VLA 类型，都可以使用 Default Initializers。例如： double C[] = { 0 }; 「只读字符串」建议采用 char const*const 的形式，例如：char const*const bird[3] = { ”raven”, ”magpie”, ”jay”,};Function Use EXIT_SUCCESS and EXIT_FAILURE as return values for main 函数 这样做为移植性考虑：main 函数在有的平台期望 return 一个整数，而有的平台不希望 return 值 满足 2 个特性的 function 叫 pure function： The function has no effects other than returning a value. The function return value only depends on its parameters. 当不能用 pure function 解决问题时，需要使用「指针」作为函数参数来解决问题 pure function 就是只用值来传递参数，函数内部对参数的修改不会影响函数返回后入参的值 如果 function 除了 return 之外，还有其他方式来改变「抽象状态机」的状态，那么就不是 pure function。例如： 修改了「全局变量」 使用了 static 变量 做了 IO 操作 指针 pointer 用来 hold objects 的地址。objects 的 names 就是 variables * 号用来定义指针类型时，最好靠左：int* p ＝ ＆n * 号用来「解引用」时，最好靠右：int i ＝ *p 如果一个指针指向一个 array object，那么 array object 的长度不能通过对这个指针做 sizeof 操作得到 指针不是数组 把指针作为函数的参数时，如果语义上这个指针是一个数组，需要也指定这个数组的长度 对指针之间做减法，只能是当这些指针指向的是同一个 array object 时才行 object 上做 sizeof，返回的结果的类型是：size_t ptrdiff_t：指针之间相减以后的值的类型是 ptrdiff_t 使用 printf 打印一个指针时，用 p％，而且要把这个指针 cast 成 void* C 里面可以把一个类型的 object 强制解释成另外一个不同的类型。但不要这样做，这样做有一个专门的术语：trap representation，这是一种 UB 行为。一个指针只能指向 3 种目标： 有效的 object 有效位置之后的一个位置 0（空值） 总之，对一个 pointer 做 dereferenced 时，这个 pointer 指向的 object 必须有效，同时这个 pointer 指向的 object 的 type 必须是明确的（designated type） C 里面有空指针的概念，但不能使用宏 NULL 来表示空指针（有坑。目前 C 标准对宏 NULL 的规定比较松散，不严格，底层具体的实现可能和平台相关）。目前推荐的做法是把指针赋值为 0 来表示空指针 数组可以被退化成指针。而且一旦退化成指针后，数组的其他信息就都丢失了// A 原先是一个数组，取地址后就退化成指针 p 了，后续使用 p 就是作为一个指针来使用int A[10] = {0};int* p = &amp;amp;A[0]; 把数组作为参数传给函数时，在数组参数之前增加一个参数来表示数组的长度：// len 参数就是用来表示「数组参数」的长度// 当然这个 len 的值是否正确，只能是靠程序员来保证了double double_copy(size_t len, double target[len], double const source[len]); 能不用 &amp;amp; 操作符（取一个 object 的地址），就不要用；引起潜在的问题（可以参考 Rust 的思想） void *：无类型的指针。任何对象的指针无需强转就可以转换为 void*，但也会损失掉这个对象的 type 信息 尽管损失了 type 信息，但转换过程中 object 对应的 storage 实例是不会动的。在用强转回来以后，还能恢复原先的类型和数据（值能保持一致） 一种常见的做法是：函数的入参是一个 void* 参数和一个 size_t 的参数，这样的话，代表该函数要对一段固定长度（size_t 为单位）的内存做操作，而不关心这段内存中的数据的类型（例如：memcpy 和 memset） 还有一种常见情况：创建「线程」时「程序员」需要传递数据到线程中，pthread_create 函数有一个参数的类型就是 void*，其含义就是「程序员」可以传入任何类型的数据到线程中。当然，「程序员」自己知道这个 void* 是什么类型的，「程序员」会在「线程」中做一次 recast 把数据转换回到正确的类型。当然，这种 recast 的正确性完成由「程序员」保证。 void* 还是能不用就尽量不用 restrict 关键字：用 restrict 修饰指针类型告诉「编译器」两个指针不指向同一数据（开发人员保证）。也就是该指针只会指向一个 object，不会 aliasing 其他对象 Pointer aliasing：是指两个或以上的指针指向同一数据 一个例子：// memcpy 的定义使用了 restrictvoid* memcpy(void*restrict s1, void const*restrict s2, size_t n);// 而 memmove 不能使用 restrict（可能发生 Pointer aliasing）void* memmove(void* s1, const void* s2, size_t n); 一般来说，函数的参数的类型都是「指针」，而不能是「数组」。唯一的例外只是在对函数进行「声明」的时候，下面三种写法等价：int func(int *a); /*写法1*/int func(int a[]); /*写法2*/int func(int a[10]); /*写法3：编译器会忽略 10 */ C99 中给出了「可变结构体」的标准定义。例如：typedef struct { int length; Point point[]; // 这里不指定长度} Polyline;// 利用 malloc 对其进行初始化一个长度为 5 的结构体：Polyline* polyline = malloc(sizeof(Polyline) + sizeof(Point) * 5);polyline-&amp;gt;length = 5;inline C99 引入了 inline 关键字： A function definition that is declared with inline can be used in several TUs without causing a multiple-symbol-definition error All pointers to the same inline function will compare as equal, even if obtained in different TUs An inline function that is not used in a specific TU will be completely absent from the binary of that TU and, in particular, will not contribute to its size inline 的一个关键点： An inline function definition is visible in all TUs. 但同时，编译器也不会保证一定会 emit 这个 inline 函数的符号。 如果要确保该 inline 函数的符号被 emit，可以在 .c 文件中加一行不带 inline 的函数 declare 来确保 emit 函数的符号 总之，inline 函数的最佳实践如下： An inline definition goes in a header file An additional declaration without inline goes in exactly one TU 例如一个头文件 toto.h 文件：// toto.h 文件// Inline definition in a header file.// Function argument names and local variables are visible// to the preprocessor and must be handled with care.inlinetoto* toto_init(toto* toto_x) { if (toto_x) { *toto_x = (toto){ 0 }; } return toto_x;}其对应的 toto.c 文件：#include ”toto.h”// Instantiate in exactly one TU.// The parameter name is omitted to avoid macro replacement.toto* toto_init(toto*);其他 再澄清一下几个概念 semantic type：也就是语义上的类型。例如 int32_t（即 32 位的整数） basic type：C 语言中的基本类型。例如 signed int（事实上 int32_t 是用 typedef 到 signed int） binary representation：二进制表示。signed int 就是 b31, b30, … b7, … b0（32 个bits，4 个 bytes） object representation：C 中所有的 object 都可以用 unsigned char 来表示。如果在一个小端系统中，上面的 binary representation 对应的 object representation 就是 unsigned char[4] 数组，且该数组的 [0] 是最高位，[3] 是最低位（小端系统） 总之，C 对 object 的内存模型做了以下规定： sizeof(char) 为 1（包括 3 种 char：unsigned char，signed char 和 char） 类型为 A 的 object 的 object representation 是一个数组：unsigned char[sizeof(A)] 但不要搞混了，object representation 的 char 是 unsigned char，不是 char char 只能用于「字符类型」，或者「字符串类型」 cast：不要用 cast，坑巨多 Effective Type：对 object 的 access 进行限制。关键点如下： Objects must be accessed through their effective type or through a pointer to a character type union 是个例外：Any member of an object that has an effective union type can be accessed at any time, provided the byte representation amounts to a valid value of the access type The effective type of a variable or compound literal is the type of its declaration Files that are written in binary mode (fread, fwrite) are not portable between platforms错误码 一般还是不要用 errno 那一套方法（多线程环境不能用）。而是利用「枚举」来定义错误码，把错误码作为函数的返回值返回给「调用方」，并采用以 API 文档的形式定义错误码的具体含义。 并且要用 API 的形式告诉「调用方」如何处理对应的错误；如果和「调用方」无关（或者说是「调用方」无法处理的错误）就不需要返回错误码给「调用方」了 " }, { "title": "Jekyll-Chirpy notes", "url": "/posts/Jekyll-Chirpy-notes/", "categories": "notes", "tags": "notes", "date": "2022-03-21 13:23:32 +0800", "snippet": "Just a Note.Please flow this guild: https://chirpy.cotes.page/posts/getting-started." }, { "title": "Rust Notes", "url": "/posts/rust-notes/", "categories": "编程", "tags": "rust", "date": "2022-02-23 00:00:00 +0800", "snippet": " type ownership move borrowing copy references lifetime 函数定义使用生命周期注解 结构体的生命周期注解 pointers Box Rc RefCell collections vector slices hash map String string slice struct enum Patterns match module Attribute error panic Result 实现 Error Trait 日志 traits Drop Sized Clone Copy Deref and DerefMut Default AsRef and AsMut Borrow and BorrowMut From and Into input output Read BufRead Write threads Channels Arc async Future async/.await Pin 异步运行时 更多底层细节 closure Iterator 概念 获取 iterator 使用 iterator macros macro rules unsafe Unsafe Block Unsafe Function Unsafe Trait Raw Pointer type 不变：一旦一个变量绑定了一个值，就不能被绑定到其他值上面 可变：该变量可以被绑定到其他值上 不变 vs. shadowing：shadowing 相当于创建了一个新的变量 静态语言：编译时就要知道所有变量的类型 标量：整型（有符号和无符号都支持），浮点（f32 和 f64），布尔，字符（注意：不是字符串） 复合类型：tup, array（stack上，固定长度） 字符串常量（string literal）vs. String 类型：前者在栈上？（运行期不可变），后者在堆上（运行期可变） const 和 变量的区别： const 是编译期可决定的值（或表达式） 可以在函数之外定义一个 const，在 module 的范围内使用这个 const const 更快 type 关键字：给某个类型声明一个新的名字。例如：#![allow(unused)]type Bytes = Vec&amp;lt;u8&amp;gt;;fn decode(data: &amp;amp;Bytes) { //...}// 给每个 module 定义一个 Result 的 alias// 一般来说，某个 module 内，使用同一个 Error，使用这种 alias 就可以简化 Result 的书写pub type Result&amp;lt;T&amp;gt; = result::Result&amp;lt;T, Error&amp;gt;;ownership Note：「变量」（variable）拥有「值」（value）所有权（ownership）3 原则： Rust 中每一个值都有一个称之为其「所有者」（owner）的「变量」 「值」有且只能有一个所有者（「变量」） 当所有者（「变量」）离开作用域，这个「值」将被丢弃，同时其资源也被释放 思考：使用 C 语言的时候，如果遵循这 3 个原则来使用指针？对复合数据结构，所有权可以是「树状结构」。例如：#![allow(unused)]fn main() { // 定义一个 struct：Person struct Person { name: String, birth: i32 } // 使用 let 来创建值或者资源，同时该「变量」成为「值」的 owner // 一个 Person 的 Vector let mut composers = Vec::new(); // Vector 中放入 3 个 structs composers.push(Person { name: &quot;Palestrina&quot;.to_string(), birth: 1525 }); composers.push(Person { name: &quot;Dow&quot;.to_string(), birth: 1563 }); composers.push(Person { name: &quot;Lully&quot;.to_string(), birth: 1632 }); // composers 是所有权树的 root，其拥有 3 个 structs // 每个 struct 又拥有 2 个字段 // struct 中的字段 name，又拥有其对应的「值」（文本内容） // 从而形成了一个「所有权树」}// 离开作用域，从所有权树的 root 节点 composers 开始释放整个树另外一个例子，使用了 Box 类型：#![allow(unused)]fn main() { // A Box&amp;lt;T&amp;gt; is a pointer to a value of type T stored on the heap // Box::new 在 heap 上给一个 tuple 分配空间，然后指向分配的空间的指针 let point = Box::new((0.625, 0.5));}// 离开作用域，释放 head 上分配的空间除了上面的基本规则，ownership 还有几个扩展概念，这里只提一下，后面的章节会深入细节： 可以把「值」从一个 owner 移交给另外一个 owner，也就是 move 一些简单类型（Integer，char），默认不遵守所有权规则，这些类型统称为 Copy Type 利用 Rc，Arc 等指针机制，一个「值」可以有几个 owners 除了 own 一个值，Rust 还提供了另外一种机制来访问值：borrowmove move：把「值」的所有权转移给另外一个「变量」（owner） 赋值（=），传递「函数参数」，返回「函数返回值」都会发生 move 一旦某个「变量」的所有权转移（move）走了以后，该「变量」失效，不能再使用这个「变量」 例如：在【函数】和【闭包】中，在入参是 move 的场景（而不是 borrow 的场景），进入函数后，参数和返回值的 owner 关系发生改变，原先的变量不再有效 可以通过实现 Copy trait 把默认的 move 改成 copy（复制，类似深拷贝） Rust 中，一些简单类型，默认实现了 Copy trait，这些类型统称为 Copy Type Copy trait 继承了 Clone trait 一个函数入参是 move 的例子：fn main() { let s1 = String::from(&quot;abc&quot;); // 该字符串的 ownership 被 move，之后 s1 失去了这个字符串的 ownership，s1 不再有效 let len = calculate_length(s1); // 不能再使用 s1（已经发生了 move） println!(&quot;The length is {}.&quot;, len);}// movefn calculate_length(s: String) -&amp;gt; usize { // 函数内部拿到了这个字符串的 ownership s.len() // 当函数结束的时候，这个字符串被 dropped} 和大部分类型不同，Copy Type（例如：integers，整型）不使用 move 规则，而是进行 copy。例如：#![allow(unused)]fn main() { // copy let x = 5; // bind the value 5 to x let y = x; // make a COPY of the value in x and bind it to y // move let s1 = String::from(&quot;hello&quot;); let s2 = s1; // s1 was moved into s2。之后 s1 不再有效 // clone let s1 = String::from(&quot;hello&quot;); let s2 = s1.clone();}其它一些要点： 不能把 Vector 中的单个 element move out。例如：let third = v[2] 这种场景可以使用 borrow。除了 move 之外，还可以 borrow 一个「值」（后面会详细说） borrowing借用 (borrowing)：对函数参数来说，如果不想 ownership 发生变化可以利用引用（reference）。下面给一个简单的例子：fn main() { let s1 = String::from(&quot;hello&quot;); // 把 s1 的引用（&amp;amp;s1）作为参数传给函数，这样就可以达到 borrowing 的效果：s1 对这个字符串值的 ownership 不变 let len = calculate_length(&amp;amp;s1); // s1 依旧是这个字符串值的 owner，可以使用 println!(&quot;The length of &#39;{}&#39; is {}.&quot;, s1, len);}// 借用：使用引用（&amp;amp;String）作为函数的入参fn calculate_length(s: &amp;amp;String) -&amp;gt; usize { // 函数内部使用引用 s（borrowing 到一个字符串的值） s.len() // 当函数结束的时候，这个引用 s 本身（s 本身也是类型为引用的值）被 dropped} 要想在函数内部修改借用的参数的值，需要利用「可变引用」：fn main() { let mut s = String::from(&quot;hello&quot;); change(&amp;amp;mut s);}fn change(some_string: &amp;amp;mut String) { some_string.push_str(&quot;, world&quot;);} NOTE： 为啥不需要对这个可变引用进行 deference（类似 C 语言中的 * 操作符）？ (*some_string).push_str(“, world”); 这是因为在「method」或者「field」上的 . 操作符可以自动 deference 引用。也就是说在这种场景，不需要区分是引用还是值，直接使用 . 操作符就行了： some_string.push_str(“, world”); 但对于其他场景，还是需要进行 deference，例如： (*some_string) = String::from(“new string it”);copy默认是 copy 而不是 move 的类型有： integer floating-point numeric char bool A tuple or fixed-size array of Copy types is itself a Copy typereferences 定义：The &amp;amp;s1 syntax lets us create a reference that refers to the value of s1 but does not own it 翻译：s1 是一个值；而 &amp;amp;s1 就是指向这个值的「引用」（但并不 own 这个值） Rust 提供了「引用」这种方式来 access 一个 value：可使用 value，但不 own 这个 value。又分为 2 种「引用」： &amp;amp;T；「shared 引用」；多个 「shared 引用」可共享同一个 value，但不能修改这个 value &amp;amp;mut T；「可变引用」；不能共享同一个 value，但可以修改这个 value 和普通类型一样，引用实际上也是类型： i32：一个整数类型（32 位） &amp;amp;i32：一个指向 i32 的不变引用类型 &amp;amp;mut i32：一个指向 i32 的可变引用类型引用两原则： At any given time, you can have either (but not both of) one mutable reference or any number of immutable references 在给定作用域中的给定值有且只有一个「可变引用」 if we have an immutable reference to something, we cannot also take a mutable reference References must always be valid 值的生命周期必须比指向它的引用的生命周期大（outlives） 如果被引用的值失效了（被 drop 掉），这个引用也就失效了 在给定作用域中的给定值已经存在引用，也不能对一个值的 owner 进行 move：因为一旦对这个值的 owner 进行 move 操作，这个值就会被 drop 掉，就导致这个值上的引用失效 注意 引用原则中的【作用域】指的是：从创建开始，一直持续到它最后一次使用的地方，而不是从创建持续到某一个花括号请仔细对比以下 2 个例子，一个【合法】，一个【非法】：【合法】：#[allow(unused_assignments)]fn main() { // 变量 s 对该 String 值有 ownership let mut s = &quot;12345&quot;.to_string(); // r 是到 s 的「可变引用」，或者可以说：“引用 r is a borrow of 变量 s” let r = &amp;amp;mut s; //【合法】：使用「可变引用」 r 对值进行修改 // 而且这里是 r 【最后一次】被使用的地方 r.push_str(&quot;67890&quot;); //【合法】：因为这里已经不是引用 r 的作用范围（已经在最后一次使用之后了） // 所以满足原则：“在给定作用域中的给定值已经存在引用，也不能对一个值的 owner 进行 move” s = String::from(&quot;123456789012345&quot;);}【非法】：#[allow(unused_assignments)]fn main() { // 变量 s 对该 String 值有 ownership let mut s = &quot;12345&quot;.to_string(); // r 是到 s 的「可变引用」，或者可以说：“引用 r is a borrow of 变量 s” let r = &amp;amp;mut s; //【合法】：使用「可变引用」 r 对值进行修改 r.push_str(&quot;67890&quot;); //【非法】：因为这里还在引用 r 的作用范围之内（之后还使用了 r.len，不是【最后一次】被使用） // 所以违反了原则：“在给定作用域中的给定值已经存在引用，也不能对一个值的 owner 进行 move” s = String::from(&quot;123456789012345&quot;); // r 还在被使用 r.len();} 总之： owner 和 可变引用的根本区别就在于 owner 会负责值的 drop（释放）；这个区别也就决定了一旦值上存在引用，就要小心的使用 owner，要保证这个值要始终有效，不要被释放，否则 Rust 的编译器会提示失败。另外，Rust 中，可以「引用」到任意的表达式，包括「值」，例如：fn factorial(n: usize) -&amp;gt; usize { (1..n + 1).product()}let r = &amp;amp; factorial(6);// Arithmetic operators can see through one level of references. // 可以是 r + &amp;amp;1009 这种形式assert_eq!(r + &amp;amp;1009, 1729);上面的例子中，Rust 会创建一个「匿名变量」来 hold 这个表达式的「值」，然后让这个「引用」指向这个「匿名变量」，而这个「匿名变量」也有它自己的生命周期。lifetime 【生命周期】原则：一个引用的生命周期不能超过其引用的变量的有效期 Rust 中，【生命周期】的概念只和变量的引用有关；Rust 中所有的引用都会关联一个「生命周期」 【生命周期】是程序可以安全使用这个引用的一个范围 【生命周期注解】告诉 Rust 多个引用的生命周期如何相互联系函数定义使用生命周期注解先给一个在函数中使用生命周期注解的例子： 告诉 Rust 这 3 个引用的【生命周期】的关系；返回值的生命周期和「两个入参」的生命周期相同：fn longest&amp;lt;&#39;a&amp;gt;(x: &amp;amp;&#39;a str, y: &amp;amp;&#39;a str) -&amp;gt; &amp;amp;&#39;a str { if x.len() &amp;gt; y.len() { x } else { y }}编译时，会检查其是否符合生命周期原则：fn main() { let string1 = String::from(&quot;long string is long&quot;); { let string2 = String::from(&quot;xyz&quot;); let result = longest(string1.as_str(), string2.as_str()); println!(&quot;The longest string is {}&quot;, result); } // 内层代码块的结束}检查过程如下： 这里 2 个入参的生命周期 ‘a，是 string1 和 string2 这 2 个参数作用域重叠的部分（内层代码块的结束之前） 返回值 result 的生命周期也是 ‘a（string1 和 string2 这 2 个参数作用域重叠的部分） 那么 println! 使用 result 的时候，’a 是有效的（内层代码块的结束之前），所以符合生命周期原则再来一个不符合生命周期原则的例子：fn main() { let string1 = String::from(&quot;long string is long&quot;); let result; { let string2 = String::from(&quot;xyz&quot;); result = longest(string1.as_str(), string2.as_str()); } println!(&quot;The longest string is {}&quot;, result);} 这个例子中返回值 result 的生命周期也同样是：string1 和 string2 这 2 个参数作用域重叠的部分 但 println! 使用 result 的时候，已经超出了 ‘a 的有效范围（内层代码块的结束之后），所以不符合生命周期原则结构体的生命周期注解 结构体的生命周期注解：结构内有引用的话，结构体本身的生命周期必须和其中引用的对象的生命周期保持一致#![allow(unused)]struct ImportantExcerpt&amp;lt;&#39;a&amp;gt; { part: &amp;amp;&#39;a str,}fn main() { let novel = String::from(&quot;Call me Ishmael. Some years ago...&quot;); let first_sentence = novel.split(&#39;.&#39;) .next() .expect(&quot;Could not find a &#39;.&#39;&quot;); let i = ImportantExcerpt { part: first_sentence };}最后详细分析一个生命周期的例子。先定义一个结构体 S：struct S&amp;lt;&#39;a&amp;gt; { x: &amp;amp;&#39;a i32, y: &amp;amp;&#39;a i32}然后在下面这段代码中使用这个结构体（具有 2 层代码块）：fn main() { let x = 10; let r; { let y = 20; let s = S { x: &amp;amp;x, y: &amp;amp;y }; r = s.x; } // 内层代码块的结束} // 外层代码块的结束检查过程如下： 结构体 S 的 2 个引用字段有相同的生命周期 ‘a 这个例子中，生命周期 ‘a 为变量 x 和 y 的重叠部分（也就是内层代码块的结束） 变量 s 的生命周期也要和这 2 个字段的生命周期 ‘a 相同，所以变量 s 的生命周期也必须是 x 和 y 这 2 个变量的重叠部分（也是内层代码块的结束） 但 r = s.x 这个赋值语句要求 s 生命周期 ‘a 也必须能涵盖 r 的生命周期（也就是说 ‘a 不能比 r 先结束而失效） 但实际上 r 的有效期需要到外层代码块结束 最终检查结果是：不符合「生命周期原则」解决方法是修改结构体 S 的定义：struct S&amp;lt;&#39;a, &#39;b&amp;gt; { x: &amp;amp;&#39;a i32, y: &amp;amp;&#39;b i32}pointers 实际上，Rust 中的 Smart Points（以及常用 Traits）就是给开发人员提供了 Rust 中一些惯用模式。Rust 中的指针有 3 种： 【引用】：Rust 中安全的指针。也就是【非所有权指针】，分 &amp;amp;T 和 &amp;amp;mut T 两种。其中 &amp;amp;T 本身是一种 Copy 类型；而 &amp;amp;mut T 并没有实现 Copy Trait（Copy trait 的细节请参考其他小节） 【原始指针】：也就是 Raw Points，用于 unsafe 代码；这里不详细介绍了 【智能指针】：智能指针的 2 个关键 trait：Drop（离开作用域后，自动释放资源） 和 Deref。标准库提供了几种智能指针：Box&amp;lt;T&amp;gt;，Rc&amp;lt;T&amp;gt;，Arc&amp;lt;T&amp;gt;，Cell&amp;lt;T&amp;gt;，RefCell&amp;lt;T&amp;gt;下面是对其中一些【智能指针】的介绍。Box使用 Box 的例子：#![allow(unused)]fn main() { let x = 42; let y = Box::new(84); { let z = (x, y); } // 该 scope 结束后，不但 z 被释放，而且 y 也被释放（因为 y 这个变量是用 Box::new 来初始化的，所以用 y 来构建 z 的时候是 move，而不是 copy） let x2 = x; // 能通过生命周期检查 let y2 = y; // 不能通过生命周期检查，因为 y 已经被释放}既然 Box 用于把数据存储在「堆」（heap）上，那么 Box 常常被使用在以下场景： 在编译时，无法确认数据 size 的场景 不关心当前「对象」具体的「类型」，而是只关心它实现了什么 Trait下面是一个例子：// 错：这个定义是不能通过编译的。因为 List 的这个定义是递归的，在编译期间不能决定 List 的 sizeenum List { Cons(i32, List), Nil,}改用 Box 可通过编译：enum List { Cons(i32, Box&amp;lt;List&amp;gt;), Nil,}Rc如果上一节例子中的某个 List 节点被多个 List 指向怎么办？如果还是使用 Box 指针不能通过编译：#![allow(unused)]enum List { Cons(i32, Box&amp;lt;List&amp;gt;), Nil,}fn main() { let a = Cons(5, Box::new(Cons(10, Box::new(Nil)))); // b 获取到了 a 的「所有权」 let b = Cons(3, a); // 错：这里不能通过编译，a 的「所有权」已经被 b 占有 let c = Cons(4, a);}这种场景需要使用 Rc（单个 value 可以有多个 owners）：#![allow(unused)]use std::rc::Rc;enum List { Cons(i32, Rc&amp;lt;List&amp;gt;), Nil,}fn main() { let a = Rc::new(Cons(5, Rc::new(Cons(10, Rc::new(Nil))))); // b 获取到了 a 的「所有权」，引用计数 +1 let b = Cons(3, Rc::clone(&amp;amp;a)); // c 也获取到了 a 的「所有权」，引用计数 +1 let c = Cons(4, Rc::clone(&amp;amp;a));}其他使用 Rc 的例子：use std::rc::Rc;// Rust can infer all these types; written out for claritylet s: Rc&amp;lt;String&amp;gt; = Rc::new(&quot;hello it&quot;.to_string());let t: Rc&amp;lt;String&amp;gt; = s.clone();let u: Rc&amp;lt;String&amp;gt; = s.clone(); A value in an Rc box is always shared and therefore always immutable. 翻译：Rc 中的 value 总是被共享的值，也就是说，这个 value 是「不可变的」，可读但不可写。RefCell为了解决「内部可变」（interior mutability ）问题，可以使用 RefCell 指针。所谓 interior mutability 问题持有不可变引用又需要修改其中的 value（而不像 Rc 中的值是「不可变的」）。正常来说，不能对「不变量」进行「可变引用」：let x = 5;let y = &amp;amp; mut x; // 编译错：不能对「不变量」进行「可变引用」但有时候，开发人员需要对一个不变引用中的 value 做可变（修改这个 value 的值），这时候就可以使用 RefCell。例如，要实现一个 trait，该 trait 的定义如下：pub trait Student { // 用于接收老师消息，注意这里的 &amp;amp;self 是不可变引用 fn on_message(&amp;amp;self, msg: &amp;amp;str);}on_message 的定义限制了 self 是「不可变」的，但我们又希望在 on_message 中改变。那么解决方法如下：use std::cell::RefCell; // 从标准库中引入struct Boy { messages: RefCell&amp;lt;Vec&amp;lt;String&amp;gt;&amp;gt;, // messages 的类型为 RefCell}impl Boy { fn new() -&amp;gt; Boy { Boy { messages: RefCell::new(vec![]) // 将 vec 保存在 RefCell 中 } }}impl Student for Boy { fn on_message(&amp;amp;self, message: &amp;amp;str) { // self 仍然是不可变引用 // 在运行时借用可变引用类型的 messages self.messages.borrow_mut().push(String::from(message)); }}RefCell 中包含一个可变引用（mut reference）。RefCell&amp;lt;T&amp;gt; is a generic type that contains a single value of type T. RefCell supports borrowing references to its T value： RefCell::new(value)。Creates a new RefCell, moving value into it. ref_cell.borrow()。Returns a Ref&amp;lt;T&amp;gt;, which is essentially just a shared reference to the value stored in ref_cell. This method panics if the value is already mutably borrowed ref_cell.borrow_mut()。Returns a RefMut&amp;lt;T&amp;gt;, essentially a mutable reference to the value in ref_cell. This method panics if the value is already borrowed ref_cell.try_borrow(), ref_cell.try_borrow_mut()。Work just like borrow() and borrow_mut(), but return a Result. Instead of panics if the value is already mutably borrowed, they return an Err value一个例子：use std::cell::RefCell;let ref_cell: RefCell&amp;lt;String&amp;gt; = RefCell::new(&quot;hello&quot;.to_string());let r = ref_cell.borrow(); // ok, returns a Ref&amp;lt;String&amp;gt;let count = r.len(); // ok, returns &quot;hello&quot;.len()assert_eq!(count, 5);let mut w = ref_cell.borrow_mut(); // panic: already borrowedw.push_str(&quot; world&quot;);其实 RefCell 的使用规则和普通的「引用」基本一致，唯一的区别就是使用普通「应用」时违反规则的话，会在「编译期」给出错误提示；而使用 RefCell 违背规则的话，会在运行时 panic。另外： 将 Rc&amp;lt;T&amp;gt; 和 RefCell&amp;lt;T&amp;gt; 结合使用来实现一个拥有多重所有权的可变数据。 但这种做法不是线程安全的，常用于用 Rust 刷算法题，并不会用到工程上。总之，RefCell 提供了一种在编译期间绕过 Rust 不变性检查的方法；但同时也付出了复杂性的代价。arrayRust 中表示内存中连续的值的序列的类型有 3 种： Array：[T; N] Vector：Vec&amp;lt;T&amp;gt; Slice：&amp;amp;[T] 和 `&amp;amp;mut [T] 这 3 种形式中 v[i] 都代表的是第 i 个元素。这里介绍 Array（Vector 的介绍放到后面一节）： [T; N] 表达的是有 N 个值的数组，且该数组中的每个元素的类型是 T [T; N] 形态的数组必须在编译期就决定该数组的大小和类型，同时该数组的大小不能再变化 这里也先提一下 Slice：&amp;amp;[T] 和 &amp;amp;mut [T] 表达的是另外一个内存中连续的值的序列（Array 或 Vector）的 slicecollections collections 存放的是指向 heap 上的数据的指针的集合（和 array/tuple 不一样）： vector：可变长度，并且存放同种类型的元素 string：Rust 标准库提供了 String 类型 hash map vector 一些创建 vector 的例子：#![allow(unused)]fn main() {// 使用 vec! macro let mut primes = vec![2, 3, 5, 7]; // 变长 let mut z = vec![0; rows * cols];// 使用 Vec::new let mut pal = Vec::new(); pal.push(&quot;step&quot;);// 使用 iterator let v: Vec&amp;lt;i32&amp;gt; = (0..5).collect(); assert_eq!(v, [0, 1, 2, 3, 4]);} 容量不等于 vector 的当前面 size：// capacity 是 2，但 size 是 0let mut v = Vec::with_capacity(2);assert_eq!(v.len(), 0);assert_eq!(v.capacity(), 2); 两种方法获取 vector 中的某个元素 let does_not_exist = &amp;amp;v[100]; // 直接 panic，如果数组的 size 小于 100 let does_not_exist = v.get(100); // 返回 None，如果数组的 size 小于 100 下面代码编译时直接报错：first 是不变引用；但 push 的时候发生的 borrow 行为：编译失败。fn main() { let mut v = vec![1, 2, 3, 4, 5]; let first = &amp;amp;v[0]; v.push(6);}遍历 vector，并修改其中的值：fn main() { let mut v = vec![100, 32, 57]; for i in &amp;amp;mut v { *i += 50; }}如果一个 vector 中需要存不同类型的值，可以利用 Enum 类型来实现：fn main() { enum SpreadsheetCell { Int(i32), Float(f64), Text(String), } let row = vec![ SpreadsheetCell::Int(3), SpreadsheetCell::Text(String::from(&quot;blue&quot;)), SpreadsheetCell::Float(10.12), ];}总之，Rust 必须在编译期确定 vector 中的数据的类型。slicesslice（[T]）是 array 或 vector 中的一部分；可能是 array，也可能是 vector，总之是一段连续的数据的引用。最后再说一下数组的 move数组（或 vector）中的元素是不能被 move 的：// Build a vector of the strings &quot;101&quot;, &quot;102&quot;, ... &quot;105&quot;let mut v = Vec::new();for i in 101..106 {v.push(i.to_string());}// 错：Pull out random elements from the vector.let third = v[2]; // error: Cannot move out of index of Veclet fifth = v[4]; // here too如果要对数组（或 vector）中的元素做 move，需要专门的函数，例如：pop、swap_remove、std::mem::replace……另外，把数组（或 vector）中的元素 move 出来也叫做 consume （消费）。例如，对 vector 中的所有元素进行「消费」：let v = vec![&quot;aaa&quot;.to_string(), &quot;bbb&quot;.to_string(), &quot;ccc&quot;.to_string()];for mut s in v {s.push(&#39;!&#39;);println!(&quot;{}&quot;, s);}上面的循环中，把 v 中的元素逐个 move 给 s，每次 s 拥有 v 的元素，并进行操作。最后看一个技巧：// 数组中元素中有 Optionstruct Person { name: Option&amp;lt;String&amp;gt;, birth: i32}let mut composers = Vec::new();composers.push(Person { name: Some(&quot;Palestrina&quot;.to_string()),birth: 1525 });// 用 take 方法把 Option 的值 move 出来，而数组中留下一个 Nonelet first_name = composers[0].name.take();hash map Rust 中使用 std::collections::HashMap 来表示 1 对 1 的关系。 Rust 的 Map，所有的 key 必须为相同的类型。所有的 value 必须为相同的类型。创建一个 map 来记录 2 个队伍的分数：use std::collections::HashMap;fn main() { let mut scores = HashMap::new(); scores.insert(String::from(&quot;Blue&quot;), 10); scores.insert(String::from(&quot;Yellow&quot;), 50);}利用 vector 的 zip 方法在初始化时创建 HashMap：use std::collections::HashMap;fn main() { let teams = vec![String::from(&quot;Blue&quot;), String::from(&quot;Yellow&quot;)]; let initial_scores = vec![10, 50]; let scores: HashMap&amp;lt;_, _&amp;gt; = teams.iter().zip(initial_scores.iter()).collect();}Hash Map 会对其中的值有 ownership。下面的例子中，在执行了 map.insert 以后，field_name 和 field_value 失去对原先值的 ownership：use std::collections::HashMap;fn main() { let field_name = String::from(&quot;Favorite color&quot;); let field_value = String::from(&quot;Blue&quot;); let mut map = HashMap::new(); map.insert(field_name, field_value); // 之后 field_name 和 field_value 失去对原先值的 ownership}拿到 HaspMap 中的值：#![allow(unused)]fn main() { let team_name = String::from(&quot;Blue&quot;); let score = scores.get(&amp;amp;team_name);}这里的 score 是一个 Some(&amp;amp;10)，也就是说，是一个 Option&amp;lt;&amp;amp;V&amp;gt;；如果值不存在，返回 None。遍历 HashMap：fn main() { for (key, value) in &amp;amp;scores { println!(&quot;{}: {}&quot;, key, value); }}利用 entry 方法和 or_insert 来实现只有当 key 不存在时才 insert 新值：fn main() { scores.entry(String::from(&quot;Blue&quot;)).or_insert(50);}StringString 特性 UTF-8 编码 growable mutable新建 String#![allow(unused)]fn main() { let mut s = String::new(); // 新建 let s = &quot;initial contents&quot;.to_string(); // 带初始值的新建 let s = String::from(&quot;initial contents&quot;); // 和上面等价} push_str 和 push#![allow(unused)]fn main() { let mut s1 = String::from(&quot;foo&quot;); let s2 = &quot;bar&quot;; s1.push_str(s2); // push_str 把一个 string slice 加在后面。push_str 不需要 s2 的 ownership s.push(&#39;l&#39;); // push 用来添加单个字符在后面} 用 + 号连结#![allow(unused)]fn main() { let s1 = String::from(&quot;Hello, &quot;); let s2 = String::from(&quot;world!&quot;); // s1 会被 move，失去「所有权」 let s3 = s1 + &amp;amp;s2; // Note s1 has been moved here and can no longer be used} 复杂的字符串拼接用 format!#![allow(unused)]fn main() { let s1 = String::from(&quot;tic&quot;); let s2 = String::from(&quot;tac&quot;); let s3 = String::from(&quot;toe&quot;); let s = format!(&quot;{}-{}-{}&quot;, s1, s2, s3);} 支持 == 和 != 字符串内容（存储在内存中的字符串内容）是否相等的比较 例如：”th\\u{e9}” 和 “the\\u{301}” 是不相等的，尽管它们显示出来都是法文的 thé，但它们内存中的字符串内容不同 Rust 保证 String 中的字符必须是有效的 UTF-8 编码的字符 当真的要处理无效的 UTF-8 编码的时候，需要使用其他的类型（&amp;amp;Path，OsString 等等），而不能使用 String 类型 string slice slice 允许你引用 collection 中一段连续的元素序列，而不用引用整个 collection 除了「引用」之外，slice 也不拥有数据的 ownership 例如，引用一个整数数组中的一部分：#![allow(unused)]fn main() { let a = [1, 2, 3, 4, 5]; let slice = &amp;amp;a[1..3];} 注意：在编译期间不知道 String 类型的 size；而 &amp;amp;str 的 size 在编译期间已知 Rust 专门有一个 string slice（&amp;amp;str）类型来表达 string literals：a reference to part of a String举个例子：需要写一个函数获取字符串的第一个单词。函数入参可以是一个对 String 类型的引用（&amp;amp;String）。但返回值是什么呢？这里返回值可以使用 string slice（&amp;amp;str）：fn first_word(s: &amp;amp;String) -&amp;gt; &amp;amp;str { let bytes = s.as_bytes(); // 把 String 转换为 byte 数组 for (i, &amp;amp;item) in bytes.iter().enumerate() { if item == b&#39; &#39; { return &amp;amp;s[0..i]; } } &amp;amp;s[..]} 但在处理字符串时，尽量不要牵涉到 byte 操作，而是要尽量用字符相关的操作 因为 Rust 中的字符是 UTF-8 编码，一个 UTF-8 编码的字符有几个 bytes 是不确定的。而且也是因为这个「不确定」，要获取 String 的某个 UTF-8 字符，只能是遍历整个 String： 利用 chars 来遍历字段串的例子：#![allow(unused)]fn main() { for c in &quot;您好&quot;.chars() { println!(&quot;{}&quot;, c); }} String 上的 slice range 语法是 byte 级别的操作。如果尝试从一个多字节字符的中间位置创建字符串 slice，则程序将会因错误而退出。#![allow(unused)]fn main() { let s = String::from(&quot;hello world&quot;); // 该例子是 ASCII 字符集上的 String，所以不会出错 let hello = &amp;amp;s[0..5]; let world = &amp;amp;s[6..11];}其他一些关于 &amp;amp;str 的要点： “您好”.len() 返回的是 byte 数，而不是字符数。”您好”.chars().count() 才返回的是字符数 &amp;amp;str 不能被修改；所以如果要在运行时修改一个字符串，需要使用 String 不过 make_ascii_uppercase 和 make_ascii_lowercase 这 2 个方法是例外，它们会修改 &amp;amp;mut str 从 String 中拿到对应的 &amp;amp;str。例如，要在一个 String 上进行 match 可以这样：fn main() { let s = String::from(&quot;Canada&quot;); match s.as_str() { &quot;Japan&quot; =&amp;gt; { println!(&quot;Match&quot;); } _ =&amp;gt; { println!(&quot;Un-Match&quot;); } }}structRust 中利用 struct 实现了一种把不同的数据聚合在一起的方法。这种方法主要使用在需要不同数据类型需要同时存在在一起的场景。以此向对应的，如果需要把多种数据类型聚合在一起，但不需要它们同时出现，可以使用 enum 。定义一个 struct ，然后定义一个返回 struct 的方法#![allow(unused)]fn main() { struct User { username: String, email: String, sign_in_count: u64, active: bool, } fn build_user(email: String, username: String) -&amp;gt; User { User { // 变量名和结构体字段名相同时，可以使用初始化简写 email, // 变量名和结构体字段名相同时，可以使用初始化简写 username, active: true, sign_in_count: 1, } }}结构体的上下文中可以定义方法（method）: method 的第一个参数总是 self，它代表调用该方法的结构体实例下面的例子给 Rectangle 这个结构体定义并实现了 area 和 can_hold（注意 impl 关键字的使用）：#[derive(Debug)]struct Rectangle { width: u32, height: u32,}impl Rectangle { fn area(&amp;amp;self) -&amp;gt; u32 { self.width * self.height } fn can_hold(&amp;amp;self, other: &amp;amp;Rectangle) -&amp;gt; bool { self.width &amp;gt; other.width &amp;amp;&amp;amp; self.height &amp;gt; other.height }}fn main() { let rect1 = Rectangle { width: 30, height: 50 }; let rect2 = Rectangle { width: 10, height: 40 }; println!(&quot;The area of the rectangle is {} square pixels.&quot;, rect1.area()); println!(&quot;Can rect1 hold rect2? {}&quot;, rect1.can_hold(&amp;amp;rect2));} 一般来说 method 的第一个参数可以是 &amp;amp;self 或者 &amp;amp;mut self tuple structs：tuple structs 的用法和普通 struct 类似，但 tuple structs 没有具体的字段名，只有字段的类型。简单点说，就是定义一个有类型名字的 tuple 类型。给个例子：/// 定义一个颜色类型，其包含 3 个三元色的值struct Color(i32, i32, i32); struct update。如下例子：#![allow(unused)]struct User { active: bool, // 注意：这里的 username 是 String 类型，一旦发生 move，整个 struct 也不能再被使用 username: String, email: String, sign_in_count: u64,}fn main() { // 先定义 user1 let user1 = User { email: String::from(&quot;someone@example.com&quot;), username: String::from(&quot;someone123&quot;), active: true, sign_in_count: 1, }; // 然后通过更新的方式来定义 user2 // 注意：这里发生了一次 String 类型 username 的 move，之后不能再使用 user1 let user2 = User { active: user1.active, username: user1.username, // 实际上 user1 和 user2 只有 email 这 1 个字段不同 email: String::from(&quot;another@example.com&quot;), sign_in_count: user1.sign_in_count, };} 上面的例子也可以采用简约写法：#![allow(unused)]struct User { active: bool, username: String, email: String, sign_in_count: u64,}fn main() { // 先定义 user1 let user1 = User { email: String::from(&quot;someone@example.com&quot;), username: String::from(&quot;someone123&quot;), active: true, sign_in_count: 1, }; // 注意：这里发生了一次 String 类型 username 的 move，之后不能再使用 user1 let user2 = User { // 实际上 user1 和 user2 只有 email 这 1 个字段不同，采用了 ..user1 这种简约写法 email: String::from(&quot;another@example.com&quot;), ..user1 };} 注意：struct 的生命周期请参考：结构体的生命周期注解type-associated functionsimpl 的实现中，有时候不是针对单个 struct 实例的，而是针对整个类型，所以参数列表中没有 self 参数。把参数中没有 self 的 methods 叫做 type-associated functions。例如：impl Queue { pub fn new() -&amp;gt; Queue { Queue { older: Vec::new(), younger: Vec::new() } }}对应的使用方法如下：let mut q = Queue::new();q.push(&#39;*&#39;);enumRust 的枚举（enum）中的「成员」可以存储各种类型。例如：#![allow(unused)]fn main() { enum Message { // 普通「枚举成员」，没有嵌入任何其他类型 Quit, // 存储了一个匿名 struct 的「枚举成员」 Move { x: i32, y: i32 }, // 存储一个 String 类型的「枚举成员」 Write(String), // 存储了 3 个 i32 的「枚举成员」 ChangeColor(i32, i32, i32), }}Rust 还可以在枚举上定义方法（类似 struct）。例如针对上面定义的 Message 枚举类型定义了一个 call 方法：#![allow(unused)]fn main() { enum Message { Quit, Move { x: i32, y: i32 }, Write(String), ChangeColor(i32, i32, i32), } impl Message { fn call(&amp;amp;self) { // 在这里定义方法体 } } let m = Message::Write(String::from(&quot;hello&quot;)); m.call();}标准库中的 Option 就是一个枚举类型，其定义如下：enum Option&amp;lt;T&amp;gt; { Some(T), None,}Patterns再重点介绍一下 Patterns。首先明确一点：相对于 Expressions 产生 values；Patterns 消费 values。Patterns 的作用对象可以是：enum，struct 或 tuple。Patterns 内部有 identifiers 的话，这些 identifiers 会成为局部变量。这些 identifiers 的 values 会被 copy 或 move 到这些局部变量。用一个例子来说明 move：match account { Account {name, language, ..} =&amp;gt; { ui.greet( &amp;amp; name, &amp;amp; language); ui.show_setting( &amp;amp; account); // error: borrow of moved value: `account` }}上面的例子中，account.name 和 account.language 已经被 move 到 2 个局部变量中，account 然后就被 drop 掉了。所以，之后不能再 borrow account。除了 copy 和 move，也可以使用 ref 关键字，表示 borrow（不对 value 进行消费，只是 borrow）。例如：match account { Account { ref name, ref language, .. } =&amp;gt; { ui.greet(name, language); // 只 borrow，不消费 ui.show_setting( &amp;amp; account); // ok }}最后，看一个 Patterns 的应用 while let 表达式：fn main(){ //gfg is a variable let mut gfg = &quot;Printing Geeks for Geeks using while let&quot;.chars(); // let pattern = expr（如果表达式能和这个 pattern 匹配，执行循环） while let Some(x) = gfg.next() { //print is a statement that is used to print characters in one line print!(&quot;{}&quot;, x); } println!(&quot;\\n&quot;);}match match 关键字后跟一个表达式 执行时，根据这个表达式计算出来的值，按顺序进行匹配，进入匹配成功的分支 match 的每个分支由 2 部分组成 模式：用来匹配是否进入该分支 代码：进入该分支后需要执行的代码； 而且每个分支的执行代码有一个结果值，被匹配到的分支的结果值就是整个 match 表达式的值 枚举的成员是 struct 的例子：#![allow(unused)]pub enum Protection { // 枚举成员是一个匿名的 struct Secure { version: u64 }, Insecure,}fn process(prot: Protection) { match prot { // 匹配的时候，匿名 struct 的值：{version} Protection::Secure { version } =&amp;gt; { println!(&quot;Hacker-safe thanks to protocol v{}&quot;, version); } Protection::Insecure =&amp;gt; { println!(&quot;Come on in&quot;); } }}fn main() { process(Protection::Secure { version: 2 })}问题：如果 match 的表达式是一个「引用」，那么 match 匹配的值是否也是「引用」？看一个例子：#![allow(unused)]pub enum Protection { // 枚举成员存储了一个 SecureVersion 类型的值 Secure(SecureVersion), Insecure,}#[derive(Debug)]pub enum SecureVersion { V1, V2, V2_1,}fn process(prot: &amp;amp;Protection) { match prot { // 答案：这里的 version 是一个引用：&amp;amp;SecureVersion Protection::Secure(version) =&amp;gt; { println!(&quot;Hacker-safe thanks to protocol {version:?}&quot;); } Protection::Insecure =&amp;gt; { println!(&quot;Come on in&quot;); } }}module基本概念： Module：对代码进行组织的单元（逻辑上把对代码分成不同的部分，便于管理） Crate：独立的可编译单元，可以编译为「库」，或者是「可执行文件」 Package：类似项目（或者工程）的概念，一个 Package 只能包含一个 Cargo.toml 文件 一个 Package 只能包含一个「库」；但一个 Package 可以包含多个「可执行文件」 这里的「库」和「可执行文件」就是上面提到的 Crate：独立的可编译单元 下面给一个典型的 Package（项目）的目录结构：.├── Cargo.toml├── Cargo.lock├── src│ ├── main.rs│ ├── lib.rs│ └── bin│ └── main1.rs│ └── main2.rs├── tests│ └── some_integration_tests.rs├── benches│ └── simple_bench.rs└── examples └── simple_example.rs几个关键词的作用： mod pub super use另外就是下面 3 个 cargo 命令的用法： cargo new comm –lib cargo fmt cargo test cargo doc –no-deps –open 不生成依赖的文档 Attribute 在 Rust 中，Attributes 类似 Java 的注解（annotations），或者 C/C++ 中的 #ifndef 这种给编译器使用的属性 Attributes 都是给编译器使用的 注解在 Rust 的各种 items 上 例如：注解在 module 上，并对整个 module 生效（标注在 module 上：#[cfg]，#[allow]） Rust 中 item 的类型具体有： module extern crate 声明 use 声明 函数定义 类型定义 结构体定义 枚举定义 联合体定义 常量项 静态项 trait 定义 实现 外部块 crate 上：标注在整个 crate 的最前面，需要使用 #![cfg] （多一个 ! 表示作用在整个 crate 上） Rust 中常用的 Attributes 有 4 大类： Built-in attributes（内建属性）：下面会有些例子 Macro attributes（宏属性）：略 Derive macro helper attributes（派生宏辅助属性） Tool attributes（工具属性）：略 一些常用的 Attributes： #[allow]：显示的关闭编译 warning。例如：#[allow(non_camel_case_types)] #[cfg]：用于设置编译选项，具体配置项可参考说明。例如：#[cfg(test)]（用于标注只在测试时生效的代码） #[test]：测试相关 #[doc]：自动生成代码文档 #[derive]：derive 属性会在使用 derive 语法标记的类型上生成对应 trait 的默认实现的代码。给 2 个例子： #[derive(Debug)]：只要给 struct 和 enum 加上这个 derive 属性，就可以在 println! 里面直接使用 {:?} 或者 {:#?} #[derive(Copy, Clone)]：Copy 和 Clone error 开发人员作为不同身份时，可以利用不同的方式来应对异常 作为库的开发人员 定义这个库的专用异常类型，通过这个专用异常类型告诉库的使用者，当遇到不同的异常时，需要进行相应的处理 举个例子，一个用于查询 MySQL 的库；当执行 SQL 语句失败时，需要告诉库的使用者失败原因；而库的使用者根据失败原因做不同的动作：可重试的进行重试，不可重试的直接通知最终用户本次请求失败等等 而且大多数场景，库本身可以不直接打印日志（而是由库的使用者来决定库相关日志的配置，例如日志级别，日志文件位置） 作为应用（在线服务）的开发人员 不可恢复异常；对一个在线服务，开发人员可以考虑把「不可恢复异常」透传到最外层，在最外层完成下列工作： 记录上下文（统计，排查，报警等目的） 结束当前请求 同时返回给最终用户一个友好的提示 可恢复异常：这里的「可恢复异常」可以理解为程序正常执行的多种可能的分支中的一个逻辑分支 开发人员的工作：把代码流程转入对应的逻辑分支（如有必要可打印日志，例如用于统计） 要达到以上目标，一种程序设计语言中，需要提供以下功能： 函数调用除了返回正常结果，也可以抛出异常 异常需要带必要的信息：异常码，异常信息，异常时的上下文信息（调用堆栈，代码行号等） 尽可能提供模块级别（Module Level）的异常类，而不是全局（Crate Level）的异常类 相同的底层异常（例如一个 IO 异常）抛出时，最好能区分该异常抛出时的上下文信息 便捷的把异常透传到最外层 捕捉到异常后，便捷的根据异常具体的信息，执行不同的代码逻辑 通过日志门面库（类似 Java 的 slf4j）和日志实现库（类似 Java 的 log4j，logback）配合，来灵活的记录日志而在 Rust 中，异常处理相关的语法有： panic! RESULTpanic其实对在线服务来说，panic 其实不怎么用。使用 panic 的例子：fn main() { panic!(&quot;crash and burn&quot;);} 发生 panic 后，Rust 会 unwind stack 后继续执行。但有 2 个例外的情况 Rust 不会做 unwind： 当遇到一个 panic 后，Rust 在做 drop 时，又触发了一个新的 panic；此时会干掉整个进程！ 编译时带上 -C panic=abort 选项，当遇到一个 panic 后，直接干掉整个进程！ panic 是线程级别的。单个线程里面 panic 了，并不会影响其他线程的正常执行 也可以 catch 住 panic，这样线程可以继续执行（参考标准库的：std::panic::catch_unwind）ResultRust 使用一个标准库定义的 Enum 类型 Result 来表达函数的正常返回和异常：enum Result&amp;lt;T, E&amp;gt; { Ok(T), Err(E),} 用 enum 的潜台词就是函数返回时，要么返回一个正常的结果，要么遇到一个可恢复异常。分别对正常结果，和异常处理的例子：fn main() { match get_weather(hometown) { Ok(report) =&amp;gt; { display_weather(hometown, &amp;amp;report); } Err(err) =&amp;gt; { println!(&quot;error querying the weather: {}&quot;, err); schedule_weather_retry(); } }}异常类型忽略的写法：#![allow(unused)]// 返回一个 () 或遇到一个异常fn remove_file(path: &amp;amp;Path) -&amp;gt; Result&amp;lt;()&amp;gt; {}这种写法用来表示这个 Module 中定义了 Result 类型的 type alias，省得到处都要写异常的具体类型（一般来说，某个 Module 中的异常都是相同类型的异常）：pub type Result&amp;lt;T&amp;gt; = result::Result&amp;lt;T, io::Error&amp;gt;;Result 配合 unwrap 的使用：#![allow(unused)]use std::fs::File;fn main() { // unwrap()：成功的话，从 Result&amp;lt;T&amp;gt; 中拿到 T，失败的话，panic // 不推荐使用，因为一般来说，不应该使用 panic let f = File::open(&quot;hello.txt&quot;).unwrap(); // 一般推荐的做法有 2 种： // 1) 使用 unwrap_or_else：成功拿到 T；失败的话，通过执行一个 closure 得到 T // 2) 或者把异常传递到调用者}Result 配合 expect 的使用：#![allow(unused)]use std::fs::File;fn main() { let f = File::open(&quot;hello.txt&quot;).expect(&quot;Failed to open hello.txt&quot;);}一个把异常传递到调用者的例子：use std::io;use std::io::Read;use std::fs::File;fn read_username_from_file() -&amp;gt; Result&amp;lt;String, io::Error&amp;gt; { let f = File::open(&quot;hello.txt&quot;); let mut f = match f { Ok(file) =&amp;gt; file, Err(e) =&amp;gt; return Err(e), // 第一个可能传递异常的位置 }; let mut s = String::new(); match f.read_to_string(&amp;amp;mut s) { Ok(_) =&amp;gt; Ok(s), Err(e) =&amp;gt; Err(e), // 第二个可能传递异常的位置 }}上面的例子，可以利用 ? 进行简化（功能完全相同，但代码更整洁）use std::io;use std::io::Read;use std::fs::File;fn read_username_from_file() -&amp;gt; Result&amp;lt;String, io::Error&amp;gt; { let mut f = File::open(&quot;hello.txt&quot;)?; let mut s = String::new(); f.read_to_string(&amp;amp;mut s)?; Ok(s)}还可以利用链式写法进一步简化：use std::io;use std::io::Read;use std::fs::File;fn read_username_from_file() -&amp;gt; Result&amp;lt;String, io::Error&amp;gt; { let mut s = String::new(); File::open(&quot;hello.txt&quot;)?.read_to_string(&amp;amp;mut s)?; Ok(s)}注意： 一个方法如果不返回 Result 类型，就不能使用 ? 来进行简化。实现 Error Trait在很多场景里面，使用 Rust 时，目前最好的实践还是自己实现一个 Error 类型（枚举或结构），也就是实现标准库里面的 Error Trait：pub trait Error: Debug + Display { /// The lower-level source of this error, if any. /// dyn Error 是一个 trait object /// trait object 作为 Rust 中的多态，这样既可以无需关注异常的具体类型，也可以通过 source 获取异常时具体的调用链 /// 如果该错误类型中包含了底层的错误 Err，那么 source 方法应该返回 Some(err), 如果没有返回 None。不重写则默认为 None fn source(&amp;amp;self) -&amp;gt; Option&amp;lt;&amp;amp;(dyn Error + &#39;static)&amp;gt; { None }}一个实现了 source，并利用 source 打印异常堆栈的例子：// A new error type, wrapping a sqlx::Errorpub struct StoreTokenError(sqlx::Error);impl std::error::Error for StoreTokenError { fn source(&amp;amp;self) -&amp;gt; Option&amp;lt;&amp;amp;(dyn std::error::Error + &#39;static)&amp;gt; { // The compiler transparently casts `&amp;amp;sqlx::Error` into a `&amp;amp;dyn Error` Some(&amp;amp;self.0) }}fn error_chain_fmt( e: &amp;amp;impl std::error::Error, f: &amp;amp;mut std::fmt::Formatter&amp;lt;&#39;_&amp;gt;,) -&amp;gt; std::fmt::Result { writeln!(f, &quot;{}\\n&quot;, e)?; let mut current = e.source(); while let Some(cause) = current { writeln!(f, &quot;Caused by:\\n\\t{}&quot;, cause)?; current = cause.source(); } Ok(())}impl std::fmt::Debug for StoreTokenError { fn fmt(&amp;amp;self, f: &amp;amp;mut std::fmt::Formatter&amp;lt;&#39;_&amp;gt;) -&amp;gt; std::fmt::Result { error_chain_fmt(self, f) }}另外的惯用法是，在一个 Module 中定义一个枚举错误类型来封装多种异常类型：#[derive(Debug)]pub enum SubscribeError { ValidationError(String), DatabaseError(sqlx::Error), StoreTokenError(StoreTokenError), SendEmailError(reqwest::Error),}然后用 From 来把其他错误类型转换为这个 Module 专用的错误类型。例如：impl From&amp;lt;sqlx::Error&amp;gt; for SubscribeError { fn from(e: sqlx::Error) -&amp;gt; Self { Self::DatabaseError(e) }}尽管通过以上代码可以实现一个 Error Trait，但还是太繁琐了。现在流行的做法是用库来自动生成 Error 类型，例如： thiserror snafu具体用法可以参考这些库的文档。日志 Rust 比较流行的「日志门面库」是：log 库（类似 Java 的 slf4j）use log::*; 真正输出日志，除了「日志门面库」之外，还需要「日志库」。这里重点介绍一下 tracing 库 由于很多场景，记录日志时，需要区分不同的「执行流」，并能在日志中查看属于某个「执行流」的所有日志。tracing 库引入了 span 概念 例如：在服务端，按某一次用户请求来记录日志，然后可以在服务端日志里面查看这次请求的执行流程。那么服务端对这次请求处理的开始到结束就是一个 span 也就是说，可以利用 tracing 库来跟踪「逻辑上下文」 tracing 使用「结构化数据」来记录「逻辑上下文」的信息（一般来说就是 kv 对）。然后在最后输出的每条日志中，同时输出这个 kv 对的值。例如如下输出（每条日志都输出了每次请求的 client.addr 的值，用来标明每条日志的「逻辑上下文」）：DEBUG server{client.addr=106.42.126.8:56975}: accepted connectionDEBUG server{client.addr=82.5.70.2:53121}: closing connectionDEBUG server{client.addr=89.56.1.12:55601} request{path=&quot;/posts/tracing&quot; method=GET}: received requestDEBUG server{client.addr=111.103.8.9:49123}: accepted connectionDEBUG server{client.addr=106.42.126.8:56975} request{path=&quot;/&quot; method=PUT}: received requestDEBUG server{client.addr=113.12.37.105:51342}: accepted connection WARN server{client.addr=106.42.126.8:56975} request{path=&quot;/&quot; method=PUT}: invalid request headersTRACE server{client.addr=106.42.126.8:56975} request{path=&quot;/&quot; method=PUT}: closing connection最后给一个异步代码使用 tracing 的例子：#![allow(unused)]fn main() {// 对每个用户请求 spawn 一个异步任务 tokio::spawn(async move { let fd = socket.as_raw_fd(); if let Err(err) = process(socket, fd, &amp;amp;mut cli).await { error!(&quot;this client has an error, disconnect it {}!&quot;, err); } }); // 然后利用 tracing 的 instrument 宏，指明用于「逻辑上下文」的数据为 fd #[instrument(skip(socket, cli))] pub async fn process(socket: TcpStream, fd: i32, cli: &amp;amp;mut reqwest::Client) -&amp;gt; Result&amp;lt;()&amp;gt; { // 最后日志输出的时候，都会带上 fd 的值作为 info!(&quot;the server accepted a new client. fd is: {}&quot;, fd); // 例如这个日志输出： // 2022-04-26T08:00:28.904581Z INFO process{fd=10}: rmr: the server accepted a new client. fd is: 10 }}traits 在 Rust 中，Trait 就类似 Java 的 Interface（接口）的作用 使用 trait 时必须先 use 它。但 Clone 和 Iterator 是例外，可以不导入也能用，这是因为 Clone 和 Iterator 在 standard prelude 中 standard prelude（标准前置模块）：Rust 会把「标准前置模块」自动导入到所有 module 中；所有可以直接用，无需导入 trait 作为函数参数的例子（注意 impl 关键字的使用）：// item 可以是实现了 Summary trait 的类型pub fn notify(item: impl Summary) { println!(&quot;Breaking news! {}&quot;, item.summarize());} 给一个和上面 impl 等价的使用了泛型参数声明的写法：pub fn notify&amp;lt;T: Summary&amp;gt;(item: T) { println!(&quot;Breaking news! {}&quot;, item.summarize());} 指定多个 trait bound 的例子：#![allow(unused)]pub fn notify&amp;lt;T: Summary + Display&amp;gt;(item: T) {}等价：#![allow(unused)]pub fn notify(item: impl Summary + Display) {} 通过 where 简化的例子：#![allow(unused)]fn some_function&amp;lt;T, U&amp;gt;(t: T, u: U) where T: Display + Clone, U: Clone + Debug{} Rust 中可以使用 trait 来实现多态（polymorphism）的效果。利用 trait 来达到多态的效果有 2 种方法： Trait Objects：A reference to a trait type is called a trait object 翻译：trait object 就是对一个 trait 类型的引用 trait object 是 fat pointer，由 2 部分组成： 指向 value 的指针 执行 value 的类型的指针 Generic Type Parameter：使用泛型作为函数类型参数 给一个 trait object 的例子：#![allow(unused)]fn main() { let mut local_file = File::create(&quot;hello.txt&quot;)?; // 函数入参 &amp;amp;mut local_file 是一个 &amp;amp;mut File 类型的引用 // 而 say_hello 的入参定义是一个 trait object：&amp;amp;mut dyn Write // 这里隐含了一个 Rust 的自动转换：&amp;amp;mut File =&amp;gt; &amp;amp;mut dyn Write say_hello(&amp;amp;mut local_file)?;}给一个泛型作为函数类型参数的例子：#![allow(unused)]use std::hash::Hash;use std::fmt::Debug;// 这里的类型 T 必须同时是 3 个 traits 的实现：Debug，Hash，Eqfn top_ten&amp;lt;T: Debug + Hash + Eq&amp;gt;(values: &amp;amp;Vec&amp;lt;T&amp;gt;) {} 利用 trait 来达到多态的效果的 2 种方法的对比： Trait Object： 如果一个集合中需要包含多种不同类型的元素，那么要用 trait object 另外，使用 trait object 可以节约编译时间（因为 trait object 是在运行时才确认指向的具体对象）；就是所谓的「dynamic dispatch」（dyn 关键字） Generic Type Parameter： 泛型类型参数的运行效率更高（泛型是在编译时展开，所以运行时无额外的代价） 其他一些 trait 相关的注意事项： 可以给 trait 提供一个 default 实现 Rust 允许在任何类型实现任何 trait；也就是说，我们甚至可以在基础类型（举例： char 类型）上实现新的 trait 也可以在某个泛型类型上实现 trait。给个例子：#![allow(unused)]// 在 Write 这个泛型类型上实现一个 WriteHtml traitimpl&amp;lt;W: Write&amp;gt; WriteHtml for W { fn write_html(&amp;amp;mut self, html: &amp;amp;HtmlDocument) -&amp;gt; io::Result&amp;lt;()&amp;gt; {}} orphan rule：can’t implement external traits on external types 不能给对外部类型实现外部 traits；这样 Rust 才能确保当前的这个 trait 的实现是「唯一」的（在一个 crate 内，一个 trait 不能同时有 2 个实现） 举个例子：在我们自己的 crate 里面不能对 Vec&amp;lt;T&amp;gt; 实现 Display trait（标准库里面已经在 Vec&amp;lt;T&amp;gt; 上实现了 Display trait） 下面的小节会介绍一些常用 TraitsDrop 当一个 value 的 owner 失效后，Rust 会 Drop 这个 value。具体成来说可能有几种场景： 变量的 scope 结束 vector 中的一个元素被删除 通常来说，Rust 已经提供了 Drop Trait 的实现（例如 vector/标准库中的文件相关） 但我们也可以自己重新实现 Drop Trait。但一旦实现了 Drop Trait，就不能再实现 Copy Trait 另外一个值得注意的是，Drop 只会被调用一次（move 的时候其实不会调用 Drop，只有 value 真正失效时，才会调用 Drop）Sized A sized type is one whose values all have the same size in memory 翻译：该类型中所有的 values 的 size 必须是相同 实现了 std::marker::Sized 这个 trait。Rust 已经都实现了，开发者不能再提供自己的实现 所以唯一的用途就是作为类型的 bound，比如使用泛型时的 bound enum 也是 sized 类型：尽管 enum 中各个变量的类型可能不同，但 Rust 会按最大所需的 size 给每个变量分配空间尽管 Rust 中绝大多数类型是 Sized 的，但有几种类型不是 Sized： slice type str (note, without an &amp;amp;) is unsized Array slice types like [T] (again, without an &amp;amp;) dyn type：the referent of a trait object unsized 类型不能作为函数参数，也不能直接用一个变量来 store 这个类型的值。只能通过引用或者指针来操作 unsized 类型的值CloneClone trait 定义如下：trait Clone: Sized { fn clone(&amp;amp;self) -&amp;gt; Self; fn clone_from(&amp;amp;mut self, source: &amp;amp;Self) { *self = source.clone() }} Clone 必然是工作在 Sized 类型上（该类型的大小可确定） 大多数时候，clone_from （clone 出来源的值，并替换掉 self）使用默认实现就行了：直接用 clone 来实现CopyRust 中，赋值时，默认的行为时 move。但可以通过实现 Copy trait （std::marker::Copy）来替换默认行为。Copy trait 定义如下：trait Copy: Clone {} 限制：实现了 Drop trait 的类型不能被 Copy #[derive(Copy, Clone)] 的使用Deref and DerefMutDeref 和 DerefMut 主要的用途有 2 个： 给自定义的类型提供类似 Rust「原生」的「解引用」操作 deref coercions，也就是可以实现「自动的」把一个类型的「引用」转换」成另外一个类型的「引用」先看「解引用」，回顾一下 Rust 中的「解引用」操作符号 * 的用法：fn main() { let x = 5; // &amp;amp; 引用就是 Rust 中的安全指针 let y = &amp;amp;x; assert_eq!(5, x); assert_eq!(5, *y);}通过实现 std::ops::Deref 和 std::ops::DerefMut 这 2 个 trait 来实现「自定义」类型的 * 「解引用」操作符号。例如 Box&amp;lt;T&amp;gt; 实现了 Deref 和 DerefMut，就可以如下使用：fn main() { let x = 5; // Box&amp;lt;T&amp;gt; 就是 Rust 中的智能指针 let y = Box::new(x); assert_eq!(5, x); assert_eq!(5, *y);}Deref 和 DerefMut 这 2 个 trait 大概定义如下：trait Deref { type Target: ?Sized; // 注意：这里返回的是一个「引用」 fn deref(&amp;amp;self) -&amp;gt; &amp;amp;Self::Target;}trait DerefMut: Deref { // 注意：这里返回的是一个「引用」 fn deref_mut(&amp;amp;mut self) -&amp;gt; &amp;amp;mut Self::Target;} 其中，通过 self 可以拿到（own 这个值，或者有这个值的「引用」）类型为 Target 的值，而且 deref 会返回这个值的「引用」，同时 2 者生命周期保持一致 而且，DerefMut 需要实现 Deref给一个 Box 的对 Deref 实现的例子：use std::ops::Deref;struct MyBox&amp;lt;T&amp;gt;(T);impl&amp;lt;T&amp;gt; MyBox&amp;lt;T&amp;gt; { fn new(x: T) -&amp;gt; MyBox&amp;lt;T&amp;gt; { MyBox(x) }}impl&amp;lt;T&amp;gt; Deref for MyBox&amp;lt;T&amp;gt; { type Target = T; // 注意，这里返回的是 Target 的引用 fn deref(&amp;amp;self) -&amp;gt; &amp;amp;Self::Target { &amp;amp;self.0 }}fn main() { let x = 5; let y: MyBox&amp;lt;i32&amp;gt; = MyBox::new(x); assert_eq!(5, x); assert_eq!(5, *y);}再看一下 deref coercions。deref coercions：one type is being “coerced” into behaving as another。也就是自动把一个类型的「引用」转换为另外一个类型的「引用」。简单点说，如果实现了 Defer（入参是 &amp;amp;self，返回是 &amp;amp;Self::Target），就有了自动 &amp;amp;U 转换为 &amp;amp;T 的效果。如果 Rust 发现当前使用 &amp;amp;U 会编译失败，需要 &amp;amp;T 才能编译通过；同时又发现该类型 U 提供的 Deref 实现了 &amp;amp;U 到 &amp;amp;T，Rust 就会自动做这种转换。例如： Rc&amp;lt;T&amp;gt; 实现了 Deref&amp;lt;Target=T&amp;gt;。对 Rc&amp;lt;String&amp;gt;，如果要调用 String::find 方法，r.find(&#39;?&#39;) 可以通过编译（等价于 (*r).find(&#39;?&#39;)） Rust 自动做了转换：&amp;amp;Rc -&amp;gt; &amp;amp;String String 实现了 Deref&amp;lt;Target=str&amp;gt;。所以可以 coerce 一个 &amp;amp;String 的值到 &amp;amp;str Vec&amp;lt;T&amp;gt; 实现了 Deref&amp;lt;Target=[T]&amp;gt;。所以可以把 bytes vector 传给一个入参是 slice &amp;amp;[u8] 的函数最后还是给一个 MyBox 的例子：use std::ops::Deref;struct MyBox&amp;lt;T&amp;gt;(T);impl&amp;lt;T&amp;gt; MyBox&amp;lt;T&amp;gt; { fn new(x: T) -&amp;gt; MyBox&amp;lt;T&amp;gt; { MyBox(x) }}impl&amp;lt;T&amp;gt; Deref for MyBox&amp;lt;T&amp;gt; { type Target = T; // 注意，这里返回的是 Target 的引用 fn deref(&amp;amp;self) -&amp;gt; &amp;amp;Self::Target { &amp;amp;self.0 }}fn main() { let x = 5; let y: MyBox&amp;lt;i32&amp;gt; = MyBox::new(x); assert_eq!(5, x); assert_eq!(5, *y); let m = MyBox::new(String::from(&quot;hello it&quot;)); // &amp;amp;MyBox&amp;lt;String&amp;gt; -&amp;gt; &amp;amp;String -&amp;gt; &amp;amp;str hello(&amp;amp;m);}fn hello(name: &amp;amp;str) { println!(&quot;OK, {}!&quot;, name);}Default实现了 Default（std::default::Default）这个 trait，可以通过实现这个 trait 来给类型设置默认值。trait 定义如下：trait Default { fn default() -&amp;gt; Self;}给个例子：impl Default for String { fn default() -&amp;gt; String { String::new() }}AsRef and AsMut这 2 个 traits 定义如下：trait AsRef&amp;lt;T: ?Sized&amp;gt; { fn as_ref(&amp;amp;self) -&amp;gt; &amp;amp;T;}trait AsMut&amp;lt;T: ?Sized&amp;gt; { fn as_mut(&amp;amp;mut self) -&amp;gt; &amp;amp;mut T;}当需要借用（borrow）一个 &amp;amp;T 的时候，可以直接传入实现了这 2 个 traits 的类型的值。给个例子：#![allow(unused)]// 该函数的本义是需要一个 &amp;amp;Path（借用 Path 的值）类型的入参fn open(path: &amp;amp;Path) -&amp;gt; Result&amp;lt;File&amp;gt; {}但事实上，为了方便，该函数的声明可以是下面这个样子：#![allow(unused)]// 入参是一个 AsRef&amp;lt;Path&amp;gt;，这样所有实现了 AsRef&amp;lt;Path&amp;gt; 的类型的值都可以传入这个函数（例如：String 和 str）fn open&amp;lt;P: AsRef&amp;lt;Path&amp;gt;&amp;gt;(path: P) -&amp;gt; Result&amp;lt;File&amp;gt; {}这种机制提供了引用间的类型转换，有点类似 C++ 中的函数参数的重载（overload）： 如果类型 U 实现了 AsRef&amp;lt;T&amp;gt;，则 as_ref 可以实现 &amp;amp;U 到 &amp;amp;T 的转换 如果类型 U 实现了 AsMut&amp;lt;T&amp;gt;，则 as_ref 可以实现从 &amp;amp;U 到 &amp;amp;mut T 的转换给个例子：fn print_ref(v: impl AsRef&amp;lt;str&amp;gt;) { println!(&quot;{}&quot;, v.as_ref());}#[allow(dead_code)]enum Msg { Hello, World,}impl AsRef&amp;lt;str&amp;gt; for Msg { fn as_ref(&amp;amp;self) -&amp;gt; &amp;amp;str { match self { Msg::Hello =&amp;gt; &quot;hello&quot;, Msg::World =&amp;gt; &quot;world&quot;, } }}fn main() { // s1 是 &amp;amp;str 类型, str 类型实现了 AsRef&amp;lt;str&amp;gt;，&amp;amp;str 也实现了 AsRef&amp;lt;str&amp;gt; let s1 = &quot;hello&quot;; // s2 是 String 类型, 实现了 AsRef&amp;lt;str&amp;gt;, &amp;amp;String 也实现了 AsRef&amp;lt;str&amp;gt; let s2 = String::from(&quot;word&quot;); // msg 是 Msg 类型, 实现了 AsRef&amp;lt;str&amp;gt;, &amp;amp;Msg 也实现了 AsRef&amp;lt;str&amp;gt; let msg = Msg::Hello; // str 类型实现了 AsRef&amp;lt;str&amp;gt; print_ref(s1); // hello // &amp;amp;str 类型实现了 AsRef&amp;lt;str&amp;gt; print_ref(&amp;amp;s1); // hello // &amp;amp;String 实现了 AsRef&amp;lt;str&amp;gt; print_ref(&amp;amp;s2); // world // String 实现了 AsRef&amp;lt;str&amp;gt; print_ref(s2); // world // &amp;amp;Msg 实现了 AsRef&amp;lt;str&amp;gt; print_ref(&amp;amp;msg); // hello // Msg 实现了 AsRef&amp;lt;str&amp;gt; print_ref(msg); // hello}Borrow and BorrowMutBorrow 的定义：trait Borrow&amp;lt;Borrowed: ?Sized&amp;gt; { fn borrow(&amp;amp;self) -&amp;gt; &amp;amp;Borrowed;}从其定义上看，Borrow 和 AsRef 非常类似。用一个例子说明 2 者的区别： String 实现了AsRef&amp;lt;str&amp;gt;，AsRef&amp;lt;[u8]&amp;gt;，和 AsRef&amp;lt;Path&amp;gt;，但这 3 个类型（str，u8，Path） 的 hash 值各不相同，不能保持一致。其中只有 &amp;amp;str slice 的 hash 值能和原类型 String 的 hash 值保持一致，所以 String 类型只会实现 Borrow&amp;lt;str&amp;gt;。Borrow trait 主要用于 hash 表，或其他的集合类型里的 key 值的 hash 值计算。给个例子：#![allow(unused)]impl&amp;lt;K, V&amp;gt; HashMap&amp;lt;K, V&amp;gt; where K: Eq + Hash { fn get(&amp;amp;self, key: &amp;amp;K) -&amp;gt; Option&amp;lt;&amp;amp;V&amp;gt; { // ... }}上面这个例子里面的实现是需要用一个 k 来获取 Map 的 v，但这里的 k 是 String 类型，传入了一个 &amp;amp;K。调用它的一个例子： hashtable.get( &amp;amp; “twenty-two”.to_string())该调用的开销如下： 分配空间（”twenty-two”） 拷贝字符串（”twenty-two”.to_string()） 执行 drop，销毁 String（调用结束）为了更高效，可以改进为：入参是类型 Q，只要类型 Q 满足 Eq + Hash。同时，Map 中的 K 需要满足约束：实现了 Borrow&amp;lt;Q&amp;gt;：#![allow(unused)]impl HashMap&amp;lt;K, V&amp;gt; where K: Eq + Hash{ fn get&amp;lt;Q: ?Sized&amp;gt;(&amp;amp;self, key: &amp;amp;Q) -&amp;gt; Option&amp;lt;&amp;amp;V&amp;gt; where K: Borrow&amp;lt;Q&amp;gt;, Q: Eq + Hash { // ... }}简单点说，就是利用 Borrow 这个 trait 来对类型 K 进行约束。From and Into 类型转换：把一个类型的值转换为另外一个类型的值 和 AsRef 不同，AsRef 进行的引用间的转换；而 From 和 Into 是值之间的转换 定义如下：trait Into&amp;lt;T&amp;gt;: Sized { fn into(self) -&amp;gt; T;}trait From&amp;lt;T&amp;gt;: Sized { fn from(other: T) -&amp;gt; Self;} 和 AsRef 差不多，Into 也有点像 overload（重载） From 常用来进行值的初始化input outputRust 的 IO 标准库重点就是 3 个 traits： Read：统称 Reader，具体的实现包括：Stdin，File，TcpStream BufRead：也是 Reader 的一种：就是 buffered Reader，继承了 Read trait；其内部有 buffer，这样读的效率会更高效。具体的实现包括：BufferReader，Cursor，StdinLock 如果需要在一个文件上重复读很多次，每次又只读少量数据，会比较低效；先读大块数据到 buffer 里面会更高效 比如 File 类型只实现了 Read，如果需要更高效的读文件，可以考虑使用实现了 BufRead 的 BufReader Write：统称 Writer，具体的实现包括：Stdout，Stderr，File。。。Read这里只重点介绍一些重点方法： reader.read(&amp;amp;mut buffer)：把 byte 数据读到指定的 buffer 中（buffer 空间事先已经分配好了），然后返回实际读到的数据的长度。如果出错的话，就返回 io::Error reader.read_to_end(&amp;amp;mut byte_vec)：把整个数据内容读到 byte_vec （Vec&amp;lt;u8&amp;gt;）中BufReadBufRead 的重点方法： reader.read_line(&amp;amp;mut line)：按 line 把数据读入到一个 String 类型中（结果包括行分割符：\\n，\\r\\n）。返回本次读到的 byte 数 reader.lines()：返回一个 iterator，然后通过迭代拿到一个 io::Result&amp;lt;String&amp;gt;；同时 \\n 不会被放到读到的 String 中一个例子，利用 BufReader（实现了 BufRead）来读取文件：use std::io::{self, BufReader};use std::io::prelude::*;use std::fs::File;fn main() -&amp;gt; io::Result&amp;lt;()&amp;gt; { // File 没有 buffer let f = File::open(&quot;foo.txt&quot;)?; // 使用带 buffer 的 BufReader let f = BufReader::new(f); // BufReader 实现了 BufRead，可以利用 lines() 方法来读取文件，更加高效 for line in f.lines() { println!(&quot;{}&quot;, line.unwrap()); } Ok(())}Write 利用 write!() 和 writeln!() 这 2 个宏来使用 Write Write 的重点方法： write(&amp;amp;buf)：把 byte 数据写出。成功的话，返回写入的大小；失败的话，返回 Error flush()：把数据 flush 到写入目标 Write 本身可以创建对应的 BufWriter。例如：BufWriter::new(writer)；并且 BufWriter 也继承了 Write trait BufWriter 被 drop 的时候，会自动把被 buffer 的数据写出。但如果出现 Error，不保证能成功，所以有可能需要显示调用 flush threads首先明确一点：线程适合同时消耗多个 CPU 并行计算的场景；而不适合同时等待多个 IO 请求（磁盘或网络）的场景。给一个最简单的例子：use std::thread;fn main() { let handle = thread::spawn(move || { // do stuff in a child thread }); // do stuff in the main thread // block until child thread has exited handle.join.upwrap();}一个复杂的例子：use log::{error, info};use std::{thread, time::Duration};fn sleep(seconds: f32) { thread::sleep(Duration::from_secs_f32(seconds));}pub mod dad { use super::{info, sleep}; pub fn cook_spaghetti() -&amp;gt; bool { info!(&quot;Cooking the spaghetti...&quot;); sleep(4.0); info!(&quot;Spaghetti is ready!&quot;); true }}pub mod mom { use super::{info, sleep}; pub fn cook_sauce_and_set_table() { sleep(1.0); info!(&quot;Cooking the sauce...&quot;); sleep(2.0); info!(&quot;Sauce is ready! Setting the table...&quot;); sleep(2.0); info!(&quot;Table is set!&quot;); }}fn main() { env_logger::init(); // dad 在工作子线程进行工作 let handle = thread::spawn(|| dad::cook_spaghetti()); // mom 在主线程进行工作 mom::cook_sauce_and_set_table(); // 主线程和工作子线程的工作都完成后，整体完成 if handle.join().unwrap_or(false) { info!(&quot;Spaghetti time! Yum!&quot;) } else { error!(&quot;Dad messed up the spaghetti. Order pizza instead?&quot;); }}ChannelsRust 线程间的通讯需要使用 channel。注意： 不推荐使用 std::sync::mpsc 推荐使用 crossbeam::channel（性能好，功能多；唯一的缺点可能只有不是标准库） A channel is a one-way conduit for sending values from one thread to another（Rust 保证 channel 线程安全）channel 又分为有界队列和无界队列 2 种： channel::bounded(8)：队列长度为 8；如果队列长度大于 8，生产者（sender）会被阻塞；直到消费者（receiver）消费后才能解开 channel::unbounded()：无界队列；不会阻塞生成者，但发生突发负载时，系统可能爆 同一个 channel 可以有多个生产者和多个消费者一个完整的例子：use crossbeam::channel::{self, Receiver, Sender};use std::{thread, time::Duration};#[derive(Debug)]enum Lunch { Soup, Salad, Sandwich, HotDog,}// 不断接受 orders 队列过来的任务，然后对不同类型的食物做一个对应的处理，最后处理好的食物送给 lunches 队列fn cafeteria_worker(name: &amp;amp;str, orders: Receiver&amp;lt;&amp;amp;str&amp;gt;, lunches: Sender&amp;lt;Lunch&amp;gt;) { // 一旦 orders 队列所有的 senders 都被关闭，且队列中的数据被消费完，循环结束 for order in orders { println!(&quot;{} receives an order for {}&quot;, name, order); let lunch = match &amp;amp;order { x if x.contains(&quot;soup&quot;) =&amp;gt; Lunch::Soup, x if x.contains(&quot;salad&quot;) =&amp;gt; Lunch::Salad, x if x.contains(&quot;sandwich&quot;) =&amp;gt; Lunch::Sandwich, _ =&amp;gt; Lunch::HotDog, }; for _ in 0..order.len() { thread::sleep(Duration::from_secs_f32(0.1)) } println!(&quot;{} sends a {:?}&quot;, name, lunch); // lunches 队列被关闭的话，整个循环结束 if lunches.send(lunch).is_err() { break; } }}fn main() { // 新建 2 个 channels：orders 和 lunches let (orders_tx, orders_rx) = channel::unbounded(); // orders 队列有 2 个 receivers let orders_rx2 = orders_rx.clone(); let (lunches_tx, lunches_rx) = channel::unbounded(); // lunches 队列有 2 个 senders let lunches_tx2 = lunches_tx.clone(); // 创建 2 个工作子线程负责处理任务： // orders_rx 从 orders 里面接受任务，处理完成后再使用 lunches_tx 发送给 lunches // orders_rx2 从 orders 里面接受任务，处理完成后再使用 lunches_tx2 发送给 lunches let alice_handle = thread::spawn(|| cafeteria_worker(&quot;alice&quot;, orders_rx2, lunches_tx2)); let zack_handle = thread::spawn(|| cafeteria_worker(&quot;zack&quot;, orders_rx, lunches_tx)); for order in vec![ &quot;polish dog&quot;, &quot;caesar salad&quot;, &quot;onion soup&quot;, &quot;reuben sandwich&quot;, ] { println!(&quot;ORDER: {}&quot;, order); // orders 只有 1 个 sender（主线程） let _ = orders_tx.send(order); } // 不会清空已经发送到队列的数据，但会关闭这个 sender。之后，一旦队列所有的 senders 都已经关闭，消费者的循环会结束 drop(orders_tx); // lunches 只有 1 个 receiver（主线程） for lunch in lunches_rx { println!(&quot;Order Up! -&amp;gt; {:?}&quot;, lunch); } // 最后，简单的退出子线程 let _ = alice_handle.join(); let _ = zack_handle.join();}Arc Arc&amp;lt;T&amp;gt;：原子引用计数 Arc&amp;lt;T&amp;gt; 是一种「智能指针」，和 Rc&amp;lt;T&amp;gt; 功能上类似，不同之处只是因为是原子计数，所以能保证线程安全 对一个分配在堆上的 T 值，可以有多个 Arc&amp;lt;T&amp;gt; 指针指向这个 T 值。只有当指向这个 T 值的所有指针都被销毁后，这个 T 值才也被销毁（引用计数为 0） 一个简单的例子：use std::thread;use std::sync::Arc;use std::time::Duration;fn main() { // foo 指向一个值 let foo = Arc::new(vec![0]); // bar 也指向这个值（也就是引用计数加一） let bar = Arc::clone(&amp;amp;foo); // 启动一个子线程，这个子线程内部，会在 20 毫秒后，发生一次 move，使用这个值，然后引用计数减一 thread::spawn(move || { thread::sleep(Duration::from_millis(20)); println!(&quot;{:?}&quot;, *bar); }); // 主线程会立即进行 move，使用这个值，然后引用计数减一 // 但此时引用计数还没有被减到 0，所以这个值在主线程执行之后不会被销毁 println!(&quot;{:?}&quot;, foo);} 默认场景 Arc&amp;lt;T&amp;gt; 不能是可变的（mut）。如果需要可变，可以配合 Mutex 使用一个简单的配合 Mutex 使用的例子：use std::sync::{Mutex, Arc};use std::thread;fn main() { let counter = Arc::new(Mutex::new(0)); let mut handles = vec![]; for _ in 0..10 { // 引用计数加一 let counter = Arc::clone(&amp;amp;counter); // 启动一个新子线程，子线程内部操作获取 Mutex 的 lock，然后对其中的数据 num 进行先读后写 // 这里不能直接使用 Mutex，而是要套一层 Arc； // 因为直接使用 Mutex 的话，一旦前一次循环把 Mutex 的值 move 走，后面的循环就不能再使用这个 Mutex let handle = thread::spawn(move || { let mut num = counter.lock().unwrap(); *num += 1; // 当前线程结束后，会释放 lock，其它子线程能再获取这个 lock }); handles.push(handle); } // 等待所有子线程完成 for handle in handles { handle.join().unwrap(); } // 最后输出：Result: 10 println!(&quot;Result: {}&quot;, *counter.lock().unwrap());}async先明确几个概念： Future：对计算本身的抽象（重点关注 what：计算什么） Future 只是描述计算本身：”开始做 X，等到 X 做成功后，再做 Y“（而不是「过程式」的运行一段代码：”先执行 X，执行 X 成功后，执行 Y“） 简单点说，就是用来描述一个状态机 Future 本身不执行代码，Future 只有配合 Executor 才能真正的把代码运行起来，推进状态机 Executor：用来真正的把 Future 执行起来（重点关注：when &amp;amp; how：计算什么时候执行，怎么执行） 当前 Rust 语言本身只定义 Future 等异步组件相关的 traits，Executor 的实现由异步运行时（async_std，tokio 等开源库）负责 FutureRust 中，Future 是一个 trait，其定义大致（简化版）如下：trait Future { type Output; fn poll(&amp;amp;mut self) -&amp;gt; Poll&amp;lt;Self::Output&amp;gt;;}其中 Poll 的定义大致如下：enum Poll&amp;lt;T&amp;gt; { // Future 完成时返回 Ready Ready(T), // Future 还没有完成的话，返回 Pending Pending}When a future eventually returns Poll::Ready(T), we say that the future resolves into a T 翻译：当一个 Future 实例完成后，会返回 Poll::Ready(T)，这时我们可以说这个 Future 实例被 resolve 成一个类型 T 的返回值为了更容易理解，Future 还可以分为 2 类： leaf Futures（底层 Future）：直接封装某种 IO 操作的异步调用（由 async_std，tokio 等开源库实现）。tokio 库的例子：// 异步的创建 socket 连接，并返回一个 **leaf-future**：streamlet mut stream = tokio::net::TcpStream::connect(&quot;127.0.0.1:3000&quot;); non-leaf Futures（非底层 Future）：多个 Futures 组合起来描述一个异步任务，事实上是一个状态机（开发者进行开发）。例如：#![allow(unused)]fn main() {// 创建了一个异步任务（事实上是一个有 4 个状态的状态机）// 其中用到了 async/.await（这里只是先举例，接下来会对它们详细介绍）// 最后成功的构建出一个 non-leaf Future let non_leaf = async {// 状态 1：创建 socket 连接。这里使用了一个 leaf Future，并进行等待 let mut stream = TcpStream::connect(&quot;127.0.0.1:3000&quot;).await.unwrap();// 状态 2: socket 连接成功 println!(&quot;connected!&quot;);// 状态 3：异步写文件，并等待中 let result = stream.write(b&quot;hello world\\n&quot;).await;// 状态 4: 写完成 println!(&quot;message sent!&quot;); };}async/.await怎么创建一个 non-leaf 类型的 Future 来描述异步任务（状态机）？Rust 提供了 async/.await 机制供开发者使用，可以利用它们构建一个描述异步任务的 Future。先分别给一下同步读文件和异步读文件的例子，对比一下看看。同步：#![allow(unused)]fn main() { use std::{fs::File, io, io::prelude::*}; fn read_file(path: &amp;amp;str) -&amp;gt; io::Result&amp;lt;String&amp;gt; { let mut file = File::open(path)?; let mut contents = String::new(); file.read_to_string(&amp;amp;mut contents)?; Ok(contents) }}异步（使用 async_std 异步运行时库）：#![allow(unused)]fn main() { extern crate async_std; use async_std::{fs::File, io, io::prelude::*}; // 实际上时返回一个 Future：Future&amp;lt;Output = io::Result&amp;lt;String&amp;gt;&amp;gt; async fn read_file(path: &amp;amp;str) -&amp;gt; io::Result&amp;lt;String&amp;gt; { // 可以把整个 read_file 异步函数理解成一个状态机。 // File::open(path) 本身返回一个 Future // .await 是关键，使用了 .await 后，当真正执行到这里时，状态机进入等待状态 // 直到 open 成功后，能拿到 Ready(T) 中的 T：file。当前状态结束，状态机继续推进到下个状态 let mut file = File::open(path).await?; let mut contents = String::new(); file.read_to_string(&amp;amp;mut contents).await?; Ok(contents) }} NOTE：如果 executor 有多个线程，那么 Future 恢复执行后有可能会到另外一个线程里去执行，需要注意线程安全（互斥和死锁） 通过 async/.await 机制创建异步任务（也就是 non-leaf 类型的 Futures）的惯用法： 开发者使用 async fn 来定义一个异步函数，异步函数返回一个 non-leaf 类型的 Future 然后需要由异步运行时库来真正执行这个 Future 在异步函数内部，需要使用「异步」版本的 IO 函数（由异步运行时提供） 「异步」版本的 IO 函数不会直接返回 IO 操作结果，而是会返回一个异步 IO Future（也就是 leaf Futures） 再配合 .await 使用异步 IO Future，执行时等待异步 IO 成功后，会返回异步 IO 的结果 生命周期 async fn 的入参如果是 non-‘static 参数（非 static 生命周期的参数），那么这个 async fn 返回的 Future 的生命周期需要被限制在函数入参的生命周期之内 换句话说，当在这个 Future 上做 .await 的时候，当初调用这个 async fn 时的入参的生命周期必须仍然有效 async move 代码块：// `async move` block:// 如果某个 `async move` block 使用了外部的 my_string 这个变量后, 其他的 `async move` block 就不能使用这个变量了fn move_block() -&amp;gt; impl Future&amp;lt;Output=()&amp;gt; { let my_string = &quot;foo&quot;.to_string(); async move { // ... println!(&quot;{}&quot;, my_string); }}PinPin 机制最重要的用途就是用来保证 Rust 异步机制的安全。本节深入介绍 Pin 的细节。可移动进入本节之前，建议参考 move 对 move 的概念进行回顾。用一个例子再详解一下 move 的细节：#![allow(unused)]fn main() { // s1 (变量) 是 String `hello` (值) 的 owner // 底层的实现大概是：`hello` 被存放在分配在 heap 上的空间 // s1 是个肥指针：记录了「字符串长度」，「字符串存放空间地址」等信息 let s1 = String::from(&quot;hello&quot;); // 进行 move：s1 记录的信息被 copy 给了 s2（「字符串长度」，「字符串存放空间地址」） // 但字符串内容（`hello`）本身地址不会变动 // 移动以后可以使用 s2 完全没有问题 // 这是因为 String `hello` 本身的地址没有变化，可以通过 s2 被安全的继续使用 let s2 = s1;}上面例子中的 s1 move 到 s2 具体动作如下图（从 TRPL 书摘录）所示：就上面的例子来说，String 类型是一个「可移动」（movable）类型。 可移动（movable）：所谓一个类型「可移动」，是指一旦拿到这个类型的「值」的 ownership 或 &amp;amp;mut（独占指针），并进行 move 操作，而不会引起问题（不会发生未定义行为）正常来说，就像上面的 String 类型，Rust 中的所有类型都应该是「可移动」的。但有例外的场景，某些类型被移动后，会有问题。比如对 Self-Referential Structs（自引用 structs）类型进行 move 就可能引起问题（发生未定义行为）。举个 Self-Referential Structs 的例子（一个字段指向另外一个字段）：struct Test { a: String, b: *const String,}自引用 structs 的 move 问题的细节请参考：Pinning in Detail，看了它里面的图就一目了然了：总之 Self-Referential Structs 可能会引起 move 后的指针无效问题。该问题可能的解决方法有： 每次 move 时，修改指针指向的地址；但这个方法会影响运行时的性能，代价较高 指针不储存绝对地址，只储存偏移量；这样需要编译器针对 Self-Referential Structs 做专门的处理，编译器的实现会比较复杂 Rust 采用 Pin 机制来解决问题：开发者负责把不能被 move 的类型标记出来；利用类型系统对这些被标记了的类型进行限制：使得没有办法对这些类型做 move 动作 运行时付出的代价为 0，不影响性能 代价就是开发者需要学习 Pin 的用法 Pin 的定义 核心理念： 要限制一个类型 T 不能被 move，也就是要对这个类型 T 的访问进行限制：只要不能拿到到这个类型 T 的 ownership 或者 &amp;amp;mut T（独占指针），也就不能对这个类型 T 做 move 操作。 在 Pin 机制中，只要利用 Pin 把这个类型 T 包起来（或者说屏蔽起来）就能实现这个限制效果。Pin 的定义：// 一个包了指针的 structpub struct Pin&amp;lt;P&amp;gt; { pointer: P,}其中： Pin 自己是一个指针，Pin 本身实现了 Deref 和 DerefMut， P 必须是一个指针，也就是实现了 Deref 或 DerefMut 的类型。例如：Box&amp;lt;T&amp;gt;怎么利用 Pin 把需要限制的类型 T 包起来（屏蔽起来）？ 因为 P 只能包一个指针，所以先要构建一个指向 T 的指针 P。可以构建 2 种指针： &amp;amp;mut T：「可变引用」实际上就是 T 的「独占指针」 Box&amp;lt;T&amp;gt;：使用智能指针 Box 然后再用 Pin 把构建好的指针类型 P 包起来。既然有 2 种指针，那么也有 2 种 Pin： Pin&amp;lt;&amp;amp;mut T&amp;gt;：但这种坑多，使用起来需要很小心，先不推荐使用这种方法 Pin&amp;lt;Box&amp;lt;T»：可以使用标准库 Box::pin 函数来构建。得到一个在 heap 上的 T 的值，然后这个值被 Pin 屏蔽住，推荐使用这种方法 Unpin and !Unpin上一节已经说明了通过 Pin 机制，可以把「不可移动」的类型 T 封装到 Pin 中，这样就没有办法获取到 T 的 ownership 或者 &amp;amp;mut T（独占指针），进而保证不能对 T 进行 move。但实际上 Pin 是和 Unpin trait 组合在一起使用的，所以还需要详细介绍 Unpin（以及 !Unpin）的概念。 Unpin trait 是一种 auto trait。简单点说，Rust 中的各种类型，要么实现了 Unpin，要么实现了 !Unpin（事实上，Rust 中大多数类型都已经默认实现了 Unpin trait）。配合 Unpin 和 !Unpin，Pin 的使用原则如下：原则一： Unpin Types can be safely moved after being pinned。 如果类型 T 是「可移动」类型，那么需要给该类型 T 实现 Unpin trait。 一旦类型 T 实现了 Unpin trait，那么即使用 Pin 包住这个类型 T（例如 Pin&amp;lt;&amp;amp;mut T&amp;gt;），也不会对 T 有屏蔽效果。 还是可以从 Pin 中拿到 T 的 ownership 或者 &amp;amp;mut T（独占指针），并进行 move。原则二： Guarantee that an object implementing !Unpin won’t ever be moved。 只有当 Pin 包住的类型 T（例如 Pin&amp;lt;Box&amp;lt;T» 中的 T）实现了 !Unpin trait，才无法获取到 T 的 ownership 或者 &amp;amp;mut T（独占指针），从而达到了对 T 的屏蔽效果，没办法对类型 T 进行 move。原则三： 一个 struct 类型 T 只要有一个 field 是 !Unpin 的，这个 struct 类型 T 就是 !Unpin 的。而 Rust 中绝大多数正常类型，都是「可移动」的，默认都已经实现了 Unpin。比如 String 类型，也已经默认实现了 Unpin。按照上面的「原则一」，即使被 Pin 包住，也不会并屏蔽，可以使用多种方法从 Pin 中拿到 String 进行操作:#![allow(unused)]fn main() { let mut string = &quot;Pinned?&quot;.to_string(); // 构建 Pin&amp;lt;&amp;amp;mut T&amp;gt; let mut pinned: Pin&amp;lt;&amp;amp;mut String&amp;gt; = Pin::new(&amp;amp;mut string); // String 实现了 Unpin，所以不会被 Pin 屏蔽 // 可以直接从 Pin&amp;lt;&amp;amp;mut T&amp;gt; 拿到内部真正的 T String，进行操作 pinned.push_str(&quot; Not&quot;); // 也提供了 Pin::into_inner 方法来返回指针 P（相当于可以把外层包的 Pin 去掉） Pin::into_inner(pinned).push_str(&quot; so much.&quot;); let new_home = string; assert_eq!(new_home, &quot;Pinned? Not so much.&quot;);}特别注意： Box&amp;lt;T&amp;gt; 也是「可移动」的，也实现了 Unpin Box&amp;lt;T&amp;gt; 是一个 heap 上的指针，所以可以安全的移动 但 T 可能「不可移动」，可能实现了 !Unpin Pin&amp;lt;Box&amp;lt;T» 也是「可移动」的，也实现了 Unpin Pin 的 field 是 Unpin 的 Box&amp;lt;T&amp;gt;，所以 Pin 也是 Unpin 的 但 T 可能「不可移动」，可能实现了 !Unpin 在 Rust 中，真正实现了 !Unpin trait 的只有 2 个类型（这里只先提一下，下一节会详细讲）： Future 转换为状态机时，编译器生成 Self-Referential Structs 来保存状态机上下文数据，编译器会给这些 Self-Referential Structs 实现 !Unpin trait 标准库中的 std::marker::PhantomPinned 类型也实现了 !Unpin traitFuture and Pin Rust 里面，使用 Future 来描述一个状态机 编译器把 async fn 或 async block 转换为状态机代码时，使用到了 Self-Referential Structs 后续使用该 Future 的人如果不小心对这个 Future 进行了 move，就会出问题 所以需要使用 Pin 机制来保证 Future 的安全（不会被 move）先简单举例说明怎么把异步代码（async fn 或 async block）转换为状态机（更多细节请参考：The Async/Await Pattern）。这里举个 async block 的简单例子：#![allow(unused)]fn main() { async { let mut x = [0; 128]; let read_into_buf_fut = read_into_buf(&amp;amp;mut x); read_into_buf_fut.await; println!(&quot;{:?}&quot;, x) }}编译器在对这个 async block 生成状态机： 构建一个有 3 个状态的状态机：StartState，WaitingOnReadState，EndState 对每个状态，需要定义对应的 struct 来保存该状态的上下文信息：// 整个 async block 被转换为一个 future：struct AsyncFuture { // future 中包括了 async block 内部使用到的数据： x: [u8; 128], // future 中也需要包含状态机工作时，每个状态的上下文信息（只有记录了每个状态的上下文信息，状态机才能被不断推进） // 本例子中，我们只重点关心 WaitingOnReadState 状态的上下文 waiting_on_read_state: WaitingOnReadState,}// 定义一个 WaitingOnReadState 状态的上下文信息的 structstruct WaitingOnReadState&amp;lt;&#39;a&amp;gt; { // 指向了 AsyncFuture 的 `x` 字段，所以 AsyncFuture 是一个 Self-Referential Struct buf: &amp;amp;&#39;a mut [u8],}可以看出，async block 的状态机的底层实现中，会把 Future 实现为 Self-Referential Structs（自引用 struct），只能利用 Pin 机制来防止使用 Future 的人对 Future 进行 move。异步运行时异步运行时由 Rust 社区提供，它们的核心是一个 Reactor 和一个或多个 Executor： Reactor 负责封装底层的事件通知，例如 异步 IO，异步 Sleep 等 Executor 负责执行 Future（前面的小节已经介绍过） Executor 和 Reactor 利用 Waker（Rust 标准库定义）进行交互： Executor 执行 Future 进行 poll，并把 waker 作为调用时的入参 Reactor 执行底层 IO 操作，并接收到 waker 当 IO 完成时，Reactor 利用 waker 通知 Executor 继续推进 Future 的执行 目前有一些流行的「异步运行时」，本节重点介绍 Tokio（值得注意的是，一旦你的项目选择了某个「异步运行时」，不太可能后续又能切到其他的「异步运行时」）。main 函数使用 Tokio 时，需要用 #[tokio::main] 宏。会把 async fn main 转换为 fn main，以及做一些异步环境的初始化工作。最简单的例如：#[tokio::main]async fn main() { println!(&quot;hello&quot;);}异步任务使用 tokio::spawn 创建「异步任务」。tokio::spawn会返回一个 JoinHandle 对象，然后开发者可以通过 JoinHandle 使用这个「异步任务」。最简单的例子：#[tokio::main]async fn main() { // spawn 入参是 async block // async block 就是这个「异步任务」要做的事情（本身是返回一个 Future） // 返回一个 JoinHandle（handle）对应这个「异步任务」 let handle = tokio::spawn(async { 10086 }); // JoinHandle 本身也是一个 Future // 下面真正的执行这个「异步任务」（执行由「Executor」负责） let out = handle.await.unwrap(); println!(&quot;GOT {}&quot;, out);}Mutex「异步任务」间数据同步的安全保证，可使用「互斥」。Mutex 的用法可参考之前的小节（Arc）。更多底层细节Future trait 真实定义如下：#![allow(unused)]fn main() { use std::pin::Pin; use std::task::{Context, Poll}; pub trait Future { type Output; fn poll(self: Pin&amp;lt;&amp;amp;mut Self&amp;gt;, cx: &amp;amp;mut Context) -&amp;gt; Poll&amp;lt;Self::Output&amp;gt;; }}详解： Output 是 Future 执行完成后返回的值的类型 开始调用 poll() 方法之前，必须先用一个 Pin 类型把 Future 包装起来才能调用 poll() 方法 用一个 Pin 类型把 Future 包装起来的目的是防止这个 Future 实例被 move 由第 3 方异步运行时库提供的 Executor 负责执行 Future： 整个状态机由一个最外层 Future，和其它内部 Futures 一起组成 Executor 先做第一次 poll，启动状态机 Executor 会继续调用 poll，推进状态机，直到整个任务完成 但如果 Executor 通过循环重试的方式来不断 poll 效率太低。高效的方式是通过某种通知机制，当 Future 已经就绪时，才去做 poll。所以又引入了 Waker： 当 Executor 调用 poll 的时候需要提供一个 Context 参数，这个 Context 参数就包含了 Waker 未来当 Future 完成时，可以获取到 Waker（waker = cx.waker()），然后调用 waker.wake() 来通知 Executor 执行 poll 具体 waker.wake() 要怎么通知，由 Executor 实现： 一种可能的实现，就是调用 wake() 把就绪的任务加到就绪队列，Executor 消费就绪队列中已完成任务，进行 poll 实现要保证线程安全 先展示一个简单的，不断 wake Executor 做 poll 的例子：use std::{ future::Future, pin::Pin, task::{Context, Poll},};// 这个例子中，我们使用 tokio 作为运行时来执行 Future#[tokio::main]async fn main() { let fut = MyFuture {}; println!(&quot;Awaiting fut...&quot;); fut.await; println!(&quot;Awaiting fut... done!&quot;);}// 声明一个自己的 Futurestruct MyFuture {}// 实现 Future traitimpl Future for MyFuture { // 该 Future 返回 () type Output = (); fn poll(self: Pin&amp;lt;&amp;amp;mut Self&amp;gt;, cx: &amp;amp;mut Context&amp;lt;&#39;_&amp;gt;) -&amp;gt; Poll&amp;lt;Self::Output&amp;gt; { println!(&quot;MyFuture::poll()&quot;); // 如果没有 wake_by_ref() 的话，poll 就只会被调用一次 // 现在加上了 wake_by_ref()，每次 poll 时，会再次通知 Executor 进行下一次 poll，Executor 会再次调用 poll cx.waker().wake_by_ref(); // 但该例子中，无论 poll 多少次都返回 Pending，不会返回 Ready：无限次调用 poll Poll::Pending }}执行以后的无限次 poll 的效果如下：$ cargo run --quietAwaiting fut...MyFuture::poll()MyFuture::poll()MyFuture::poll()MyFuture::poll()MyFuture::poll()MyFuture::poll()MyFuture::poll()MyFutur^C然后再展示一个例子：启动一秒以后，能返回 Ready 的 Future：use tokio::time::Sleep;use tokio::time::Duration;use std::{ future::Future, pin::Pin, task::{Context, Poll},};#[tokio::main]async fn main() { let fut = MyFuture::new(); println!(&quot;Awaiting fut...&quot;); fut.await; println!(&quot;Awaiting fut... done!&quot;);}struct MyFuture { // Sleep 是一个 Future，不能直接调用 poll()， // 需要 Pin 和 Box 组合起来才能调用 poll（下面一个小节会详细说） sleep: Pin&amp;lt;Box&amp;lt;Sleep&amp;gt;&amp;gt;,}impl MyFuture { fn new() -&amp;gt; Self { Self { // 使用 tokio 异步版本的 sleep // Box::pin 创建一个 Pin&amp;lt;Box&amp;lt;T&amp;gt;，如果 T 没有实现 Unpin 的话，Pin 成功 // 本例子 Pin&amp;lt;Box&amp;lt;Sleep&amp;gt;&amp;gt; 中的 Sleep 类型就是没有实现 Unpin 的类型 sleep: Box::pin(tokio::time::sleep(Duration::from_secs(1))), } }}impl Future for MyFuture { type Output = (); fn poll(mut self: Pin&amp;lt;&amp;amp;mut Self&amp;gt;, cx: &amp;amp;mut Context&amp;lt;&#39;_&amp;gt;) -&amp;gt; Poll&amp;lt;Self::Output&amp;gt; { println!(&quot;MyFuture::poll()&quot;); // 这个一个典型的惯用法：Future 的实现里面对另外一个 Future 进行 poll // // 因为异步版本的 sleep 内部实现了利用 waker 来通知 Executor // 所以可以直接对异步版本的 sleep 进行 poll // // 异步版本的 sleep 的 poll()，需要 Pin 和 Box 组合起来调用 poll() self.sleep.as_mut().poll(cx) }}执行效果如下：$ cargo run --quietAwaiting fut...MyFuture::poll()这里会卡 1 秒MyFuture::poll()Awaiting fut... done! 另外一个之前例子没有覆盖的 Future 内部实现细节：每次 Executor 调用 poll 时，都会传入一个的 waker 参数；poll 的内部需要判断这次的 waker 和之前 poll 被调用时传入的 waker 的值是否匹配。代码大概如下：fn main() { // 获取原先的 waker let mut waker = waker.lock().unwrap(); // 判断原先的 waker 和本次调用传入的 waker 是否匹配 if !waker.will_wake(cx.waker()) { // 如果不匹配，需要 clone 本次传入的值，并记录到 Future 内部 *waker = cx.waker().clone(); }}closure closure 就是匿名函数，可以赋值给变量，也可以作为函数的参数来使用。例如，可以把 closure 作为函数参数的一些库方法： Iterator thread::spawn 一般来说，Rust 会自动推断 closure 用到的类型。但也可以使用「类型标注」标明其类型#![allow(unused)]fn main() { use std::thread; use std::time::Duration; // 分别标明了参数和返回值的类型：u32 let expensive_closure = |num: u32| -&amp;gt; u32 { println!(&quot;calculating slowly...&quot;); thread::sleep(Duration::from_secs(2)); num };} closure 和函数的不同在于，closure 不仅仅可以使用入参，也可以捕获（capture）并使用调用者作用域中的值 这里可以参考一下后面提到的 Fn，FnMut，FnOnce fn main() { let x = 1; // sum 是一个 closure。这个 closure 捕获并使用了作用域中 x 的值（也就是 1） // 注意：这里只是定义了 closure，并不会执行。closure 只有在被调用的时候才会真正执行 let sum = |y| x + y; // 执行 sum，计算结果是 3 assert_eq!(3, sum(2));} closure 会自动 borrow 捕获的值（默认创建一个指向值的引用，又有 2 种可能：「可变引用」和「不可变引用」） 只要 closure 自己会在该引用指向的值的之前被 dropped，那么就是符合生命周期规则，没有问题 一个简单的例子：fn main() { let s = &quot;hello&quot;; // f 是一个 closure，它捕获 s，并自动进行 borrow let f = || { println!(&quot;{}&quot;, s); }; // f 这个 closure 在 s 之前被 dropped，所以这段代码是 OK 的 f();} 但如果不能保证 closure 自己会在该引用指向的值的前面被 dropped，不能通过编译 此时可以考虑使用 move 关键字，使用 move 关键字后，closure 会把捕获的值 move 到 closure 内部，并拿到这个值的 ownership 然后当这个 closure 被 dropped 的时候，这个参数的值也会被 dropped 一个简单的例子：fn main() { let s = &quot;hello&quot;; // f 是一个 closure，它捕获到 s，并进行 move，并拿到 s 的 ownership let f = move || { println!(&quot;{}&quot;, s); }; f(); // 之后，当 f 这个 closure 被 dropped，s 也会被 dropped // 如果之后还需要使用这个 s，只能在 move 之前 clone 出一份}一个进行 clone 之后再使用 closure 的简单例子：fn main() { let s = &quot;hello&quot;; let s2 = s.clone(); // move 的时候，用 s2 let f = move || { println!(&quot;{}&quot;, s2); }; f(); // 之后，当 f 这个 closure 被 dropped，s2 会被 dropped // 而 s 还可以继续被使用}用一些更复杂的例子说明更多的细节：struct City { name: String, population: i64, country: String,}fn sort_cities(cities: &amp;amp;mut Vec&amp;lt;City&amp;gt;) { cities.sort_by_key(|city| -city.population);} 按人口降序排序 有点像其他语言（Java）中的匿名函数 但 Rust 的 closure 和其他语言的匿名函数又有区别下面的例子，说明了 Closure 相关的生命周期：/// Sort by any of several different statistics.fn sort_by_statistic(cities: &amp;amp;mut Vec&amp;lt;City&amp;gt;, stat: Statistic) { // Rust 自动 borrow（自动有一个到 stat 的引用） cities.sort_by_key(|city| -city.get_statistic(stat));}线程相关的复杂例子：use std::thread;fn start_sorting_thread(mut cities: Vec&amp;lt;City&amp;gt;, stat: Statistic) -&amp;gt; thread::JoinHandle&amp;lt;Vec&amp;lt;City&amp;gt;&amp;gt;{ // 定义一个 Closure let key_fn = |city: &amp;amp;City| -&amp;gt; i64 { -city.get_statistic(stat) }; // spawn 新的线程 thread::spawn(|| { cities.sort_by_key(key_fn); cities })}Rust 会提示这段代码编译错误。错误的根源如下： 这个新的线程的工作完成之前，stat 和 cities 这 2 个变量都必须存活；但上面的代码按 borrow 的话，没有这个保证解决方法是利用 move 关键字告诉 Rust 把 stat 和 cities 这 2 个变量 move 到 closure（而不是像前个例子那样走 borrow 逻辑）。代码如下：fn start_sorting_thread(mut cities: Vec&amp;lt;City&amp;gt;, stat: Statistic) -&amp;gt; thread::JoinHandle&amp;lt;Vec&amp;lt;City&amp;gt;&amp;gt;{ // 利用 move 关键字，key_fn closure 获取到 stat 的 ownership let key_fn = move |city: &amp;amp;City| -&amp;gt; i64 { -city.get_statistic(stat) }; // 利用 move 关键字，该 closure 获取到 key_fn 和 cities 的 ownership thread::spawn(move || { cities.sort_by_key(key_fn); cities })} 【回顾一下】：borrow 和 move 的本质区别在于 ownership 是否改变再特别说明一下把函数作为另外一个函数的入参，和把 closure 作为另外一个函数的入参的区别。先看一个把函数作为另外一个函数入参的例子：/// Given a list of cities and a test function,/// 这个 test_fn 就是一个作为入参的函数，其入参是 &amp;amp;City，返回值是 bool：fn(&amp;amp;City) -&amp;gt; bool/// return how many cities pass the test.fn count_selected_cities(cities: &amp;amp;Vec&amp;lt;City&amp;gt;, test_fn: fn(&amp;amp;City) -&amp;gt; bool) -&amp;gt; usize{ let mut count = 0; for city in cities { if test_fn(city) { count += 1; } } count}但是不能把一个 closure 传给这个例子函数 count_selected_cities。因为 count_selected_cities 的入参是函数而不是 closure。如果需要把 closure 作为入参，需要这样定义 count_selected_cities：/// 这里的入参 test_fn 是一个 F，其实现了一个 trait：Fn(&amp;amp;City) -&amp;gt; boolfn count_selected_cities(cities: &amp;amp;Vec&amp;lt;City&amp;gt;, test_fn: F) -&amp;gt; usize where F: Fn(&amp;amp;City) -&amp;gt; bool{ let mut count = 0; for city in cities { if test_fn(city) { count += 1; } } count}也就是说： fn(&amp;amp;City) -&amp;gt; bool // fn (只能是把函数作为入参) Fn(&amp;amp;City) -&amp;gt; bool // Fn trait (既可以是函数作为入参，也可以是 closure 作为入参)上面的 Fn 是 Rust 语言支持的 trait。实际上，Rust 一共支持 3 种类型的 closure trait，开发者可以根据具体情况选择使用哪一种： FnOnce：当 closure 只能被执行一次时使用 FnMut：当 closure 会对值（通过 borrow 或者 move 拿到的值）进行修改时使用 Fn：当 closure 不会对值（通过 borrow 或者 move 拿到的值）进行修改时使用一个 Fn Mut 的例子：#![allow(unused)]fn main() { let mut i = 0;// 这个 closure（incr）就是一个 FnMut：borrow 了 外部值 i 的可变引用// 所以要在 incr 前面加一个 mut let mut incr = || { i += 1; // incr borrows a mut reference to i println!(&quot;Ding! i is now: {}&quot;, i); };}总结： Fn closure：没有限制，可以被调用多次。而且所有普通的函数类型：fn 也都是 Fn FnMut closure：可以被调用多次，但使用的时候，需要用 mut 标明该 closure FnOnce closure：只能被调用一次 这 3 种类型的 closure 使用限制一个比一个严格，它们的从属关系是：Fn 是 FnMut 的子 trait；FnMut 是 FnOnce 的子 trait。Iterator概念先明确几个概念： 所谓 Iterator 就是实现了 Iterator trait 的「类型」。 「某个类型可迭代」（iterable）就是指该类型实现了 IntoIterator，也就是对这个「类型」可以通过 into_iter() 方法获取到「该类型对应的 iterator」。从而可以通过 Iterator 来做遍历操作 Iterator 生产（produces）「值」 获取到 Iterator 生产（produces）的「值」的代码就是「消费者」（consumer） for 循环会自动调用「可迭代类型」的 into_iter 获取 iterator；不过也可以直接把一个 iterator 给 for 循环语句：iterator 本身也可以被调用 into_iter 方法（返回 iterator 自己）参考下面 for 循环的例子：#![allow(unused)]fn main() { let v = vec![6, 7, 8, 9]; // 实际上 for 并不是作用在 v 上的，实际上是作用在一个 iterator 上的 for num in v { println!(&quot;{}&quot;, num); }}实际上 for 并不是作用在 v 上的，实际上是作用在一个 iterator 上的，for 语句会自动调用 vector 的 into_iter 方法：#![allow(unused)]fn main() { let v = vec![6, 7, 8, 9]; // 显示的调用 vector 的 into_iter 方法返回一个 iterator， // 按前面说的，for 语句也可以直接接收 iterator， // 之后会 returns owned item for num in v.into_iter() { println!(&quot;{}&quot;, num); } // for 语句后 vector 不再有效（其中 item 的 ownership 已经转移）}上面这个例子中，使用了 into_iter 方法后，vector 中 item 的 ownership 已经转移，所以 for 语句后变量 v 不再有效。再看一个等价于上面 for 循环的例子：#![allow(unused)]fn main() { let v = vec![6, 7, 8, 9]; // vector 的 into_iter 方法返回一个 iterator，之后会 returns owned item // 然后使用 for_each 顺序消费这个 iterator 产出的序列中的值 v.into_iter().for_each(|num| println!(&quot;{}&quot;, num)); // vector 不再有效（其中 item 的 ownership 已经转移）}再简单介绍一下 迭代器 Trait（iterator trait）：#![allow(unused)]fn main() { pub trait Iterator { // 这里的 Item 类型是 Iterator 返回的序列中元素的类型 type Item; // Next 一次返回迭代器中的一项，封装在 Some 中，当迭代器结束时，它返回 None fn next(&amp;amp;mut self) -&amp;gt; Option&amp;lt;Self::Item&amp;gt;; // 此处省略了方法的默认实现 }} 注意：next 需要一个可变引用（&amp;amp;mut）；也就是说，每次调用 next 会消费（consume）该序列中的一项，实际上 iterator 的状态被改变（mut）了 Rust 中的很多集合类型（比如前面例子中的 vector），会默认实现另外一个 trait：IntoIterator，然后就可以调用这个 trait 的方法 into_iter 得到这个集合上的 iterator获取 iteratorRust 的标准库为了方便开发者，为各个「Collect 类型」提供了多种 Iterator。其中，为大多数 Collect 类型提供了另外 2 个方法来获取 iterator： iter() 方法：返回的 iterator 获取到序列中 item 的不变引用，对 item 只读不修改（不转移 item 的 ownership） iter_mut() 方法：返回的 iterator 获取到序列中 item 的可变引用，可以修改 item（不转移 item 的 ownership）一个在 vector 类型上使用 iter() 的例子：#![allow(unused)]fn main() { let v = vec![4, 20, 12, 8, 6]; // iterator 本身可变（mut），但处理其中的 item 时，是一个不变引用：&amp;amp;i32 let mut iterator = v.iter(); assert_eq!(iterator.next(), Some(&amp;amp;4)); assert_eq!(iterator.next(), Some(&amp;amp;20)); assert_eq!(iterator.next(), Some(&amp;amp;12)); assert_eq!(iterator.next(), Some(&amp;amp;8)); assert_eq!(iterator.next(), Some(&amp;amp;6)); assert_eq!(iterator.next(), None);} 总之，大多数场景下，iter() 方法可以遍历使用 item 的「引用」。但到具体某个「Collect 类型」是不是确实是返回 item 的「引用」可以参考文档。另外，要拿到某个类型的 Iterator，也可以直接调用 into_iter() 方法。当如先需要查询文档确认该「类型」是否实现了 IntoIterator Trait）。其中大多数「Collect 类型」都实现了几种 IntoIterator Trait，例如： &amp;amp;T：只读，不转移 ownership。在「不变引用」上调用 into_iter() 的场景 &amp;amp;mut T：读写，不转移 ownership。通常在「可变引用」上调用 into_iter() 的场景。比如常用的 (&amp;amp;mut vector).into_iter() T：转移 ownership。例如：直接通过「值」来调用 into_iter() 的场景总之，通常在不同的场景可以使用 3 种调用 into_iter() 的方式（不过具体还是要查文档）：for element in &amp;amp;collection { ... }for element in &amp;amp;mut collection { ... }for element in collection { ... }使用 iterator使用 iterator 需要 Iterator Consumer 和 Iterator Adaptor。 Iterator Consumer：可以通过 Iterator Consumer 不断调用 next() 方法，对一个 iterator 进行消费。例如： sum，max，min fold for_each collect 参考具体文档熟悉更多的 consumer。 Iterator Adaptor：把当前 iterator 变为另外类型的 iterator。例如： map filter filter_map 和 flat_map 参考具体文档熟悉更多的 Iterator Adaptor。mapmap 通过对 iterator 的 items 实施一个 closure 来对 iterator 进行转换。一个例子：// str::trim 作用到 iterator 的 items 上，转换出另外一个 iterator// 最后再调用 collect() 把 iterator 转换为 vectorlet text = &quot; ponies \\n giraffes\\niguanas \\nsquid&quot;.to_string();let v: Vec&amp;lt;&amp;amp;str&amp;gt; = text.lines() .map(str::trim) .collect();assert_eq!(v, [&quot;ponies&quot;, &quot;giraffes&quot;, &quot;iguanas&quot;, &quot;squid&quot;]);flattern对 iterator 使用 flattern 的前提是这个 iterator 中的 item 实现了 IntoIterator（也就是可以把 item 转成 iterator）这样，就可以利用 flattern 用来把一个 中的所有 items 「拉平」，从「二维」转换成「一维」。例如，Option 已经实现了 IntoIterator，那么我们可以这样使用 flattern ：assert_eq!(vec![None, Some(&quot;day&quot;), None, Some(&quot;one&quot;)] .into_iter() .flatten() .collect::&amp;lt;Vec&amp;lt;_&amp;gt;&amp;gt;(), vec![&quot;day&quot;, &quot;one&quot;]);macrosRust 中的宏分以下几种： declarative macros：使用 macro_rules! 来定义宏（据说未来会有更好的机制来替换 macro_rules!） procedural macros。翻译成「过程宏」，用代码生成代码。又分成 3 种： derive。derive 只能用于结构体和枚举 attribute（Attribute-like）。可以用于任何地方。 function（function-like） macro rules给个例子（利用宏 vec! 来初始化 vector）：#![allow(unused)]fn main() { let v: Vec&amp;lt;u32&amp;gt; = vec![1, 2, 3];}vec! 定义如下：#![allow(unused)]fn main() { #[macro_export] // vec 是宏的名字 macro_rules! vec { ( $( $x:expr ),* ) =&amp;gt; { { let mut temp_vec = Vec::new(); $( temp_vec.push($x); )* temp_vec } }; }} 需要用 #[macro_export] 属性来标识该宏可用 ( $( $x:expr ),* ) 用于模式匹配。例如，可以匹配上面例子中的：[1, 2, 3] 会被匹配出 3 个值：1、2、3，对应到宏定义中的 $x =&amp;gt; 类似 match 语句，当模式匹配成功以后，就执行 =&amp;gt; 后面的代码最终，会生成类似以下的代码：fn main() { let mut temp_vec = Vec::new(); temp_vec.push(1); temp_vec.push(2); temp_vec.push(3); temp_vec}unsafeRust 里面可以通过 2 种方法来标明代码是 unsafe 代码：unsafe block 和 unsafe function。Unsafe Block把 unsafe 关键字放在 block 代码块之前，就标明了一块 unsafe block：fn main() { unsafe { String::from_utf8_unchecked(ascii) }}如果这个代码块前面没有 unsafe 关键字，Rust 编译的时候会提示失败：from_utf8_unchecked 方法只能在 unsafe 下被使用。unsafe block 解锁了 5 个限制： 可以在 unsafe block 内使用 unsafe 函数 可以 defer raw 指针 可以存取 union 中的字段 可以存取「可变 static 变量」（但这样需要由你自己来保证该变量的线程安全） 可以存取由 Rust「外部函数接口」（FFI）定义的函数和变量一旦使用了 unsafe block 也就是提醒开发人员注意，这部分代码的安全性不再由 Rust 来保证了： 避免随意用了 unsafe block，但又完全不了解 block 中的风险所在 在做 code review 时，需要把 unsafe block 特别标注出来，特别对待 在打算使用一个 unsafe block 之前，先自己确认一下是否是真的需要使用 unsafe。例如，如果是出于更好性能考虑，先调研一下，是否真的能提升性能，或者是否存在 Rust safe 的方案来同样达到提升性能的目的给一个 unsafe block 的例子：mod my_ascii { /// An ASCII-encoded string. #[derive(Debug, Eq, PartialEq)] pub struct Ascii( // This must hold only well-formed ASCII text: // bytes from `0` to `0x7f`. Vec&amp;lt;u8&amp;gt; ); impl Ascii { /// Create an `Ascii` from the ASCII text in `bytes`. Return a /// `NotAsciiError` error if `bytes` contains any non-ASCII /// characters. /// 这里保证了数据的正确，一旦数据不是 ASCII 格式，抛 Err pub fn from_bytes(bytes: Vec&amp;lt;u8&amp;gt;) -&amp;gt; Result&amp;lt;Ascii, NotAsciiError&amp;gt; { if bytes.iter().any(|&amp;amp;byte| !byte.is_ascii()) { return Err(NotAsciiError(bytes)); } Ok(Ascii(bytes)) } } // When conversion fails, we give back the vector we couldn&#39;t convert. // This should implement `std::error::Error`; omitted for brevity. #[derive(Debug, Eq, PartialEq)] pub struct NotAsciiError(pub Vec&amp;lt;u8&amp;gt;); // Safe, efficient conversion, implemented using unsafe code. impl From&amp;lt;Ascii&amp;gt; for String { fn from(ascii: Ascii) -&amp;gt; String { // If this module has no bugs, this is safe, because // well-formed ASCII text is also well-formed UTF-8. // 这里由程序员保证 unsafe block 的安全：这里 使用 unsafe 函数 from_utf8_unchecked 是安全的 // 因为如果是 ASCII，也就是 UTF-8 unsafe { String::from_utf8_unchecked(ascii.0) } } }}Ascii 类型可以被使用在 safe 的环境，例如：fn main() { use my_ascii::Ascii; let bytes: Vec&amp;lt;u8&amp;gt; = b&quot;ASCII and ye shall receive&quot;.to_vec(); // This call entails no allocation or text copies, just a scan. let ascii: Ascii = Ascii::from_bytes(bytes).unwrap(); // We know these chosen bytes are ok. // This call is zero-cost: no allocation, copies, or scans. let string = String::from(ascii); assert_eq!(string, &quot;ASCII and ye shall receive&quot;);}Unsafe FunctionUnsafe Function： 函数定义前面加一个 unsafe 关键字，就定义一个 unsafe function。unsafe function 的函数体就是一个 unsafe block 只能在 unsafe block 里面调用 unsafe function 一旦定义了一个 unsafe function，就是告诉它的调用者必须按照约定来调用该函数，否则就可能出现 UB给个例子：// This must be placed inside the `my_ascii` module.impl Ascii { /// Construct an `Ascii` value from `bytes`, without checking /// whether `bytes` actually contains well-formed ASCII. /// /// This constructor is infallible, and returns an `Ascii` directly, /// rather than a `Result&amp;lt;Ascii, NotAsciiError&amp;gt;` as the `from_bytes` /// constructor does. /// /// # Safety /// /// The caller must ensure that `bytes` contains only ASCII /// characters: bytes no greater than 0x7f. Otherwise, the effect is /// undefined. /// 也就是说，必须由调用者来保证安全性。 /// 由调用者来保证这个 Safety 声明：bytes 参数必须是合法的 ASCII 字符串 pub unsafe fn from_bytes_unchecked(bytes: Vec&amp;lt;u8&amp;gt;) -&amp;gt; Ascii { Ascii(bytes) }}2 个关键事实： 在进入 unsafe block 之前的代码如果存在 bug，可能就会破坏协议；也就是说，协议被破坏，可能的原因不单是 unsafe block 本身里面有问题，unsafe block 之前的代码存在问题也会破坏协议 如果出现了 UB，UB 引起的问题不一定在 unsafe block 中爆出来，而是有可能在 unsafe block 之后才爆出来 一旦使用了 unsafe block，就是告诉 Rust：信任我，我保证所有都 OK。 但这种「保证」依赖所有会影响 unsafe block 的因素是否 OK。 同时，一旦出现 UB，UB 引发的问题可能出现在任何被这个 unsafe block 影响的地方。使用 unsafe block，还是使用 unsafe function？如果能保证只要正常使用就不会破坏 Rust 的安全性，就不需要指明函数是 unsafe 的。否则，才需要使用 unsafe function，并提供安全协议给其调用者。Unsafe TraitUnsafe Trait：如果一个 trait，需要在实现时遵循一个「契约」，同时这个「契约」需要开发者来保证，而 Rust 不能 check 这个「契约」；那么这个 trait 就是 Unsafe Trait： 在实现 Unsafe Trait 时，必须把实现标为 unsafe 的 如果把一个函数的类型变量绑定到一个 Unsafe Trait 上，那么这个函数本身也肯定是 unsafe。而这个 unsafe 函数的「契约」肯定就会和 Unsafe Trait 的「契约」相关联 一个例子：std::marker::Send 就是一个 Unsafe Trait，而且 std::marker::Send 不会有任何方法，这个 trait 只是用来标记实现了这个 trait 的类型必须满足能被安全移动到其他线程的「契约」Raw PointerRust 中有 2 类 Raw Pointers： *mut T：允许修改该指针指向的数据 *const T：只能读取该指针指向的数据，不能通过这个指针修改其指向的数据通过对「引用」进行转换来创建 Raw Pointer，然后用 * 操作符来对 Raw Pointer 做「解引用」。例如：fn main() { let mut x = 10; // 把 &amp;amp;mut 转成 *mut let ptr_x = &amp;amp;mut x as *mut i32; let y = Box::new(20); // 先把 y 解成 *y，然后把 &amp;amp;*y 转换成 *const let ptr_y = &amp;amp;*y as *const i32; // 解 Raw 指针操作必须 unsafe unsafe { *ptr_x += *ptr_y; } assert_eq!(x, 30);}注意：只有对「Raw 指针」进行 dereference（解引用）是 unsafe，其他操作（比如创建 Raw Pointer）是安全的。不能再把「Raw 指针」转回成「引用」，只能 borrow 解「Raw 指针」的结果。下面看一个例子：/// 如果某个类型的值是 2-byte 对齐的，可以使用一个技巧：/// 把一个 bool 值（true or false）也存在这个单个值里面。mod ref_with_flag { use std::marker::PhantomData; use std::mem::align_of; /// A `&amp;amp;T` and a `bool`, wrapped up in a single word. /// The type `T` must require at least two-byte alignment. pub struct RefWithFlag&amp;lt;&#39;a, T&amp;gt; { ptr_and_bit: usize, behaves_like: PhantomData&amp;lt;&amp;amp;&#39;a T&amp;gt; // behaves_like 字段不占用内存空间 } impl&amp;lt;&#39;a, T: &#39;a&amp;gt; RefWithFlag&amp;lt;&#39;a, T&amp;gt; { pub fn new(ptr: &amp;amp;&#39;a T, flag: bool) -&amp;gt; RefWithFlag&amp;lt;T&amp;gt; { // 如果这个类型不是 2-byte 对齐的，直接失败 // 如果 %2 为 0 的话，实际上就可以用最后一个 bit 位来存放 bool 是 true 还是 false assert!(align_of::&amp;lt;T&amp;gt;() % 2 == 0); RefWithFlag { // ptr 是类型的值的「不可变引用」，borrow 进来的 // 把 ptr 转成指向该类型的「不变」raw pointer，再强转成 usize // 把 bool 值也强转成 usize // 然后把这 2 个 usize 的值做「或」操作 // 这样就实现了用 raw pointer 的最后一个 bit 位来存放 bool 的值： ptr_and_bit: ptr as *const T as usize | flag as usize, behaves_like: PhantomData } } pub fn get_ref(&amp;amp;self) -&amp;gt; &amp;amp;&#39;a T { unsafe { // 恢复原先的指针值（恢复最后一个 bit 位） let ptr = (self.ptr_and_bit &amp;amp; !1) as *const T; // 解引用，恢复原先的类型的值 &amp;amp;*ptr } } pub fn get_flag(&amp;amp;self) -&amp;gt; bool { // 通过最后一个 bit 位的值来获取 bool 是 true 还是 false self.ptr_and_bit &amp;amp; 1 != 0 } }}// 使用 RefWithFlagfn main() { use ref_with_flag::RefWithFlag; let vec = vec![10, 20, 30]; let flagged = RefWithFlag::new(&amp;amp;vec, true); assert_eq!(flagged.get_ref()[1], 20); assert_eq!(flagged.get_flag(), true);}上面这个例子的一些细节： 上面这个技巧首先基于一个事实：raw pointer 和 usize 是可以安全的来回转换的 在 get_ref 中，先用 * 操作符对 raw pointer 进行 deference；然后再用一个 &amp;amp; 对这个 deference 的结果进行 borrow Rust 中，对一个指针的 referent 进行 borrow，可以得到一个不受「生命周期」规则约束的「引用」。但给 Rust 标记出精确的「生命周期」能避免引起错误，所以 get_ref 的返回值，标记了其返回值的「生命周期」和 RefWithFlag 的「生命周期」相同：’a A pointer’s referent object is the object that it is intended to reference PhantomData 作为「影子数据」不占用内存空间；但这个例子中的 PhantomData 字段不可或缺，否则编译失败 struct 中存在引用字段的话，struct 本身不能存活超过这些引用字段所引用的值的「生命周期」，否则就会产生悬挂指针 所以需要以某种方式标记出 struct 本身的「生命周期」必须和其引用字段的「生命周期」相匹配 在这个例子中，既然 RefWithFlag 的生命周期不能比 ptr_and_bit 字段的生命周期要长，我们需要对 RefWithFlag 的生命周期进行标记。但又不能对 usize 字段做标记，只能再加一个 PhantomData 引用字段，并把这个字段的生命周期和 RefWithFlag 的生命周期都标记为：’a struct 的生命周期请参考：结构体的生命周期注解Nullable Pointers2 种空指针。分别由 2 个函数返回： std::ptr::null&amp;lt;T&amp;gt;：返回 *const T std::ptr::null_mut&amp;lt;T&amp;gt;：返回 *mut T检查一个 raw pointer 是否是空指针的方法有： is_null 方法 as_ref 方法或 as_mut 方法。分别用于检查 *const T 和 *mut TType Sizes 和 Alignments使用 unsafe 时也会用到「类型大小」和「类型对齐」相关的知识。对于 sized 类型，可以使用下面的方法获取其 size 和 align： 使用 std::mem::size_of::&amp;lt;T&amp;gt;() 获取类型的 size 使用 std::mem::align_of::&amp;lt;T&amp;gt;() 获取类型的对齐数类型的 size 一定是该类型的 align 数的整倍数。而对于 unsize 的类型，它的 size 大小和 align 数不固定。给几个例子：// Fat pointers to slices carry their referent&#39;s length.let slice: &amp;amp; [i32] = &amp;amp; [1, 3, 9, 27, 81];assert_eq!(std::mem::size_of_val(slice), 20);let text: &amp;amp; str = &quot;alligator&quot;;assert_eq!(std::mem::size_of_val(text), 9);use std::fmt::Display;let unremarkable: &amp;amp; dyn Display = &amp;amp; 193_u8;let remarkable: &amp;amp; dyn Display = &amp;amp; 0.0072973525664;// These return the size/alignment of the value the// trait object points to, not those of the trait object// itself. This information comes from the vtable the// trait object refers to.assert_eq!(std::mem::size_of_val(unremarkable), 1);assert_eq!(std::mem::align_of_val(remarkable), 8);Pointer Arithmetic原则：在数组中，每个元素占用的 size 是 n，那么第 i 个元素的起始位置是第 i * n bytes。raw 指针上有 2 个方法可以使用： ptr.offset(i)：获得 ptr 的偏移量为 i 的指针。这个方法可能引起 UB ptr.wrapping_offset(i)：和 ptr 类似，但 Rust makes no assumptions about the relative ordering of ptr.wrapping_offset(i) and ptr itself" }, { "title": "URL Encoding", "url": "/posts/URL-Encoding/", "categories": "notes", "tags": "字符编码", "date": "2015-03-31 00:00:00 +0800", "snippet": "一, 要解决的问题给一段 Url 参数中编码过（可能用的是 GBK 编码，也可能用的是 UTF-8 编码）的中文串，在不知道是哪种编码时（可能是 GBK，也可能是 UTF-8），要都能成功解出中文例如对于%C1%B9%D0%AC%C5%AE和%E5%87%89%E9%9E%8B%E5%A5%B3都要能解出中文：凉鞋女本文的组织方式： URL 的语法和编码 字符集及字符编码 问题解决方案二, URL 的语法和编码在讲 URL 的 ecoding 之前，需要先了解 URL 的语法。1. URL 组成URL 一般由几个部分组成。例如：https://bob:bobby@www.lunatech.com:8080/file;p=1?q=2#third Part Data Scheme https User bob Password bobby Host address www.lunatech.com Port 8080 Path /file Path parameters p=1 Query parameters q=2 Fragment third 2. URL 的保留字从 URL 的组成也可以看出, 有一些字符是 URL 的保留字。但有需要特别注意：有些字符对 URL 所有的部分都是保留字；但有些字符只是对 URL 的某个部分是保留字。 ”?” 对 Query 部分不需要转义 ”/” 对 Query 部分不需要转义 ”=” 对 Path 部分不需要转义; 对 Query 的 值的部分不需要转义 对 Path 部分不需要转义：:@-._~!$&amp;amp;&#39;()*+,;= 对 Fragment 部分不需要转义：/?:@-._~!$&amp;amp;&#39;()*+,;=3. URL 保留字编码既然 URL 语法中存在保留字，那么当需要真正使用到这些保留字的时候，就需要对这些保留字进行编码（也就是转义），例如：英文问号（?）转为 %3F （％百分号开头，随后是16进制的数字）。但就像前面提到的，URL 各个部分的保留字不同；所以，各个部分的 encoding 规则也不同。例如： 对 PATH 部分，空格被编码为：％20；而英文加号（+）不需要编码 对 Query 部分，空格可以被编码为：英文加号（+）或者 %20；而英文加号（+）必须被编码为：%2BQuery 部分的例子：blue+light blue应该被编码为：blue%2Blight+blue结论：对 URL 转义的时候，不能对整个 URL 进行处理；而是需要分别对 URL 各个部分做处理。这里我们只关注请求参数的编码请求参数中： 空格可以被编码为：英文加号（+） 空格也可以被编码为：%20 而英文加号（+）必须被编码为：%2B4. 非 ASCII 字符的 URL 编码根据 URL 规范，将非 ASCII 字符按照某种 编码格式 编码成 16 进制数字然后将每个 16 进制表示的字节前加上“%”这里所说的 编码格式 对我们来说就是两种编码 GBK，UTF-8 中的一种。5. 例子凉鞋女： GBK编码：C1B9 D0AC C5AE （每个字2个Bytes，一共6个Bytes） UTF编码： E58789 E99E8B E5A5B3 （每个字3个Bytes，一共9个Bytes）这里可以使用：编码工具。所以其 URL 编码分别为（每Byte前加一个百分号）： %C1%B9%D0%AC%C5%AE %E5%87%89%E9%9E%8B%E5%A5%B3三, 字符集及字符集编码1. 字符集 ASCII： 7bits 表示一个字符，共128字符 ASCII的增强： 8bits表示一个字符，共256字符。例如： ISO­8859­1 （西欧字符） 2Bytes表示一个字符，GBK。（其特点后面详细讲） Unicode字符集：对全世界的每个字符，规定一个唯一的数字（其范围目前是U+0000~U+10FFFF，大概100万，32bits可以搞定 ）来代表。例如：\\u5973 女 其他很多扩展字符集。。。2. 字符集编码字符集编码：计算机里面怎么存储、传输Unicode字符。例如：UTF-8，UTF-16，UTF-32 。。。3. GBK既是一种字符集又是一种编码。 GB2312（双字节）：小于127的字符兼容ASCII，但两个大于127的字符连在一起时，就表示一个汉字，前面的一个字节（高字节）从0xA1到 0xF7，后面一个字节（低字节）从0xA1到0xFE。一共 7000 多个字符 （其中6千多个简体字） GBK （双字节）：微软对GB2312 进行扩展，加入繁体字，日语及朝鲜语汉字等。2万多个汉字 GB18030 （四字节）：是中华人民共和国现时最新的内码字集，与GB2312完全兼容，与GBK基本兼容，支持GB 13000及Unicode的全部统一汉字4. Unicode编码Unicode 字符集目前的范围是 U+0000~U+10FFFF。对 Unicode 字符集有多种编码。 UTF-32：直接用4字节存储 （有大小端问题） UTF-16：最早的时候，Unicode的范围没有这么大，可以直接用2字节搞定；现在2字节不够，UTF-16的编码做了更新，2字节或者4字节。 （同样有大小端问题） UTF-8：变长，最多4字节。没有大小端问题。可以检测到每个字符的边界。5. Java 中的字符集编码String使用 UTF-16（2字节或者4字节）：String chinese=&quot;ab中文”String b= new String(bs,“GBK&quot;); // bs is a byte arrayRefer: 1 2四, 问题解决方案 %E5%87%89%E9%9E%8B%E5%A5%B3 =&amp;gt; 一个 java byte 数组 Java byte 数组 =&amp;gt; Java 字符串类型, 分别用 UTF-8 和 GBK 解码String a= new String(bs,“UTF-8&quot;);String b= new String(bs,“GBK&quot;); 利用 nio 对 a 和 b 进行检测，判断是否是中文（没有乱码）： java.nio.charset.Charset.forName(“GBK”).newEncoder().canEncode(a)); 如果都返回 true（看起来都不是乱码）。用正则判断是否是 UTF-8，否则就是 GBK 例外 case（正则判断是 UTF-8，但实际上还是 GBK）： “鏈條”, “瑷媄”, “妤媞”, “浜叉鼎” 五, References What every web developer must know about URL encoding" }, { "title": "On Designing and Deploying Internet-Scale Services", "url": "/posts/On-Designing-and-Deploying-Internet-Scale-Services/", "categories": "notes", "tags": "分布式", "date": "2014-07-23 00:00:00 +0800", "snippet": "“On Designing and Deploying Internet-Scale Services”– James Hamilton – Windows Live Services Platform目的：本文是设计和部署易于运维的服务最佳实践的概括和总结.先提出3个原则: Expect failures; 所依赖的任何服务都可能发生故障, 需要优雅的处理所有的故障. Keep things simple; 复杂性会引入问题; 任何一个 server 出故障不应该影响到 data center 的其他 servers; Automate everything; 引入自动化以后, 系统更可测试, 更可修复, 更可依赖;这 3 个原则会贯穿整篇文章接下来分 10 个方面讨论怎样设计和部署易于运维的服务.1. Overall Application Design80% 以上的运维事故的根源在于设计和开发. 所以 Overall Application Design 是最重要, 因为要从根源上解决这些问题的手段通常就是在设计上和开发上做改进.而且开发团队, 测试团队, 运维团队需要尽可能的一起工作, 这样能减少运维的成本.下面列举为了实现一个易于运维的系统, Overall Application Design 时最最重要的 5 大原则:1) Design for failure 比如说量上去以后, 每天都会有硬盘会坏; 故障恢复路径必须是非常简单的路径, 而且故障恢复功能必须易于测试. 测试故障最好的方法就是不要按正常流程来 shutdown 服务, 而是要 hard-fail it;2) Redundancy and fault recovery 要到达 5 个 9, redundancy 是必须的; 靠单台服务要达到 4 个 9 也是非常困难的 (尽管这个观点已经是业界共识, 但是还是经常会看到很多服务构建在非 redundant 的数据层上) 为了设计一个能达到 SLA 的系统必须非常仔细; 为了完全满足这个设计原则的 acid test 指的是: 运维团队有意愿, 有能力在任意时刻 shutdown 整个服务中的任意服务器, 同时不会降低整个系统的 work load 这里推荐一种设计手段来发现和纠正系统潜在的安全问题: security threat modeling (仔细考虑每种可能的 security threat, 针对每种 security threat, 实现合适的应对) 把所有想到的故障记录成文档; 确保每种故障时, 整个服务可以继续工作, 同时保证服务质量损失在可接受范围内; 判断一些组合故障是 “unusual” 时, 必须很谨慎; 因为当规模上去以后, Rare combinations 故障 can become commonplace.3) Commodity hardware slice 用大规模的 commodity servers 来替换 少量的大型服务器会便宜很多 server performance 一直会比 I/O 性能增长快; 对于同样数量的存储空间 (disks), 用小 server, 整个系统会更加平衡 电力消耗是和 server 数量成线性增长的; 但是和 时钟频率 成 cubically 级增长; 所以用高 performance 的 server 电费会更贵; 小 server 挂了以后影响的范围也小4) Single-version software 如果一个服务的软件只需要以 single internal deployment 为目标, 同时软件 (面向企业的产品) 的旧版本不需要支持 10 年; 部署和升级的代价会非常小; 要做到Single-version software 需要: 1) 每次 release 时保证用户的体验不会变化 2) a willingness to allow customers that need this level of control to either host internally or switch to an application service provider willing to provide this people-intensive multi-version support备注: 这里提到的 people-intensive 型的服务指的是: Those businesses tend to be more people intensive and more willing to run complex, customer specific configurations.4) Multi-tenancy Multi-tenancy 优先的理念和 Single-version software 优先是一脉相承的 对自动化和大规模化有更好的支持; 节约服务成本除了以上的 5 大原则, 还有其他一些设计易运维服务时的 best practices: Quick service health check. 可以在开发环境快速跑的测试, 确保服务不会被破坏; 这种测试可以不包含各种边界测试, 但是要求 quick; 跑通以后就可以 check in 代码; Develop in the full environment. 整套服务能在单台 server 上搭起来, 这样开发人员可以容易的单元测试他负责的 components, 同时也可以测试这些 components 上的改动对整个 service 的影响 Zero trust of underlying components. 假设所有依赖的服务都可能出故障, 同时要确认这些依赖都可以恢复重新提供服务. 一些通用的恢复技术: 继续在只读模式下访问 cached 的数据 除了少部分用户外的绝大多数用户能够继续使用服务 (利用 redundant copy) Do not build the same functionality in multiple components; 前期的时候确实可能会一份代码到处 copy; 但如果长期不关注这个, 代码会快速恶化 (随着服务的快速增长, 进化) One pod or cluster should not affect another pod or cluster; 虽然很难做到, 但是还是要尽可能的让一个 cluster 所有的依赖都在这个 cluster 内; Allow (rare) emergency human intervention; 目标是所有场景都不需要人工操作; 但还是有极少的场景 (某些突发事件导致) 会需要人工处理 (这种场景下人为的错误往往是灾难的根源); 这些场景的处理办法不要用文档的形式来准备, 而是事先准备好脚本 需要定期搞 “实弹演习”, 验证这些脚本是可以 work 的 If the service-availability risk of a drill is excessively high, then insufficient investment has been made in the design, development, and testing of the tools Keep things simple and robust 采用简单, 傻瓜的解决方法在 high-scale service 中比采用复杂的算法好 其原则是, 如果复杂的方法能带来的 magnitude 级别的改进才值得上; 如果只是 percentage or even small factor 的改进就算了; Enforce admission control at all levels 好点的系统都会在最前端有一个流量进入控制; 但也需要在所有重要的组件上有 流量控制机制 (有可能总的流量没有过载, 但是由于某种原因, 某个组件上的流量过载了, 如果没有相应的控制, 最终会导致整个服务受到影响) 通用准则是: 尝试优雅的 degrading, 而不是 hard failing (block entry to the service before giving uniform poor service to all users); 后面会详细展开讨论这方面的内容 Partition the service; Partition 的粒度必须把握好; 很多时候不能简单的按照真实世界的界限来做 Partition (按用户, 公司…); 合理粒度划分出来的 Partition 可以在各个 servers 间自由迁移; Understand the network design; 开发人员必须 understand 网络设计; 必须尽早的和运维团队 review networking specialists; 必须尽早的 understand 同机架, 不同机架, 不同 data center 间的带宽; Analyze throughput and latency; 必须做 UI 相关的核心服务的 throughput 和 latency 的分析工作; 了解其影响; 特别是在一些日常运维操作 (数据库维护, 配置更新, 服务 debugging…) 发生的同时也对 throughput 和 latency 进行分析; 这样能帮忙定位这些操作带来的问题; 对每个服务需要度量容量规划指标. 例如: 每秒用户请求数; 并行在线用户数, 以及其他一些通过 work load 来反映资源需求量的指标 Treat operations utilities as part of the service; 所有的 Operations utilities 必须 code review 过, 提交到代码库中, 被测试过; (实际上 Operations utilities 经常被当作低优先级的事情来处理, 也没有被测试过) Understand access patterns: 对每个新 feature 都要考虑到它对后端 load 的影响; 上线之前必须量化和验证这种影响; Version everything: 前面提了, 目标是 single version software; 但实际上多版本是不可避免的 (联调测试环境…) 所有组件的版本 n 和 版本 n+1 需要能共存; Keep the unit/functional tests from the last release: 不仅仅用单元测试和功能测试来验证前一个版本的功能没有出问题; 在生产环境也跑单元测试来对服务进行验证 (这个后面会详细讲) Avoid single points of failure: 无状态的实现优先; 不要把某个 client 的请求限制在某个 server 上; 基于静态hash之类的负载均衡可能也会有问题; 用好的 Partition 策略 (前面提过了) 尽量少做跨 Partition 的操作 All database state is stored redundantly (on at least one) fully redundant hot standby server and failover is tested frequently in production 2. Automatic Management and Provisioning 许多 service 使用: 故障检测-&amp;gt;报警-&amp;gt;人工恢复 的模式; 这种模式的问题在于在压力下, 20%的时候可能会出现人为的错误 结果就是运维成本高, 而且降低了整个服务的可靠性但是 Designing for automation 也会给服务 model 带来很大的限制 (例如, 数据库主从切换时可能丢数据, 要保证一致性 automation 的方式可能处理不好);所以为了 automation, 服务质量上可能付出一定的代价;下面列出一些 automation 设计的 best practices: Be restartable and redundant: 服务可重启; 存储有冗余; Support geo-distribution: 好像我们这里提到的 automation 都可以没有 geo-distribution; 但实际上, 如果一个 high-scale 的服务不支持 geo-distribution 的话, 由于运维上的限制, 成本会高很多 Automatic provisioning and installation: 加快部署速度, 降低部署成本 Configuration and code as a unit: 需要确保 1) 开发团队把代码和配置作为一个整体来发布 2) 在测试中采用和部署时同样的部署流程 3) 运维团队把代码和配置作为一个整体来部署 如果对线上系统做了更新, 有 log 记录所有的更新内容 (更新了什么? 什么时候更新的? 谁做的操作? 哪些服务器上做了操作?); 同时要经常 scan 所有服务器的状态是否正确 (确保配置什么的没有问题) Manage server roles or personalities rather than servers: 部署到哪些 servers 上要看部署服务的角色和身份, 并不会限定服务应该被部署到哪些具体的 servers 上; Multi-system failures are common: Expect failures of many hosts at once (power, net switch, and rollout) Recover at the service level: 在 service 层做 recovery, 而不是在底层的软件层 (比如在 service 层做冗余, 而不是在底层的软件层做) Never rely on local storage for non-recoverable information: Always replicate all the non-ephemeral service state Keep deployment simple: 不要用复杂的安装脚本; 通过 文件 copy 来部署是最理想的部署模式; 要避免: 同一个 component 的不同版本, 或者不同的 components 如果不能跑在同一个 server 上; Fail services regularly: 定期通过 take down data centers, shut down racks, and power off servers 等手段来暴露服务, 系统, 网络的弱点; Those unwilling to test in production aren’t yet confident that the service will continue operating through failures And, without production testing, recovery won’t work when called upon 3. Dependency ManagementAs a general rule, dependence on small components or services doesn’t save enough to justify the complexity of managing them.两种依赖是可以被 justified 的: 正在被依赖的 components 数量多和复杂度高 (例如: 存储服务, 一致性算法等) 正在被依赖的 service 作为一个 single, central instance (例如: 认证管理系统)假设现有的 dependencies 满足上面的那些规则, 那么 dependencies are justified.对这些 可以被 justified 的 dependencies, 下面列举一些管理它们的 best practices: Expect latency: 设置 timeout; Isolate failures: 必须防止 cascading 式的故障, 尽量 “fail fast” Use shipping and proven components; 宁愿用版本老一点, 但是稳定的软硬件; (就算新版本性能好, 功能多) Implement inter-service monitoring and alerting; 出问题的时候要能联系到人来解决 Dependent services require the same design point; 相同的 SLA; Decouple components: 其他 components 故障的时候, 要能继续工作 (可能在 degraded 模式)4. Release Cycle and Testing 对所有的 internet-scale 服务, 在线上环境进行测试是一种必备 QA 的手段 尽管很多服务都有一个模拟线上环境的测试环境 (会在这个环境上模拟线上负载来进行测试); 但实际上, 无论怎么样, 这个模拟环境不能完美的模拟线上环境 推荐的做法是在模拟环境 (尽可能的模拟线上环境) 测试之后, 可以在线上环境 (会做一些限制) 做最后阶段的测试; 但需要注意的是, 我们不希望在线上环境做测试会影响到线上服务. 所以必须非常小心, 同时遵照以下规则: 线上环境必须要充分的冗余; 出现问题可以快速切换 必须保证数据不会被破坏, 状态相关的故障不会发生 错误必须能检测到; 开发团队 (而不是运维团队) 必须对系统进行监控 必须能够快速回滚; 而且回滚动作本身之前必须测试通过 听起来比较危险, 但是实际上效果很好, 而且可以和灰度发布配合起来灰度发布实际上会降低风险 (对比一下子把更新上线)另外, 做做到了上面这些规则之后, 在白天上线比在半夜上线好: 半夜操作更容易犯错 出了问题半夜找人更困难下面列出 Release Cycle and Testing 相关的 best practices: 更频繁的发布; 可以促进发布的质量的提高, 用户体验会更好. 发布周期大于 3 个月是危险的 (有的团队现在已经做到了 Week 级别的发布周期) Use production data to find problems: 大规模系统的 QA 问题已经不仅仅是测试问题, 而是 data-mining 和 visualization 问题. 需要从线上环境采集大量的数据: 测量发布标准: 根据用户体验来定义发布规格; 并持续对这个指标进行监控 (例如, 可用性如果被设置为 99%, 就持续监控报警是否真的达到这个标准) 不要纠结需要把标准定为多少, 99%? 99.9%? 而是设置一个可接受的目标, 随后不断进行提升 收集真实的数值, 而不是用 summary reports (例如, 用红灯, 绿灯什么的) 来体现 避免过度报警; Analyze trends; 用来预估问题 Make the system health highly visible; 有一个内部运维系统, 大家都可以了解当前服务的状态; Monitor continuously; 如果每天都要看所有的监控数据, 时间久了就会失去效果; 可以搞成一个明确的任务, 大家轮流做 (每个人做一段时间) Invest in engineering; 不懂? Support version roll-back; 必须有回滚 (所有的部署都要有回滚, 就像保险带); 而且必须被测试过证明过; Stress test for load: 在线上环境的一个子集上跑 2 倍的压力 Perform capacity and performance testing prior to new releases; 在 service level 上做这个; 也在每个 component 上做这个; Build and deploy shallowly and iteratively; 在开发的早期就引入 a skeleton version of the full service; Test with real data; tcp copy 测试 Run system-level acceptance tests; 不懂? Test and develop in full environments; 用预留的硬件搭建测试环境, 尽量保持和线上的硬件配置一致, 最大化投资效果 (以后这些机器可以直接用到线上环境) 5. Hardware Selection and Standardization用 services fabric 模式来进行硬件采购和管理. 使用 services fabric 模式保证了两个关键原则: 对所有的 services (即使规模比较小) 进行自动管理和自动运维; 新的 service 能被更快速的测试和部署下面列举硬件选型相关的 best practices: Use only standard SKUs; 只用一种或少数几种 SKU; 这样可以保证线上环境中的各种 services 的资源能够容易的互相调配; 最有效的方式就是部署一套标准的 service-hosting framework, 其中包括自动管理, 自动运维, 硬件, 共享 services; Purchase full racks; 购买全配置好, 而且全测试好的 racks; Racking and stacking costs are inexplicably high in most data centers, so let the system manufacturers do it and wheel in full racks. Write to a hardware abstraction; services 不能绑定在具体的硬件 SKU 上面; 而是按照 hardware abstraction 来做 services (CPU数, 内存数, 硬盘大小…) Abstract the network and naming; using DNS and CNAMEs; 而不是用 机器名, If you need to avoid flushing the DNS cache, remember to set Time To Live sufficiently low to ensure that changes are pushed as quickly as needed6. Operations and Capacity Planning高效对 services 进行运维的关键在于: 构建一个系统来消除绝大多数的运维交互操作; 这个系统的目标就是要用 8*5 /周的人力来运维一个高可用, 7*24 的服务;开发团队预先准备好针对紧急情况的恢复动作 (可以用脚本); 而且事先要在线上环境对这些脚本进行过测试; 如果这些脚本在线上环境测试风险太大, 那么到出现紧急情况的时候, 这些脚本也不安全, 不可用;如果一个小问题没有按预先计划的那样被自动恢复, 在很多时候会引发大灾难下面列举相关的 best practices: Make the development team responsible; “谁开发的谁运维”. 这个可能太激进了, 不过方向是这个方向; 要推动开发团队改进易运维性 Soft delete only; Never delete anything; 用标记删除; 如果被误操作删除了就没有办法恢复了. RAID, 镜像什么的都不能服务这种错误; Track resource allocation; services 都会定义一些负载指标 (同时在线用户数, QPS…), 关键是必须了解这些负载指标和硬件资源消耗的关系; 运维团队把这些数据反馈给市场和销售团队; 不同的 services 要求不同的采购周期; Make one change at a time; 更新线上环境时, 每次只做一个更新; Make Everything Configurable; 所有需要在线上环境修改的东西都做成可配置的, 避免改代码; 甚至在不知道这个值是否会在线上被修改的时候, 如果方便的话, 也尽量的做成可配置的 (不能随意在线上进行更动, 改动之前需要测好).7. Auditing, Monitoring and Alerting对所有的线上操作进行记录. 一旦有问题, 可以从最近动了哪些东西来找原因;Alerting is an art. 报得太多, 就会被忽略; 可以跟踪两个指标来判断报警是否合理: alerts-to-trouble ticket ratio; 越接近 1 越好 number of systems health issues without corresponding alerts; 越接近 0 越好;下面列举一些 best practices: Instrument everything; 测量每个用户操作和事务, 报告其中的异常 (可以用一个程序模拟用户的操作, 但可能还是不够) Data is the most valuable asset; 从系统中收集数据来了解系统运行的状况 (是否真正的工作正常) Have a customer view of service; 可以用前面提到的模拟用户操作的程序来覆盖路径是否正常 (也是一种报警, 所以也需要合理报警) Instrumentation required for production testing; 在线上环境做测试, 详尽的报警和监控是必须的; Latencies are the toughest problem; 很慢但是没有失败的请求最难被发现; 需要确保能监控到这种状况; Have sufficient production data (哪些数据需要被监控?): Use performance counters for all operations: 至少要记录每秒的 latency 和 QPS; Audit all operations: 一方面可以发现问题; 另外一方面可以发现用户在做什么 (但是最好运维人员不要用公共账号来操作) Track all fault tolerance mechanisms; fault tolerance 掩盖错误, 所以需要记录每次 retry 事件, 每次副本复制; Track operations against important entities; 记录成文档, 进行数据分析, 发现数据中的异常; 如果在项目后期加这个比较困难, 要早点加进去; Asserts; 多用 Asserts 来定位问题; Keep historical data: 历史日志和历史performance都需要 Configurable logging; 可以配置 level Expose health information for monitoring; 主动暴露一些接口提供信息供监控程序使用 Make all reported errors actionable; 报警信息的内容要是 actionable 的; Enable quick diagnosis of production problems; Give enough information to diagnose; “10 queries returned no results” 不够充分, 应该补充”and here is the list, and the times they happened” Chain of evidence: 需要给开发者提供一条完成的路径来诊断问题 Debugging in production; 最好不要到线上操作, 而是把信息 (内存镜像…) 导出来 debug; 如果真的要到线上调式, 最好的操作人选是开发人员 (之前必须被训练过); 不过不到万不得以还是推荐不要到线上去; Record all significant actions; This includes both when a user sends a command and what the system internally does; 更重要的是, 开发 mining 工具发现有用的 aggregates (例如: 用户正在查的流行词汇) 8. Graceful Degradation and Admission Control2 个 best practices: ‘big red switch’ admission control针对这两点, 每个 services 都要度身定做; 这两点都非常重要: big red switch 是一种事先设计好的, 可测试的动作; 当 service 不能满足 SLA, 或者是即将不能满足 SLA 时, 可以执行这些动作; big red switch 的理念就是保证关键服务继续, 同时暂时 shedding 或 delaying 不重要的 workload queue 之类的地方就是 big red switch 的候选 关键点是在系统有问题时, 什么是保证系统运行的最低要求; 需要测试 degrade 之后, 系统能切回正常模式 admission control 具体怎么好可以根据具体的系统来设计 (只放 vip 用户进来? 不再 queue 请求等等) Meter admission; 在做了 admission control 以后, 有一个关键点是之后能够修改这个控制点; 例如, 服务正常以后, 通过调整 admission control, 慢慢的恢复系统 (用多大的粒度恢复服务或发布新的release也是很重要的, 通过和用户的沟通, 引导用户的期望) Another client-side trick that can be used to prevent them all synchronously hammering the server is to introduce intentional jitter and per-entity automatic backup?9. Customer and Press Communication Plan系统有问题时 (故障, 延迟比较大) 必须通知用户; 和用户的沟通需要有多种渠道;有 client 的软件可以做很多有益的事情…如果没有 client 而是 web page 的应用, 也可以有很多方式来和用户交流; 用户对系统状态了解更多, 用户的满意度就会更高 (系统的 owner 往往倾向于不告知用户系统的故障, 但我们的经验如果告诉用户, 用户满意度会高很多)需要在平时提前准备好沟通计划; 每类事件需要提前计划好, 到时候 who to call, when to call them, and how to handle communications10. Customer Self-Provisioning and Self-Help用户自服务可以节约成本, 提升用户的满意度; 例如用户能登录 web page, 自己输入自己需要的数据, 然后就可以开始使用服务;" }, { "title": "Bash Notes", "url": "/posts/Bash-Notes/", "categories": "notes", "tags": "bash", "date": "2014-04-12 00:00:00 +0800", "snippet": "1. 变量 变量的赋值用 等号, 但是等号间不能有空格 foo=42 # sets foo to 42 引用变量的值, 用 $ 符号echo $foo # prints 422. 数组 给数组赋值 foo[0]=&quot;one&quot; foo[1]=&quot;two&quot; echo ${foo[1]} # prints &quot;two&quot; 换种方式给数组赋值foo=(&quot;a a a&quot; &quot;b b b&quot; &quot;c c c&quot;)echo ${foo[2]} # prints &quot;c c c&quot;echo $foo # prints &quot;a a a&quot; 整个数组的赋值 (注意空格时需要用引号) foo=(&quot;a 1&quot; &quot;b 2&quot; &quot;c 3&quot;) bar=(${foo[@]}) baz=(&quot;${foo[@]}&quot;) echo ${bar[1]} # oops, print &quot;1&quot; echo ${baz[1]} # prints &quot;b 2&quot;3. 专用变量 echo $0 # 脚本本身的名字 echo $1 # 传给脚本的第一个参数 echo $2 # 第2个参数 echo $9 # 第9个参数 echo $10 # 第一个参数, 后面跟一个 0 echo ${10} # 第10个参数 echo $# # 传给脚本的参数总数 echo $? # prints 0 代表前一个进程退出时返回0, 执行成功 echo $? # 非 0; 代表前一个进程退出时返回非0, 执行失败 echo $$ # 当前 shell 的 进程id $! 最近运行的背景进程的进程 id# sort two files in parallel: sort words &amp;gt; sorted-words &amp;amp; # launch background process p1=$! sort -n numbers &amp;gt; sorted-numbers &amp;amp; # launch background process p2=$! wait $p1 wait $p2 echo Both files have been sorted. 4. 字符串处理 替换 foo=&quot;I&#39;m a cat.&quot; echo ${foo/cat/dog} # prints &quot;I&#39;m a dog.&quot; echo $foo # still prints &quot;I&#39;m a cat.&quot; 替换一次和多次 foo=&quot;I&#39;m a cat, and she&#39;s cat.&quot; echo ${foo/cat/dog} # prints &quot;I&#39;m a dog, and she&#39;s a cat.&quot; echo ${foo//cat/dog} # prints &quot;I&#39;m a dog, and she&#39;s a dog.&quot; 删除 foo=&quot;I like meatballs.&quot; echo ${foo/balls} # prints I like meat.5. 数组长度ARRAY=(abcdd b c)echo ${#ARRAY} # prints 5 错误echo ${#ARRAY[@]} # prints 3 正确6. 引号world=Earthfoo=&#39;Hello, $world!&#39;bar=&quot;Hello, $world!&quot;echo $foo # 单引号, prints Hello, $world!echo $bar # 双引号, prints Hello, Earth!Bash Idiomsif 逻辑# 先判断 DIR 变量长度是否为 0，不为 0 的话，才执行 cd 命令[[ -n &quot;$DIR&quot; ]] &amp;amp;&amp;amp; cd &quot;$DIR&quot;和下面的写法等价：if [[ -n &quot;$DIR&quot; ]]; then cd &quot;$DIR&quot;fiReferences Shell programming with bash: by example, by counter-example" }, { "title": "经典 TWO-PHASE COMMIT PROTOCOL", "url": "/posts/2PC/", "categories": "分布式", "tags": "两阶段提交协议, 2pc", "date": "2014-04-07 00:00:00 +0800", "snippet": "本文详细描述经典的 TWO-PHASE COMMIT PROTOCOL（后面简称 2PC 协议）。 注意：本文只描述了经典的 2PC。没有描述 2PC 的各种变种和优化。为了更好的理解，先描述一下无故障时协议怎么工作。然后再对协议运行过程中可能碰到的各种故障进行讨论，说明在这些故障的场景下，怎么处理这些故障来保证协议的正确性。一 经典 2PC 协议无故障时的工作流程一个 commit protocol 需要满足以下特性： 始终保证事务的原子性 能够快速「forget」提交的处理结果（所谓的 forget 就是把内存中这个事务相关的内容全部清除掉） 尽可能的减小写本地日志和进程间消息通讯的代价 优化正常流程的性能（也就是没有出现故障时的性能） 最大化「单方面 abort 的能力」（所谓单方面 abort，也就是某个进程如果知道这个分布式事务肯定会 abort，那么它无需等待其他进程的通知，直接进入 abort 流程）经典 2PC 协议模型中包含两种角色的进程： coordinator（协调者）：只有一个；接收用户开始事务的请求，然后会和 多个 subordinates 进行通讯处理事务 subordinates（参与者）：有多个；互相之间不会进行通讯，只会和 coordinator 通讯处理事务另外, 特别说明一下，2PC 协议中，写日志分两种： force-write 日志：要保证日志被刷到磁盘上，而不仅仅是写到操作系统缓存中，保证系统崩溃恢复后能从磁盘上读到这种日志 一般 write 日志：写到操作系统缓存中，系统崩溃后不一定能从磁盘上读到这种日志下面是正常情况（无故障场景）下的 2PC 协议：第一阶段（提交请求阶段）： coordinator 接收到用户的事务开始请求 coordinator 并行的向所有 subordinates 发送 PREPARE 消息，询问这些 subordinates 是否愿意「commit」 这个事务 然后，coordinator 进入等待状态，等待所有 subordinates 返回的消息 每个愿意 commit 这个事务的 subordinate，先 force-write 一条 prepare 日志，这个 subordinate 进入 prepared 状态 然后 这个 subordinate 发送 YES VOTE 消息给 coordinator；等待 coordinator 第二阶段的决定 每个想要 abort 这个事务的 subordinate，先 force-write 一条 abort 日志 然后这个 subordinate 发送 NO VOTE 消息给 coordinator 之后，无需等待 coordinator 第二阶段的决定，这个 subordinate 释放相关资源后，forget 掉这个事务**第二阶段（提交执行阶段）: 如果 coordinator 从所有 subordinates 获得的响应消息都是 YES VOTE，coordinator force-write 一条 commit 日志, 进入 committing 状态 然后 coordinator 并行发送 COMMIT 消息给所有的 subordinates；并开始等待所有 subordinates 的确认消息 如果 coordinator 从某个 subordinate 获得的响应消息是 NO VOTE，coordinator force-write 一条 abort 日志，进入 aborting 状态 然后 coordinator 并行发送 ABORT 消息给那些 返回 了 YES VOTE 的 subordinates；以及那些还没有响应 PREPARE 消息的 subordinates（既没有返回 YES，也没有返回 NO） 每个 subordinate，如果收到了 COMMIT 消息，force-write 一条 commit 日志，进入 committing 状态，然后返回一个 ACK 消息给 coordinator；最后 commit，并 forget 这个事务 每个 subordinate，如果收到了 ABORT 消息，force-write 一条 abort 日志，进入 abort 状态；然后返回一个 ACK 消息给 coordinator；最后 abort，并 forget 这个事务 coordinator 收到它期望的所有 ACK 消息后（无需等待之前返回了 NO 的 subordinates 返回 ACK），使用一般 write（不需要 force-write）写一条 end 日志，完成，并 forget 这个事务以上描述的协议有一个原则：任何时候 subordinate 发送一个响应消息给 coordinator 之前，必须先 force-write 一条日志记录。一旦遵守了这个原则 subordinate 就不需要去 coordinator 那里获取这些日志中记录的信息。这个原则保证了分布式事务的原子性。换句话说，如果不遵守这个原则，事务的原子性不能被保证，可能会出现部分 subordinates 提交了事务，而部分 subordinates abort 了事务的不一致情况（后面会详细说为什么）。总结： 整个过程中，每个 subordinate 需要 force-write 两条日志；发送两条消息给 coordinator。 整个过程中，coordinator 需要 force-write 一条日志；一般 write（非 force-write）一条日志 整个过程中，coordinator 发送两条消息给 subordinates最后给出一个示意图:二. 2PC 协议可能遇到的故障所谓的故障有两种： 机器故障 通讯故障但在 2PC 协议的模型中，假定这两种故障都是最终会恢复的： 机器故障以后，可以重启起来。而且之前 force-write 的日志不会被破坏，可以把日志中的信息再读到内存中来 某两个机器间的通讯故障一段时间后，可以恢复通讯2PC 协议中的故障统一由 recovery process 来处理。所以每台机器上会起一个 recovery process。recovery process 负责以下事情: 每台机器上的 recovery process 会响应处理其他机器上的 recovery process 发送过来的消息 每台机器故障后重启起来后，这台机器上的 recovery process 会处理之前故障时正在执行的事务机器故障后重启起来时，recovery process 先从本地日志中读取之前故障时正在执行的事务的相关信息到内存中。这些在内存中的信息的用处： 当 coordinator 接收到其他机器上的 subordinates 发送过来的 query 请求（这些 query 是做什么用的？后面会讲) 时，coordinator 会根据内存中的信息做响应 当 coordinator 主动发送通知（这些通知做什么用？后面会讲）给其他机器上的 subordinates 时，coordinator 会直接根据内存中的信息发送通知把这些信息读入到内存中，可以提高整个协议的性能（不需要到日志中获取这些信息）。下面分别介绍机器故障和通讯故障的恢复动作。三 2PC 协议有机器故障时的场景下面逐一介绍当遇到机器故障时，2PC 协议中的 recovery process 需要做的恢复动作。1. subordinate 的 recovery process 发现没有本地日志和当前执行的事务相关恢复动作：直接回滚这个事务. 然后写一条 abort 日志，最后 forget 这个事务解释： subordinate 的本地没有日志：就说明没有发送过任何消息给 coordinator。直接把这个事务 abort 不会造成任何不一致 如果之后收到 coordinator 的 PREPARE 消息，直接返回 NO VOTE（对于还没有到 prepared 状态的 subordinate。只要是被 forget，内存不存在的事务，都认为是被 abort 了的事务）2. subordinate 的 recovery process 发现当前的状态是 prepared恢复动作：recovery process 周期性的给 coordinator 发送 query，询问下一步需要做 commit 还是 abort。解释： 如果是 prepared 状态，说明这台机器是一个 subordinate，之前它已经发送了 YES VOTE 消息给 coordinator 所以现在要恢复故障，必须从 coordinator 获取事务的最终的决定 3. coordinator 的 recovery process 发现没有本地日志和当前执行的事务相关恢复动作：直接回滚这个事务。然后写一条 abort 日志, 最后 forget 这个事务解释： coordinator 的本地没有日志，就说明没有进入第二阶段。直接把这个事务 abort 不会造成任何不一致 如果之后收到 subordinate 的 query 消息，询问事务的最终决定，直接返回 ABORT（coordinator 对已经被 forget，内存不存在的事务，都认为是被 abort 了的事务）4. coordinator 的 recovery process 发现当前的状态是 committing 或者 aborting恢复动作：recovery process 周期性的主动发送 COMMIT 或 ABORT 通知给所有的 subordinates。解释： 发现 coordinator 已经在 committing 或者 aborting 状态，说明已经开始第二阶段，最终决定已经做出，不得再变更。只能通知所有 subordinates，并等待所有的 ACK 消息来结束事务最后, 回答一下之前提出的问题：Q：为什么任何时候 subordinate 发送一个响应消息给 coordinator 之前，必须先 force-write 一条日志记录？A：从上面的机器故障恢复过程可以看出，所有的恢复必须依靠 force-write 的日志。如果先发消息，再写日志，就可能出现消息发出之后，机器故障，日志没有写成功的场景，最后造成没有办法从机器故障中恢复，破坏事务的原子性。四 2PC 协议有通讯故障时的场景下面逐一介绍当遇到通讯故障时，2PC 协议中的 recovery process 需要做的恢复动作：1. coordinator 已经发送了 PREPARE 消息，等待 subordinates 的响应时，发现和某个 subordinate 的通讯故障恢复动作：coordinator 直接判断这个事务可以被 abort，开始 abort 流程（直接回滚这个事务。然后写一条 abort 日志，最后 forget 这个事务）解释： 没有进入第二阶段。直接把这个事务 abort 不会造成任何不一致 如果之后收到 subordinate 的 query 消息，询问事务的最终决定，直接返回 ABORT （coordinator 对已经被 forget，内存不存在的事务，都认为是被 abort 了的事务）2. coordinator 已经发送了 COMMIT 或 ABORT 消息给所有的 subordinates，等待 ACK 的响应时，发现和某个 subordinate 的通讯故障恢复动作：coordinator 把相关工作转交给 recovery process 来处理（反复获取 subordinates 的 ACK）解释： 已经开始第二阶段，最终决定已经做出，不得再变更。只能等待 ACK 消息来结束事务3. subordinate 在 prepared 状态之前和 coordinator 通讯故障恢复动作：直接回滚这个事务。然后写一条 abort 日志，最后 forget 这个事务解释： 没有发送过任何消息给 coordinator。直接把这个事务 abort 不会造成任何不一致 如果之后收到 coordinator 的 PREPARE 消息，直接返回 NO VOTE (对于还没有到 prepared 状态的 subordinate。只要是被 forget，内存不存在的事务，都认为是被 abort 了的事务）4. subordinate 在 prepared 状态之后和 coordinator 通讯故障恢复动作：subordinate 把相关工作转交给 recovery process 来处理（反复询问 coordinator 事务的最终结果，然后走相应的流程）解释： subordinate 已经在 prepared 状态，只能等待 coordinator 关于这个事务的最终决定来结束事务最后要强调一下： 对于还没有到 prepared 状态的 subordinate。只要是被 forget，内存不存在的事务，都认为是被 abort 了的事务 coordinator 对已经被 forget，内存不存在的事务，都认为是被 abort 了的事务五 总结没有进入「第二阶段」都好说，出现故障的话，可以直接 abort 这个事务。但一旦进入「第二阶段」，就不能反悔了，只有等待所有的 subordinate 都完成才能结束这个事务。从这里也可以看出，2PC 协议可能会有性能问题：一旦进入「第二阶段」，又出现了故障，只能等待故障恢复才能推进。五 参考 Transaction Management in the R* Distributed Database Management System" }, { "title": "Java 异常实践总结", "url": "/posts/java-exception/", "categories": "notes", "tags": "java", "date": "2014-02-08 00:00:00 +0800", "snippet": "本文对异常处理的种类进归纳, 试图总结出一种好的异常处理的实践方法为了更好的描述异常处理方法, 先把项目分成两大类, 当工作在不同的项目时, 程序员需要站在不同的位置上来采用合适的异常处理方法.一, 应用程序中的异常处理主要是指程序员是在开发一个被用户使用的程序, 例如, 一个桌面软件, 一个web应用服务器, 或者一个网络应用服务器.这类程序的工作就是要帮助用户完成一个任务, 并把结果返回给用户.而这类程序的开发人员处于一个中间地位: 调用各种支持接口, 处理业务逻辑, 最终把处理结果返回给最终用户, 在这个过程中, 会和3种异常打交道: 调用其他支持接口时, 其他接口可能会抛出 checked 异常, 例如调用 DAO接口遇到一个数据库异常 调用其他支持接口时, 其他接口抛出了 RuntimeException 处理业务逻辑过程中, 发现正常业务不可能完成, 需要给用户返回一个特殊的响应消息. 例如, 用户提交的买火车票请求中的车次是不存在的当遇到这3种情况, 只有两种处理方法: 捕获到 checked exception 后, 自己能处理掉, 就自己处理掉了, 不需要通知最终用户 (其实我想不出这种例子, 从对用户友好的角度来说, 操作失败后, 至少要给用户一个提示吧?) 其他需要给用户返回合理的提示的情况: 简单而有效的处理方法就是抛出一个带有异常信息的 RuntimeException, 这个 RuntimeException 会在程序最外层被捕获统一处理, 并根据异常信息返回相应的响应消息给客户,有两点特别说明一下: 这里抛出的 RuntimeException, 主要是指抛出 RuntimeException 的一个子类 (可以是Java提供的, 也可以是自己定义的). 这样的好处是对异常的描述性更强, 可读性好 抛出 RuntimeException 而不是抛出 checked exception 的好处是, 这种情况抛出异常的目的只是为了让程序能返回响应消息给客户, 如果抛出 checked exception, 需要一层层的抛, 一层层的捕获上去, 才能在最外层返回响应给客户, 这样就几乎回退到古老的返回值的异常处理模式, 失去利用异常类的好处二, 框架开发和库开发中的异常处理这里主要指的是程序员开发一个框架或者一个库给其他的程序员使用, 例如, 开发一个 HTTP 客户端请求的封装包, 或开发一个容器框架等工作.和开发应用程序不同, 这类工作的开发人员在处理异常时需要做更多的思考, 应用程序的开发很多时候只需要给客户一个提示就行了, 而开发给他人使用的包, 根本上来说, 是要制定一个契约, 需要考虑这个契约是否合理, 是否能被别人方便的使用.在做这类开发时, 只会在2种情况下和异常打交道: 调用其他支持接口时, 其他接口可能会抛出异常 (可能是 checked 也可能是 unchecked 的异常) 当正常流程不可能完成时, 需要提供一个异常消息给调用者, 这个时候需要给提供给使用者的接口定义一个异常消息的返回这两种情况, 其实统一起来, 就是定义接口的时候, 需要考虑, 在正常逻辑不能完成的时候, 不能提供正常的返回给调用者, 需要以什么样的形式通知调用者出现了例外情况.一般来说有几种通知方法: 返回一个特殊的值 (最普遍的, 返回一个 null, 或者一个空对象, 例如返回数组时, 返回一个长度为0的数组) 返回一个 checked exception, 调用者必须在调用的时候捕获 返回一个 unchecked exception开发者需要在设计接口时具体选择哪一种处理方式, 需要开发者站在调用者的角度思考后才能确定. 也就是说需要思考, 调用者在接收到这个例外消息以后, 需要怎么处理这个例外.主要分2种情况: 可以确定是一个不可恢复的错误, 也就是说可以确定调用者接收到这种例外以后, 只能是把这个错误通知最终用户 (例如, 数据库异常, 文件打不开, 网络不可用等). 这个时候, 直接抛出一个 RuntimeException (一般来说是 RuntimeException 的子类), 调用者收到这种例外以后, 有可能根据具体的异常信息, 采用相应的恢复动作, 这种例外其实是属于业务逻辑的一种非正常状态. 例如, 火车票票价查询请求, 而请求中的车次是不存在的. 这种情况下, 调用者可以根据不同的异常消息采用不同的动作. 可以采用的处理方法: 用返回值通知调用者. (当返回值是一个字符串时, 很多时候返回一个长度为0的字符串比抛出一个 checked exception 要好) 用 checked exception 通知调用者, 好处是带更多种类的信息, 并强制提醒调用者进行处理. 坏处是一旦接口公布出去, 以后再想改动基本就不可能了, 所以抛出 checked exception 需要要深思熟虑另外这个实践方案不止适用于java异常处理，其他语言的异常处理也可以参考三, Reference http://onjava.com/pub/a/onjava/2003/11/19/exceptions.html http://www.oracle.com/technetwork/articles/entarch/effective-exceptions-092345.html http://stackoverflow.com/questions/613954/the-case-against-checked-exceptions http://stackoverflow.com/questions/3551221/guidelines-on-exception-propagation-in-java http://stackoverflow.com/questions/836104/exception-handling-standards-advice" }, { "title": "分布式系统中 Unique ID 的生成方法", "url": "/posts/Unique-ID/", "categories": "notes", "tags": "分布式", "date": "2013-12-08 00:00:00 +0800", "snippet": "本文主要介绍在一个分布式系统中, 怎么样生成全局唯一的 ID一, 问题描述在分布式系统存在多个 Shard 的场景中, 同时在各个 Shard 插入数据时, 怎么给这些数据生成全局的 unique ID?在单机系统中 (例如一个 MySQL 实例), unique ID 的生成是非常简单的, 直接利用 MySQL 自带的自增 ID 功能就可以实现.但在一个存在多个 Shards 的分布式系统 (例如多个 MySQL 实例组成一个集群, 在这个集群中插入数据), 这个问题会变得复杂, 所生成的全局的 unique ID 要满足以下需求: 保证生成的 ID 全局唯一 今后数据在多个 Shards 之间迁移不会受到 ID 生成方式的限制 生成的 ID 中最好能带上时间信息, 例如 ID 的前 k 位是 Timestamp, 这样能够直接通过对 ID 的前 k 位的排序来对数据按时间排序 生成的 ID 最好不大于 64 bits 生成 ID 的速度有要求. 例如, 在一个高吞吐量的场景中, 需要每秒生成几万个 ID (Twitter 最新的峰值到达了 143,199 Tweets/s, 也就是 10万+/秒) 整个服务最好没有单点如果没有上面这些限制, 问题会相对简单, 例如: 直接利用 UUID.randomUUID() 接口来生成 unique ID (http://www.ietf.org/rfc/rfc4122.txt). 但这个方案生成的 ID 有 128 bits, 另外, 生成的 ID 中也没有带 Timestamp 利用一个中心服务器来统一生成 unique ID. 但这种方案可能存在单点问题; 另外, 要支持高吞吐率的系统, 这个方案还要做很多改进工作 (例如, 每次从中心服务器批量获取一批 IDs, 提升 ID 产生的吞吐率) Flickr 的做法 (http://code.flickr.net/2010/02/08/ticket-servers-distributed-unique-primary-keys-on-the-cheap/). 但他这个方案 ID 中没有带 Timestamp, 生成的 ID 不能按时间排序在要满足前面 6 点要求的场景中, 怎么来生成全局 unique ID 呢?Twitter 的 Snowflake 是一种比较好的做法. 下面主要介绍 Twitter Snowflake, 以及它的变种二, Twitter Snowflakehttps://github.com/twitter/snowflakeSnowflake 生成的 unique ID 的组成 (由高位到低位): 41 bits: Timestamp (毫秒级) 10 bits: 节点 ID (datacenter ID 5 bits + worker ID 5 bits) 12 bits: sequence number一共 63 bits (最高位是 0)unique ID 生成过程: 10 bits 的机器号, 在 ID 分配 Worker 启动的时候, 从一个 Zookeeper 集群获取 (保证所有的 Worker 不会有重复的机器号) 41 bits 的 Timestamp: 每次要生成一个新 ID 的时候, 都会获取一下当前的 Timestamp, 然后分两种情况生成 sequence number: 如果当前的 Timestamp 和前一个已生成 ID 的 Timestamp 相同 (在同一毫秒中), 就用前一个 ID 的 sequence number + 1 作为新的 sequence number (12 bits); 如果本毫秒内的所有 ID 用完, 等到下一毫秒继续 (这个等待过程中, 不能分配出新的 ID) 如果当前的 Timestamp 比前一个 ID 的 Timestamp 大, 随机生成一个初始 sequence number (12 bits) 作为本毫秒内的第一个 sequence number整个过程中, 只是在 Worker 启动的时候会对外部有依赖 (需要从 Zookeeper 获取 Worker 号), 之后就可以独立工作了, 做到了去中心化.异常情况讨论: 在获取当前 Timestamp 时, 如果获取到的时间戳比前一个已生成 ID 的 Timestamp 还要小怎么办? Snowflake 的做法是继续获取当前机器的时间, 直到获取到更大的 Timestamp 才能继续工作 (在这个等待过程中, 不能分配出新的 ID)从这个异常情况可以看出, 如果 Snowflake 所运行的那些机器时钟有大的偏差时, 整个 Snowflake 系统不能正常工作 (偏差得越多, 分配新 ID 时等待的时间越久)从 Snowflake 的官方文档 (https://github.com/twitter/snowflake/#system-clock-dependency) 中也可以看到, 它明确要求 “You should use NTP to keep your system clock accurate”. 而且最好把 NTP 配置成不会向后调整的模式. 也就是说, NTP 纠正时间时, 不会向后回拨机器时钟.三, Snowflake 的其他变种Snowflake 有一些变种, 各个应用结合自己的实际场景对 Snowflake 做了一些改动. 这里主要介绍 3 种.1. Boundary flakehttp://boundary.com/blog/2012/01/12/flake-a-decentralized-k-ordered-unique-id-generator-in-erlang/变化: ID 长度扩展到 128 bits: 最高 64 bits 时间戳; 然后是 48 bits 的 Worker 号 (和 Mac 地址一样长); 最后是 16 bits 的 Seq Number 由于它用 48 bits 作为 Worker ID, 和 Mac 地址的长度一样, 这样启动时不需要和 Zookeeper 通讯获取 Worker ID. 做到了完全的去中心化 基于 Erlang它这样做的目的是用更多的 bits 实现更小的冲突概率, 这样就支持更多的 Worker 同时工作. 同时, 每毫秒能分配出更多的 ID2. Simpleflakehttp://engineering.custommade.com/simpleflake-distributed-id-generation-for-the-lazy/Simpleflake 的思路是取消 Worker 号, 保留 41 bits 的 Timestamp, 同时把 sequence number 扩展到 22 bits;Simpleflake 的特点: sequence number 完全靠随机产生 (这样也导致了生成的 ID 可能出现重复) 没有 Worker 号, 也就不需要和 Zookeeper 通讯, 实现了完全去中心化 Timestamp 保持和 Snowflake 一致, 今后可以无缝升级到 SnowflakeSimpleflake 的问题就是 sequence number 完全随机生成, 会导致生成的 ID 重复的可能.这个生成 ID 重复的概率随着每秒生成的 ID 数的增长而增长.所以, Simpleflake 的限制就是每秒生成的 ID 不能太多 (最好小于 100次/秒, 如果大于 100次/秒的场景, Simpleflake 就不适用了, 建议切换回 Snowflake).3. instagram 的做法先简单介绍一下 instagram 的分布式存储方案: 先把每个 Table 划分为多个逻辑分片 (logic Shard), 逻辑分片的数量可以很大, 例如 2000 个逻辑分片 然后制定一个规则, 规定每个逻辑分片被存储到哪个数据库实例上面; 数据库实例不需要很多. 例如, 对有 2 个 PostgreSQL 实例的系统 (instagram 使用 PostgreSQL); 可以使用奇数逻辑分片存放到第一个数据库实例, 偶数逻辑分片存放到第二个数据库实例的规则 每个 Table 指定一个字段作为分片字段 (例如, 对用户表, 可以指定 uid 作为分片字段) 插入一个新的数据时, 先根据分片字段的值, 决定数据被分配到哪个逻辑分片 (logic Shard) 然后再根据 logic Shard 和 PostgreSQL 实例的对应关系, 确定这条数据应该被存放到哪台 PostgreSQL 实例上instagram unique ID 的组成: 41 bits: Timestamp (毫秒) 13 bits: 每个 logic Shard 的代号 (最大支持 8 x 1024 个 logic Shards) 10 bits: sequence number; 每个 Shard 每毫秒最多可以生成 1024 个 ID生成 unique ID 时, 41 bits 的 Timestamp 和 Snowflake 类似, 这里就不细说了.主要介绍一下 13 bits 的 logic Shard 代号 和 10 bits 的 sequence number 怎么生成.logic Shard 代号: 假设插入一条新的用户记录, 插入时, 根据 uid 来判断这条记录应该被插入到哪个 logic Shard 中. 假设当前要插入的记录会被插入到第 1341 号 logic Shard 中 (假设当前的这个 Table 一共有 2000 个 logic Shard) 新生成 ID 的 13 bits 段要填的就是 1341 这个数字sequence number 利用 PostgreSQL 每个 Table 上的 auto-increment sequence 来生成: 如果当前表上已经有 5000 条记录, 那么这个表的下一个 auto-increment sequence 就是 5001 (直接调用 PL/PGSQL 提供的方法可以获取到) 然后把 这个 5001 对 1024 取模就得到了 10 bits 的 sequence numberinstagram 这个方案的优势在于: 利用 logic Shard 号来替换 Snowflake 使用的 Worker 号, 就不需要到中心节点获取 Worker 号了. 做到了完全去中心化 另外一个附带的好处就是, 可以通过 ID 直接知道这条记录被存放在哪个 logic Shard 上同时, 今后做数据迁移的时候, 也是按 logic Shard 为单位做数据迁移的, 所以这种做法也不会影响到今后的数据迁移" }, { "title": "Java GC 调优", "url": "/posts/java-gc/", "categories": "notes", "tags": "java", "date": "2013-09-08 00:00:00 +0800", "snippet": "序关于 Java GC 已经有很多好的文档了, 比如这些:12但是这里还是想再重点整理一下 Java GC 日志的格式, 可以作为实战时的备忘录.同时也会再整理一下各种概念一, JDK 6 提供的各种垃圾收集器先整理一下各种垃圾收集器.垃圾收集器: 新生代收集器: Serial, ParNew, Parallel Scavenge (MaxGCPauseMillis vs. GCTimeRatio 响应时间 vs. 吞吐量) 老生代收集器: Serial Old, Parallel Old, CMS垃圾收集器搭配总结: CMS 只能配 Serial 或 ParNew Parallel Scavenge 只能配 Serial Old 或 Parallel Old Serial 不能配 Parallel Old组合起来有以下几种: Serial + Serial Old (UseSerialGC): GC 线程在做事情时, 其他所有的用户线程都必须停止 (即 stop the world) Serial + CMS: 一般不会这样配合使用 ParNew + CMS (UseConcMarkSweepGC): 新生代的 GC 使用 ParNew, 有多个 GC 线程同时进行 Minor GC (主要是在多核的环境用多线程效果会好); 而老生代使用 CMS (CMS 后面会重点讲) ParNew + Serial Old (UseParNewGC): 新生代用 ParNew 的时候, 也可以选择老生代不用 CMS, 而用 Serial Old (实际上, 这个组合也不太常用) Parallel Scavenge + Serial Old (UseParallelGC): Parallel Scavenge 收集器的目的是达到一个可控制的吞吐率 (适用于各种计算任务); 这个组合中老生代仍旧使用 Serial Old Parallel Scavenge + Parallel Old (UseParallelOldGC): 新生代使用 Parallel Scavenge, 而 Parallel Old 是老年代版本的 Parallel Scavenge这里要注意一下 UseParallelGC vs. UseParallelOldGC, 如果没有调好配置, UseParallelOldGC 有可能比 UseParallelGC 的性能还要差。参考总结下来, 有 3 种场景: 客户端程序: 一般使用 UseSerialGC (Serial + Serial Old). 特别注意, 当一台机器上起多个 JVM, 每个 JVM 也可以采用这种 GC 组合 吞吐率优先的服务端程序: UseParallelGC (或者 UseParallelOldGC) 响应时间优先的服务端程序: UseConcMarkSweepGC二, Java 性能调优步骤预先设定调优目标 -&amp;gt; 部署方案 (单 JVM 部署 vs. 多 JVM 部署) -&amp;gt; 32位 or 64位 -&amp;gt; 优化内存占用 -&amp;gt; 优化响应时间 -&amp;gt; 优化吞吐率优化需要重点考量的 3 个因素: 内存占用 响应时间 吞吐率调优过程中的 3 原则: Minor GC 尽可能多的回收新生代对象 只要条件允许, 尽可能开大 JVM 的堆大小 优化中需要重点考量的 3 个因素, 只对其中的 2 个重点调优三, 内存占用调优 打开 GC 日志 Xms 和 Xmx 的值设置一致 Xmn 设置为新生代的值 应用运行稳定以后, 观察老生代和永生代所占的空间大小, 即 Live Data Size 利用 Live Data Size 估算调优开始时的各项配置: JVM 堆总大小: 3 - 4 倍 老生代的 Live Data Size 永生代大小: 1.2 - 1.5 倍 永生代的 Live Data Size 新生代大小: 1 - 1.5 倍老生代 Live Data Size 老生代大小: (堆总大小 - 新生代大小), 即 2 -3 倍 老生代 Live Data Size四, 响应时间调优调优响应时间可能的方法: 优化程序 调整部署 (单 JVM 或者 多 JVM) 减小对象分配, 降低 GC 频率调优步骤:监控 Minor GC 响应时间频率 -&amp;gt; 调整新生代大小: Minor GC 频率高 -&amp;gt; 增大新生代 Minor GC 时间长 -&amp;gt; 减小新生代监视 Full GC 响应时间频率 -&amp;gt; 调整老生代大小: 老生代空闲空间多长时间被填满决定了 Full GC 的频率 (空闲空间 = 老生代总空间 - Live Data Size) 填满空闲空间的速度由提升率决定 例如, 每次 Minor GC 提升 20M, 200M 的空闲空间 10 次 Minor GC 后会被填满 如果发现 Full GC 过于频繁, 增大老生代大小 (同时保持新生代大小不变) 但增大老生代大小会增加 Full GC 的时间. 所以需要对老生代大小进行调优. 如果调不好, 可以考虑用 CMS 替代 PS五, CMSCMS 的细节可以参考这里只着重整理一下调优相关的东东. CMS 可以不做 compacting, 所以可能避免 stop-the-world 的 compacting 过程 调优 CMS 的目的, 就是避免 stop-the-world 的 compacting 过程发生 (实际很难做到) CMS 可以和应用程序线程并行的运行, 也就是在应用程序运行的同时回收老生代空间, 避免老生代用完 但一旦老生代真的用完了, 就会发生恐怖的 stop-the-world 的 compacting GC 从 PS 迁移到 CMS 时, 最好先把老生代的大小扩大 20% - 30% 老生代不做 compacting, 所以老生代的内存碎片率特别关键. 有 2 个方法可以减轻内存碎片率: 1) 增大内存 2) 优化新生代到老生代的提升率1. PrintTenuringDistribution先介绍一下对象 age 的概念. JVM 中的一个对象新被创建时 age 是 0; 之后每次 Minor GC 后, 这个对象如果还在新生代中, 这个对象的 age 数加一.通过一个调优例子来说明 PrintTenuringDistribution 的用法打开 JVM 的参数:-XX:+PrintTenuringDistribution这样 gc.log 会有 Tenuring 相关的信息. 例如: Desired survivor size 8388608 bytes, new threshold 1 (max 15) - age 1: 16690480 bytes, 16690480 total threshold 1 表示对象在新生代中存活的阈值是 1. 也就是说, 一旦一个对象的 age 大于 1, 这个对象会晋升到老年代 age 1: 16690480 bytes: 表示在新生代中年纪是 1 的对象大小共有 16690480 bytes Desired survivor size 8388608 bytes 表示当前时刻, 需要的目标 survivor 空间为 8388608 bytes简单点说, 从这个输出, 我们发现, 当前时刻 survivor 目标空间只有 8388608 bytes, 而小于实际占用的 16690480 bytes 的空间. 因此, 我们需要扩充 survivor 空间的大小.需要把 survivor 空间扩大到多大呢?需要 16690480 bytes 的空间来装入所有 age 为 1 的对象. 而 CMS 会用一个比率来估算需要分配多少 survivor 目标空间 (这个比率的默认值是 50%). 所以, survivor 空间需要扩充到: 16690480 / 50% = 33,380,960 bytes结论: 把 survivor 空间的大小扩容到 33,380,960 bytes (大约 32M) 是合适的这里再详细说明一下前面提到的 CMS 用来估算 survivor 目标空间占用的那个比率.这个比率默认值是 50 (代表50%). 可以通过设置 JVM 的参数来配置, 例如:-XX:TargetSurvivorRatio=90不过一般来说, 极少情况需要配置这个值, 默认的 50 就 OK 了.总之, 我们通过打开 PrintTenuringDistribution 获取更多的 GC 信息来优化对象从新生代到老生代的提升率, 以及优化 Minor GC 的响应时间.2. 避免老生代的 stop-the-world要避免老生代的 stop-the-world 就是要保证 CMS 回收的内存的速度比从新生代晋升到老生代的速度快. 这样才能保证老生代不被填满而造成 stop-the-world.要保证 CMS 回收的速度, 要靠两个因素来保证: 老生代越大, 发生 stop-the-world 的概率越低 CMS 开始的时机越早, 发生 stop-the-world 的概率越低对第一个因素, 我们尽量给老生代分配更多的空间就行了.下面详细说说第二个因素.GC 参数中有一个:-XX:CMSInitiatingOccupancyFraction默认值是68. 其含义是, 每次当老生代的总空间的 68% 被占用的时候, 就进行 CMS.这个值调得越小, CMS 就会越早发生, 发生 stop-the-world 的概率就会更小;这个值调得越大, CMS 就会越晚发生, 发生 stop-the-world 的概率就会更大.所以我们可以通过调节这个值来调节 CMS 发生的时机, 避免 stop-the-world 的发生.但是也不能调的太小了, 太小了的话, 会触发不必要的 CMS, 降低了吞吐率另外需要注意, -XX:CMSInitiatingOccupancyFraction 要和 -XX:+UseCMSInitiatingOccupancyOnly 参数一起使用才会生效六, 吞吐率在响应时间调好以后, 可以考虑调吞吐率. 这里主要讲一下 CMS 吞吐率的调优方法: 增加新生代的大小 增加老生代的大小 调整 eden 和 survivor 的比率 调整 CMSInitiatingOccupancyFraction 参数注意: 以上 4 个调整方法需要和响应时间的调整做 trade off七, 其他一些可以考虑调整的 JVM 参数 -XX:PermSize -XX:MaxPermSize -XX:+PrintGCDetails -XX:+PrintGCDateStamps 打开或关闭 DisableExplicitGC -XX:+PrintTenuringDistribution -XX:+CMSParallelRemarkEnabled -XX:CMSMaxAbortablePrecleanTime -XX:ParallelGCThreads= -XX:+UseNUMA -XX:+ExplicitGCInvokesConcurrentAndUnloadsClasses -XX:+CMSScavengeBeforeRemark另外, 需要特别注意 -XX:+ParallelRefProcEnabled 这个参数的使用, 在有些 JDK 版本, 设置了这个参数会导致程序 hang 住 (具体参考八, Reference 1 « Java Performance » Charlie Hunt, Binu John" }, { "title": "Vim Notes", "url": "/posts/vim-notes/", "categories": "notes", "tags": "vim", "date": "2013-06-08 00:00:00 +0800", "snippet": "非插入模式下 英文句号 (.): 重复上一个动作 s: 删除当前字符, 并进入插入模式 f (字符): 跳到当前行下一个字符处. 英文分号 (;): 继续 f(字符) 的动作 cw: 删除当前单词 db: 删除当前单词 dw: 删除当前单词 daw: 删除当前单词 ctrl - e: 锁住光标向上移动 ctrl - y: 锁住光标向下移动 ctrl - a: 对一个数字做加一操作 ctrl - x: 对一个数字做减一操作" }, { "title": "Git Notes", "url": "/posts/git-notes/", "categories": "工具", "tags": "git", "date": "2013-06-07 00:00:00 +0800", "snippet": "1. 新建远端 Git 仓库 先取好库的名字 (例如: myfirstrepo), 然后在 GitHub 等 Git 服务上建立新的 repository （参考具体服务帮助） 在 GitHub 上新建好库以后, 在本地新建目录 myfirstrepo. 然后执行 git init 然后 新建 README.md AUTHORS 文件. 提交到本地库的 master 分支上面 git push -u ssh://XXXXX/myfirstrepogit master (用 push -u 把本地库上传到远端库) 最后, 配置好本地设置:git config user.name &quot;My Name&quot;git config user.email &quot;XXXXX@XXXX.com&quot;2. 远程库和本地库 远程库就是在服务器上的代码, 类似以前的 svn repository. 例如: ssh://XXXXX/myfirstrepogit 就是一个远程 Git 库. 本地库. Git 特殊的地方是 (相对 cvs, svn 等传统的版本管理工具), Git 不需要联网也可以进行版本控制工作 (只是提交都提交到了本地 的 Git repository), 所谓本地库就是本地的 Git repository 3. 远程分支和本地分支 远程分支: 远程库中的代码分支 (origin/master, origin/develop) 本地分支: 本地库中的代码分支 (master, develop)3. 初次从远程分支创建自己的工作分支例如, 我们把 服务器上 Git repository 中的 develop 分支下下来:git clone ssh://XXXXX/myfirstrepogit -b develop这次 clone 操作, 建立了本地的代码库, 而且对应一个远程库 origin (origin 是 远程库的名字)用下面的命令可以检查远程库信息:git remote show origin* remote origin Fetch URL: ... Push URL: ... HEAD branch (remote HEAD is ambiguous, may be one of the following): develop master...本地库中有一个 develop 分支, 同时跟踪一个远程分支 origin/develop检查一下本地分支:git branch* develop检查一下远程分支:git branch -r origin/HEAD -&amp;gt; origin/master origin/develop origin/master检查一下所有分支:git branch -a* develop remotes/origin/HEAD -&amp;gt; origin/master remotes/origin/develop remotes/origin/master5. 本地切换到其他的远程分支上去工作例如, 现在本地已经和远程的分支 master 同步了, 并在 master 上工作. 现在想同步远程的另外一个分支: develop, 并在本地切换到 develop 分支上去工作:git checkout --track origin/develop检查一下:git branch* develop master现在本地已经在 develop 分支上工作了6. 把自己的改动更新到远端库在 master 上工作:6.1. 初始状态6.2. 在本地做了一些改动, 并提交到本地库6.3. 把改动更新到远程库先 用 fetch (相当于 svn 提交之前, 先做 svn up) 把远程库中的更新拉到本地, 有更新的话, 本地库中的远程分支会被更新git fetch origin 34b420d..4f8b18f develop -&amp;gt; origin/develop ff56772..58708fa master -&amp;gt; origin/master这个时候发现远程也有更新, 本地也有更新, 所以需要先做一次 mergegit merge origin/master确认 merge 没有问题, 再更新到本地库git addgit commit -m &quot;merge...&quot;最后再更新到远程库:git push7. 本地工作模式 本地工作时, 可以任意建自己的分支, 做任意的提交 但最后要更新到远程库时, 先要把更新的内容放到 要更新的 分支上 (例如, 要更新远程的 dev 分支, 就要先把更新的内容先放到本地的 dev 分支上), 然后再 push 到远程库8. 新建一个远程分支先在本地准备好一个新的分支, 例如本地先准备好了一个新分支: feature-M5-2:git branch develop* feature-M5-2然后在把本地分支发布到远程库里面:git push origin feature-M5-29. Fork &amp;amp; upstream从 remote 库 clone 出来以后. 有一个默认的 remote: origin但是这个 remote 可能不是 upstream. 如果需要跟踪 upstream 的话, 需要再添加一个 upstream 的 remote添加命令如下:git remote add upstream https://gitlab.... (upstream 库的地址)10. Tag打一个tag:git tag -a v1.4 -m &#39;my version 1.4&#39;查看 tag:git tagpush tag 到 remote:git push origin --tags11. 合并多个提交在没有 push 到 remote 之前, 可以修改本地的提交. 例如合并最近3个提交:git rebase -i HEAD~3然后按 git 的提示做修改操作12. Git 基本工作流程该流程分为 3 个阶段。第一阶段：从「主分支」创建新的「工作分支」 在「主分支」（master）做一次 pull 确保「主分支」最新：git pull 从「主分支」创建一个新的「工作分支」，例如：feature_one：git branch feature_one checkout 到新的「工作分支」（feature_one）：git checkout feature_one第二阶段：在新的「工作分支」上工作 在新的「工作分支」（feature_one）上做开发测试 开发测试所需时间通常是 3 天 - 2 周。注意：在此期间，「主分支」（master）可能被其他人更新第三阶段：把新的「工作分支」合并到「主分支」 checkout 到「主分支」（master）：git checkout master 在「主分支」上做一次 pull 把「主分支」更新到最新：git pull checkout 到新的工作分支（feature_one)：git checkout feature_one 在新的「工作分支」上做一次 rebase：git rebase master checkout 回到「主分支」（master）：git checkout master 在「主分支」上做 merge：git merge feature_one。把「工作分支」上的内容合并到「主分支」 在「主分支」上做 push：git push 注意：在做上面第 4 步 rebase 的时候，可能提示有冲突，需要解决。此时需要按 git 的提示解决 rebase 冲突。完成 rebase 之后，再进行之后的步骤。Reference 图解Git (一级棒的参考)" }, { "title": "Linux Page Cache and Buffer Cache", "url": "/posts/page-cache/", "categories": "notes", "tags": "linux", "date": "2013-06-02 00:00:00 +0800", "snippet": "〇, 目的Linux Kernel 为存取速度慢的 Block 设备提供了两种比较通用的 Cache 机制: Page Cache: 为以页 (Page) 为单位的操作提供 Cache Buffer Cache: 为以块 (Block) 为单位的操作提供 Cache本文的目的就是介绍这两种 Cache 的相关知识.(待)" }, { "title": "Linux 内存管理", "url": "/posts/memory-manager/", "categories": "notes", "tags": "linux", "date": "2013-05-27 00:00:00 +0800", "snippet": "本文简单介绍 Linux Kernel 怎么管理内存. 包含内核物理内存管理和用户空间内存管理两大部分.1. Page FrameLinux Kernel 以物理 Page (即 Page Frame) 为基本单位来进行物理内存管理.而对不同的 Architecture, Page Frame 的大小也不一样, 不过一般的 32 位的 Architecture 的 Page Frame 大小为 4K Bytes.Kernel 用一个数据结构 struct page 来表示系统中的一个 Page Frame (下面只列出了 4 个重要的 fields):struct page { unsigned long flags; atomic_t _count; struct address_space *mapping; struct list_head lru; };4个重点字段: flags: 标识物理页面的状态 (例如, 是否是脏页) _count: 物理页面的引用计数, 一旦为 0, 说明这个页面没有被使用, 是 free 的 lru: 链表头. 多个 Page Frames 可以用链表组织起来, 例如, active 链表 和 inactive 链表 (以后介绍页面回收的时候会详细介绍) mapping 分两种情况: 如果物理页面被 Page Cache 使用, 并映射到一个文件, 那么 mapping 指向映射的文件的 inode 的 address_space 对象 (以后介绍 Page Cache 时会有更详细的介绍) 如果物理页面是一个匿名页, 那么 mapping 指向 anon_vma 对象, 而不是 address_space 对象 (关于匿名页, 以后会有详细介绍) 2. 进程地址空间从每个用户空态进程的角度来看, 它能使用的内存是一个独享的平坦线性的 32位 或 64位的虚拟地址空间 (一个大数组), 即进程地址空间 (Process Address Space).而这个大的内存空间 (32位的话, 有 4G) 又可以被分成很多内存区域 (Memory Areas), 每个 Memory Area 是一个连续的地址空间, 而且各个 Memory Area 不会重叠.对进程地址空间 (Process Address Space) 中的不同的 Memory Areas, 用户进程有不同的操作权限 (可读, 可写, 可执行). 例如, 一块区域是代码段, 那么, 用户进程对这块区域只有可读和可执行的权限.Kernel 用一个数据结构来描述进程地址空间: struct mm_struct, 其中包含了进程地址空间的所有信息 (每个进程对应一个 task 结构, 其中有一个 mm field 指向这个进程对应的 mm_struct).下面特别介绍 mm_struct 的 4 个关键 fields: mmap: 本地址空间包含的所有的 Memory Areas 的集合. mmap 是一个链表, 当遍历所有 Memory Areas 时使用; mm_rb: 和 mmap 字段一样, mm_rb 也是本地址空间所有 Memory Areas 的集合,不过不是链表, 而是一个 红黑树, 用来快速查找某个 Memory Area; pgd: 指向本进程 Page Table 的指针 (Page Table 后面会详细讲) mm_users: 有几个进程正在使用这个地址空间 (例如: 如果两个线程共享这个地址空间, 那么 mm_users 为 2)3. VMA在 Linux Kernel 中, 组成进程地址空间的 Memory Areas 被简称为 VMAs. VMA 的种类: 和文件相关的 VMA: 代码/库, 数据文件, 共享内存, 设备; 这些 VMA 的内容都是来至于文件的. 匿名 VMA: Stack, Heap, CoW pages; 这些 VMA 的内容都是用户程序管理的.Linux Kernel 用 struct vm_area_struct 来描述 Memory Area. 每个 Memory Area 对应一个 vm_area_struct, 下面介绍 vm_area_struct 的关键 fields: vm_mm: 指向 VMA 对应的 mm_struct; vm_start: vma 的起始位置 (低位) vm_end: vma 的结束位置 (高位). vm_end - vm_start 就是这个 VMA 的 size. 一组函数指针; 这些函数实现了在这个 VMA 上的各种操作 (page fault, open, close …)下图展示了管理一个进程地址空间 (Process Address Space) 怎么管理和它相关的 VMAs.4. Page Table用户进程地址空间只是一个虚拟地址空间, 当进程在运行时, 操作系统必须把进程正在使用的虚拟地址和物理地址对应起来.Linux 按页管理内存, 当用户进程要存取某个 Page, 但这个 Page 还没有存在在物理内存中, Linux 触发一次 Page Fault, 把这个 Page 和 物理 Page Frame 对应起来.Linux Kernel 利用 Page Table 来做虚拟地址到物理内存的地址映射 (每个进程都有自己的 Page Table).Linux Kernel 使用 4 级页表: PGD-&amp;gt;PUD -&amp;gt; PMD-&amp;gt;PTE; 操作系统只要先做好设置, 一般来说, 硬件 (CPU) 会自动做虚拟地址到物理地址的映射 (MMU);而且会利用 TLB 来加速映射 (映射缓存到 TLB).一旦切换进程, 也要切换 Page Table, 同时可能也要刷新 TLB. 也就是说, 各个进程的页表是隔离的, 不会互相影响.下图展示了利用页表把虚拟页映射到一个 Page Frame.5. 反向 Mapping上一节讲了了虚拟内存 Page 到 物理内存 Page Frame 的映射.而很多时候, 某个 Page Frame 可能会被多个 Page 映射 (例如一个共享库被多个进程共享); 这种物理页被多个虚拟页共享的机制可以节省内存.但是同时也引入了新的问题: 如果这个物理页长时间没有被使用, 当系统内存紧张时, 可能会把这个物理页换出到外存. 此时也需要通知那些映射到这个物理页的进程修改页表. 那么怎么才能知道某个 Page Frame 被哪些 Pages 映射了呢? 这就需要引入一个 反向映射 (反向 Mapping) 机制. Kernel 会利用一个数据结构来记录这个物理页被哪些 VMAs 使用.反向 Mapping 这里只是先提一下. 以后介绍 Page Cache 时会介绍更多的细节." }, { "title": "Basic Paxos", "url": "/posts/paxos-made-simple/", "categories": "分布式", "tags": "paxos", "date": "2013-05-26 00:00:00 +0800", "snippet": "Paxos Made Simple 论文 Notes。一. 算法背景Paxos 算法是用来解决什么问题？先看一个场景: 一个日志收集系统，需要从多个不同的机器上收集日志。最简单的实现方式是，找一台中心服务器，所有的日志生产机器作为客户端，不断的把日志发送到中心服务器，中心服务器存放所有收集到的日志。但这种实现方式有一个问题，中心服务器是一个单点，一旦中心服务器发生故障，整个系统不可用。改进方案: 找多个服务器来收集日志，例如 2 个。当每台日志生产机器需要提交日志时，每条日志都同时被提交到两台收集日志的服务器上面。这样，同时有多个日志收集服务器在工作，解决了单点问题。但这种多收集服务器的实现方式有另外一个问题，两台收集日志服务器上收集到的日志可能会不一致。例如，假设有两台日志生产机器 P1，P2; 同时有两台日志收集机器: Q1，Q2。考虑下面这种情况: P1 发送一条日志信息 M1 分别到 Q1，Q2 P2 发送一条日志信息 M2 分别到 Q1，Q2但由于实际网络的状况，日志被收集到的顺序在 Q1，Q2 可能会不一致，例如: Q1 收集到的日志的顺序是 M1，M2 而 Q2 收集到的日志的顺序是 M2，M1上面的请求只是一种不一致的情况，现实环境中，情况更加复杂，可能会遇到更多的问题 (例如，由于网络不稳定，P1 发出的消息 M1 到达了 Q1，但 M1 没有到达 Q2，丢失了)上面这种场景是一个典型的分布式系统的共识 (consensus) 问题: 在一个分布式系统中，有一些 processes，每个 process 都可以提出一个 value consensus 算法就是用来从这些 values 里选定一个最终 value 如果没有 value 被提出来，那么就没有 value 被选中 如果某个 value 被选中，那么所有的 processes 都应该被学习到有很多分布式一致性算法来解决这种问题。而 Paxos 算法就是一种分布式一致性算法，同时 Paxos 还具有容错性，能在各种错误环境中，保证一致性。二. 系统模型算法设定了三种角色: 提议者（proposers）：提议者提出提案 决策者（acceptors）：决策者负责批准提案 学习者（learners）：一旦某个提案被多数派决策者批准，那么就形成了一个决议。当决议形成以后，学习者负责学习决议 (我的理解是学习者感知到这个决议已经产生了，并以某种形式把决议持久化)在具体实现中，某个 process 可以扮演多个角色。这里还有一个 多数派 的概念: acceptors 数量为 n，超过 (n+1)/2 的 acceptors 的集合就是一个多数派。节点之间通过发送消息进行通信，通信方式采用异步，非拜占庭模型，该模型中： process 以任意的速度进行操作，可能因为故障而停止，也可以重新启动。并且 processes 选举出来的决议的值不会因为重启等故障而丢失 消息可以延迟发送，多次发送或丢失，但不会被篡改（即不存在拜占庭问题）一般来说，分布式算法都需要满足 Safety 和 Liveness： Safety：一般指一致性，正确性 Liveness: 一般指不会死锁，活锁具体到 Paxos 算法的系统模型中：Safety: 最后形成的决议必须是之前被提议者提出的提案 算法的每次执行，最后只能形成一个决议（这个最重要，如果不满足这个，那么会形成多个决议，就出现不一致了） 只有形成决议以后才能被学习者学习（在形成决议之前，提案是不能被学习的）Liveness: 只要有提案被提出，保证最终有一个提案会被选出来做为决议 决议形成以后，学习者最后能学习到决议三. 解决思路基于前面描述的系统模型，最简单的解决办法就是只设置一个 acceptor，而这个 acceptor 只批准它接收到的第一个提案。但这种实现中的 acceptor 是一个单点，不能处理各种故障。改进方式，利用多个 acceptors + 多数派： 设置多个 acceptors 每个 acceptors 最多只能批准一个提案（如果每个 acceptor 能批准多个提案，就乱了，不能形成多数派） 对于某个提案，只要 acceptors 中有一个多数派批准，就形成决议另外，为了满足 Liveness，还需要规定：P1: 一个 acceptor 必须批准它收到的第一个提案这样就保证了即使只有一个提案被提出，也必然能形成决议。但这种解决方案存在问题: 某个 acceptor 发生故障，就可能不会出现多数派，不能得到决议；这时，就可能需要进行多次提交提案，进行多次批准。 而允许多次提交，多次批准，就可能违背 Safety 中的第二条。可能形成多个不同内容的决议。为了解决「允许多次提交，多次批准」可能造成的形成多个不同内容的决议的问题，Paxos 在前面的实现方案的基础上，又加入了一系列的限制条件（P2，P2a，P2b，P2c）： P2 -&amp;gt; P2a -&amp;gt; P2b -&amp;gt; P2c 只要满足 P2 就能满足 Safety 的第二条，但是 P2 比较难实现 然后又想出来 P2a；只要满足 P2a，P2 也就满足了；但是发现 P2a 存在问题，需要修订（P1 和 P2a 有冲突，如果要满足 P1，有的情况下，就做不到 P2a） 然后又想出来 P2b；P2b 和 P1 不冲突，只要 P1 和 P2b 同时满足就可以解决问题；但 P2b 比较难实现 然后又想出来 P2c；只要满足 P2c，P2b 也就满足了；Paxos 算法就是基于 P2c 实现的下面一节详细介绍这些限制条件。四. P2 -&amp;gt; P2a -&amp;gt; P2b -&amp;gt; P2cPaxos 算法中，给每个提案引入了 版本号 的概念。每个提案由两部分组成: 提案内容: v 提案版本号：n这样每个提案就是一个二元组: {v, n}同时，Paxos 规定，在全局范围内（所有的 processes 提出的提案），每个提案有唯一的版本号，这样所有的版本号就形成了一个全序关系。而且，既然所有提案的版本号是全局范围内的全序关系，也就给所有的提案进行逻辑时间上的排序提供了可能：例如，两个提案: V{v, n}，W{w, m}，如果 n &amp;gt; m，可以认为提案 V 比 提案 W 后提出。后面具体介绍算法时也会提到，怎么利用版本号来规定提案的时间顺序。1. P2P2: 假设一个提案 {v, n} 已经被选为决议，对于选出来的其他所有决议 {w, m}， 如果 m &amp;gt; n， 那么内容 w 必须和内容 v 相同说明:前面已经说过，利用版本号可以在逻辑上对全局时间排序，m &amp;gt; n，代表，{w, m} 是在 {v, n} 之后的提案 (但怎么利用版本号来排序这里没有说，后面才会讲，现在姑且这样认为)。P2 其实就是说（参考前面 Safety 的第二条），一旦已经形成了一个决议，那么之后如果又发生了多次提交，多次批准，而形成了新的决议，那么形成的新的决议必须和之前形成的决议的内容相同。2. P2a因为 P2 限制很难实现，就想出了 P2a。P2a：假设一个提案 {v, n} 已经被选为决议，其他所有被 acceptors 批准的提案 {w, m}，如果 m &amp;gt; n，那么内容 w 必须和内容 v 相同显而易见，P2a 被满足了的话，P2 也就满足了。但 P2a 存在一个问题：P2a 和 P1 有些时候会冲突（P1 是 Liveness 的限制），也就是说，满足了 P1 的话，有的时候，做不到满足 P2a： 提案 V{v, n} 已经被选为决议 但在这次选举过程中，某个 acceptor 没有批准过任何提案 (也就是说，这个 acceptor 还可能批准更高版本号的提案，例如：W{w, m}，m &amp;gt; n） 之后，有一个 proposer 提交了一个提案 W{w, m} 给上面这个 acceptor，而根据 P1，这个 acceptor 会批准这个提案，且这个提案的 m &amp;gt; n，w 和 v 不相同 这就和 P2a 矛盾了：在 V{v, n} 已经被选为决议的情况下，任何 acceptor 批准的更高版本（&amp;gt; n）的提案的内容必须和 v 相同说明: 在有一个决议已经形成的情况下，是有可能发生某个 proposer 提出新的不同内容的提案的可能的。3. P2b为了解决 P2a 和 P1 的冲突，想出了 P2b:P2b: 假设一个提案 {v, n} 已经被选为决议，所有被 proposer 提出的提案 {w, m}，如果 m &amp;gt; n，那么内容 w 必须和内容 v 相同一个提案被 acceptor 批准之前肯定要由某个 proposer 提出，因此 P2b 就隐含了 P2a，进而隐含了 P2。4. P2c怎么做到 P2b 呢？又想出了 P2c。P2c：如果某个 proposer 要提出一个提案 {v, n}，那么必须满足下面两个条件中的一个： 存在一个 acceptors 的多数派 S，其中的任何一个 acceptor 都没有批准过比 n 小的提案 存在一个 acceptors 的多数派 S，集合中的某些 acceptor 曾经批准过小于 n 的提案，而所有的这些版本号小于 n 的提案中，拥有最大的版本号那个提案的内容与 v 相同也就是说，如果每个 proposer 都按 P2c 为约束来提出一个提案，那么就可以满足 P2b，即： 要么还没有形成决议，可以任选提案的内容 要么提出的版本号更高的提案的内容和已经形成的决议的内容相同对 P2c 的理解如下:证明1：假设现在 n=4 的 proposal 已经被选举出来了，那么我们要证明在满足上面两个条件的前提下提出一个 n=5 的提案，那么这个 n=5 的提案的内容肯定和 n=4 的提案的内容一致 第一个条件：能找到一个多数派集合，其中所有的人都没有批准过比 5 小的提案（这种情况是不可能发生的，因为 n=4 的提案已经被通过了） 第二个条件：能找到一个多数派集合，首先来看，如果其中某人批准过 n=4 的提案，只要这个n=4 的提案的内容和 n=5 的提案的内容相等的话（4 是比 5 小的数中最大的一个），那么如果 n=5 的提案被选举出来，就是 ok 的（不会破坏上一次选举 n=4 的结果）；其次，会不会其中没有人批准过 n=4，只发现有人批准过 n&amp;lt;=3 的提案？不可能，因为 n=4 的提案已经被多数派批准选举出来了证明2：假设现在 n=4 的 proposal 已经被选举出来了，那么我们要证明在满足上面两个条件的前提下提出一个 n=6 的提案，那么这个 n=6 的提案的内容肯定和 n=4 的提案的内容一致 第一个条件：能找到一个多数派集合，其中所有的人都没有批准过比 6 小的提案（这种情况是不可能发生的，因为 n=4 的提案已经被通过了） 第二个条件：能找到一个多数派集合，首先来看，如果其中某人批准过 n=5 的提案，而这个 n=5 的提案的内容肯定和 n=4 的提案的内容相同（证明 1 证明）。所以只要这个 n=5 的提案的内容和 n=6 的提案的内容相等的话，n=6 的提案的内容也就和 n=4 的提案的内容相同总之，只要 proposer 提交提案时遵守 P2c，一旦某个版本号的决议形成，之后形成的更高版本的决议的内容也和最早形成的决议的内容相同。这里可能有人会提出疑问，前面说了这么多，都说的是，决议从小到大形成的情况，会不会有先形成大版本号的决议，再形成小版本号的决议的情况？比如下面这种场景： 先形成了 4 号决议 接下来，有一个 proposer 企图提出版本号为 3 的提案。此时这个 proposer 是否能够找到一个多数派，其中所有的人都没有批准过比 3 小的提案？（多数派里面的所有成员都批准了 4 号提案）答案是考虑 P2 系列的约束时，不用考虑版本号会降低的情况，因为下面一个小节引入的 P1a。一旦 4 号决议形成，n &amp;lt; 4 的提案就不可能被提出。也就是说，多个决议形成，只能是从小的版本号升到大的版本号。5. 提案生成算法保证 P2c，也就是要保证 proposer 在提出一个提案时，必须满足 P2c。所以 proposer 想要提交提案时需要 2 个步骤： 先探查一下是否能找到 P2c 中描述的多数派 如果找到了多数派，就可以提交提案了（根据找到的多数派的不同类型，或者提出任意内容的提案，或者提出和之前形成的低版本的决议内容一致的提案）这里有个问题要特别注意一下:因为这两个步骤之间有时间间隔，可能在第一步找到了一个多数派，但到了第二步要提交提案的时候，第一步找到的那个多数派中的某个 acceptor 的状态已经变化了。此时如果再提交提案，那个多数派可能已经不满足 P2c 了。所以，在第一步探查多数派的同时，也要锁死被探查的 acceptor 的状态。具体怎么做，看下面的提案生成算法的描述：proposer 选择一个版本号 n，然后向一个 acceptors 的集合发送请求，要求 acceptor 做出以下动作： 承诺不再批准任何小于 n 的提案（锁定 acceptor 的状态不破坏 P2c） 返回这个 acceptor 到目前为止已经批准过的，且小于 n 的最大版本号的提案。返回消息为：内容+版本号（如果没有批准过任何版本号小于 n 提案，返回一个空值）如果 proposer 收到了一个多数派的返回响应，那么这个 proposer 可以提出版本号为 n 的提案，其对应的内容为： 如果返回的所有响应都是一个空值（也就是说，存在一个多数派没有批准过小于 n 的提案），提出的提案的内容可以任选 找出所有响应中最大版本号的内容，作为要提交的提案的内容总之，proposer 生成提案需要两个步骤，发送两种请求： prepare 请求: 询问状态; 希望 acceptor 端做出承诺，并返回响应 accept 请求：提交提案；希望 acceptor 端批准提案前面说的是 proposer 端的算法。接下来说一下 acceptor 端的算法： acceptor 收到 proposer 的两种请求都可以不做任何回答（因为不回答不会破坏算法的安全性） 如果要对 prepare 请求做出响应，只要做出承诺，并根据其当前的状态返回响应就行了（找出目前为止已经批准过的，且版本号小于 n 的最大版本号的提案，作为响应返回） 而对于 accept 请求，必须查看是否之前已经做过承诺，只有在不违反承诺的情况下才能返回响应这就是 P1a：一个 acceptor 只有在没有响应过任何版本号大于 n 的 prepare 请求时，才会批准一个编号为 n 的提案。换句话说，acceptor 如果承诺过不会批准任何小于 n 的提案，那么它会拒绝之后收到的任何小于 n 的 accept 请求。而 P1a 和 P1 不矛盾。所以只要同时满足 P1a 和 P2c 就可以达到 Safety。P1a 的存在也规定了在 Paxos 算法中，多个决议形成，只能是从小的版本号升到大的版本号（例如：一旦 3 号决议形成，之后再形成新的决议，新的决议的版本号 &amp;gt; 3）五. Paxos 算法上一节已经说过了，同时满足 P1a 和 P2c 就可以达到 Safety 满足一致性。不过在这个基础上还可以做一个优化： 如果某个 acceptor 先对版本号为 5 的 prepare 请求返回过承诺，那么当它之后收到一个版本号为 4 的 prepare 请求时，这个 acceptor 可以忽略这个版本号为 4 的 prepare 请求 因为，就算对 4 号 prepare 请求返回响应，之后也不会批准 4 号提案的 accept 请求acceptor 需要持久化两种信息： 它已经批准过的最大版本号的提案 它已经承诺过的 prepare 请求中的最大版本号的请求的版本号这里要特别解释一下为什么只需要保存「已经批准过的最大版本号的提案」就行了？前面说了，acceptor 收到一个版本号为 n 的 prepare 请求时，必须找出目前为止已经批准过的，且版本号小于 n 的最大版本号的提案，作为响应返回。又由于上面的这个优化，当收到一个版本号为 n 的 prepare 请求时： 如果发现目前已经批准过的所有提案的最高版本号为 m，且 n &amp;lt;= m，那么可以不响应这个 prepare 请求（因为采用前面的优化，一旦做过版本号为 m 的承诺，就可以不给所有小于等于 m 的 prepare 请求返回响应） 如果发现目前已经批准过的所有提案的最高版本号为 m，且 n &amp;gt; m，那么就返回这个版本号为 m 的提案就行了总之，当 acceptor 重启恢复以后，只要能重新拿到这两个信息，就可以恢复到正常工作状态。同时，proposer 不需要持久化任何信息，可以随意重启（只需要每次提交提案时，都用一个全局唯一的版本号就行了）【完整的算法如下】: 3 种身份：proposer，acceptor，learner 2 个阶段：【准备阶段】 &amp;amp;【 批准阶段】 准备阶段： proposer 群发一个 prepare 请求（版本号为 n） 给一个 acceptor 的多数派 每个 acceptor 会存储曾经在【批准阶段】批准过提案中，最大版本号的提案（w+m），w 为提案内容，m 为提案版本 每当 acceptor 接收到一个 prepare 请求（版本号为 n）： 如果 n &amp;gt; m，那么把（w+m）返回给 proposer 如果之前没有批准过任何提案，返回「空」给 proposer 如果 n &amp;lt;= m，那么 proposer 不对这个 prepare 请求（版本号为 n）做响应 承诺今后不会批准「版本号」比 n 还小（也就是 &amp;gt;= n 的提案才会被批准）的任何提案 准备阶段解析： 准备阶段的目的是保障 proposer 在「批准阶段」提交「版本号」为 n 的提案时，要么还没有形成决议，要么提交的版本号为 n 的提案和已形成决议的提案的内容相同 也就是要在准备阶段探察出一个 acceptor 的多数派： 要么都没有批准过比 n 小的提案 要么把这个多数派中批准过的「版本号」比 n 小的提案中，拥有最大版本号的提案的内容，作为版本号为 n 的提案 所以 acceptor 在收到一个「版本号」为 n 的 prepare 请求时，需要找出批准过的，比 n 小的最大版本号的提案的（内容+版本号）返回给 proposer 按道理，acceptor 需要存储所有曾经批准过的（提案内容+提案版本）；但可以做一个优化：只存储曾经批准过的最大版本号的 提案内容 + 提案版本（w+m） 因为当接收到的 prepare 请求的「版本号」n&amp;lt;=m 时，那么 acceptor 不应该对这个版本号为 n 的 prepare 请求做应答（这时因为，即便是做了应答，但由于在批准阶段，曾经承诺过不会批准比 m 小的提案，版本号为 n 的提案会被拒绝掉） 每个 acceptor 需要存储承诺过的「最大版本号」（假设为 maxVersion） 批准阶段： proposer 从 acceptor 的多数派的获得到了 prepare 请求的响应 proposer 从这些响应中挑出 版本号「最大的」提案内容（假设该内容为 w） proposer 群发一个「批准请求」（版本号为 n，内容为 w） 给一个 acceptor 的多数派 如果 proposer 获得的所有响应都为空，那么群发的「提案」可以是任何内容 每个 acceptor 接收到一个「批准请求」（版本号为 n，内容为 w）： 如果不违反之前的承诺（n &amp;gt;= maxVersion 的提案才会被批准），就返回「批准响应」给 proposer 否则，对这个「批准请求」不做回应 六. 思考 在现实中，其实是更关注 Paxos「多实例」的场景，也就是不是单次 Paxos 过程，而是一序列的多个 Paxos 过程，例如：instance0, instance1, instance2… 但值得注意的是，并不要求时间上按顺序完成这些 instances。例如，可以先完成 instance1 和 instance2，但 instance0 可能一直未完成；也就是出现了一个「实例空洞」 为什么可以不按顺序完成这些 instances？这时因为计算机的工作可以抽象成：数据+计算；而 Paxos 只关注数据上达成共识，而出现了 instance 空洞，是否能完成计算，可由计算的具体策略来决定，Paxos 无需关注 参考文档 https://github.com/dsdoc/dsdoc/blob/master/paxosmadesimple/index.rst http://hi.baidu.com/tkdsheep/item/bebe140729acd60b6c9048f7 http://blog.csdn.net/sparkliang/article/details/5740882 http://blog.csdn.net/colorant/article/details/8431934 http://www.vpsee.com/2009/09/paxos-algorithm/ http://stblog.baidu-tech.com/?p=1196" }, { "title": "Redis PSYNC", "url": "/posts/Redis-PSYNC/", "categories": "notes", "tags": "redis", "date": "2013-03-14 00:00:00 +0800", "snippet": "本文主要介绍 Redis 2.8 新引入的增量同步 PSYNC 机制.一, 动机Redis 的 2.8 版本已经进入 beta 阶段. 这个版本最重要的特性就是新增了增量同步的功能.之前 Redis 在生产环境中最让人质疑的就是它的全量同步, 也就是每次从节点连上主节点的时候, 都会把主节点的所有数据同步过来, 哪怕是只是一次网络瞬断.而 Redis 做一次全量同步的代价是巨大的. 接下来先简单的回顾一下 Redis 原来的全量同步 SYNC 实现, 然后再会介绍增量同步的实习原理.二, 全量同步 (SYNC)目前 Redis 主从同步的简要流程: 每次 slave 连到 master 后, master bgsave 到 rdb 文件 master 发送 rdb 文件给客户端 slave 收到 rdb 文件后复制 master 开始向 slave 发送写命令 (master 作为 slave 一个特殊的 client)整个同步过程是一个状态机: slave 有6个状态: REDIS_REPL_NONE, REDIS_REPL_CONNECT, REDIS_REPL_CONNECTING, REDIS_REPL_RECEIVE_PONG, REDIS_REPL_TRANSFER, REDIS_REPL_CONNECTED slave 接收到 slave 命令, 或者启动的时候从配置文件里面读到 slave 配置. 做初始化工作 (例如: 断开所有的slaves). 初始化结束后, 状态为: REDIS_REPL_CONNECT Redis 周期的调用 replicationCron 函数 REDIS_REPL_CONNECT 状态时, replicationCron 中会创建一个socket fd 连接到 master, 并在上面创建读写事件. slave 状态为 REDIS_REPL_CONNECTING. 消息循环中, socket fd 会阻塞发送一个 PING 消息给master (发送成功后, delete fd 的写事件) slave 状态为 REDIS_REPL_RECEIVE_PONG 消息循环中, 如果 slave 状态为 REDIS_REPL_RECEIVE_PONG. 开始阻塞读取 master 返回的响应. 并删除读事件. 并认证. 认证成功了阻塞发送 SYNC 命令请求同步. 发送了 SYNC 命令后, slave 建临时文件准备接收 rdb 文件. 创建读文件时间, 状态为: REDIS_REPL_TRANSFER. 开始在消息循环中获取 rdb 内容. 完全获取到 rdb 文件后, 把 master 作为 一个特殊的 client, 开始接收写命令. slave 状态为: REDIS_REPL_CONNECTED master 有4个状态: REDIS_REPL_WAIT_BGSAVE_START, REDIS_REPL_WAIT_BGSAVE_END, REDIS_REPL_SEND_BULK, REDIS_REPL_ONLINE master 接收到 SYNC 命令后, 线性检查是否正好有其他的 slave 正在做 bgsave. 如果有, 就直接开始传输 rdb 文件. 没有的话, 之后开始做 bgsave 来创建 rdb 文件 (创建过程中, master 的状态为: REDIS_REPL_WAIT_BGSAVE_START). 子进程中创建rdb文件 rdb 创建好了以后, master 的状态为 REDIS_REPL_WAIT_BGSAVE_END. 开始发送 rdb 文件的内容给 slave. serverCron 里面检测到有bgsave子进程结束, 顺序检查 slaves 列表. 如果是 END 状态的 slave, 创建写事件. 把 rdb 文件的内容在消息循环中发送给 slave. master 为 REDIS_REPL_SEND_BULK rdb 文件内容发送完成以后, 删除原先的写事件, 新建发送写命令到 slave 的写事件. 状态为: REDIS_REPL_ONLINEmaster 的同步过程中的关键点: 在发送 rdb 文件的过程中, master 新获取到的写命令被缓冲在 client 对象的发送缓冲区中. 一旦进入 REDIS_REPL_ONLINE, 会把缓冲区的写命令发送给 slave 进行写操作 多个阶段写事件的创建和删除 只要是 REDIS_REPL_WAIT_BGSAVE_START 状态之后的写命令, 都会写入到 slave 的写缓冲区, 等待发送给 slaves 线性遍历 slaves, 效率不太好总之, 全量同步实现比较简单: 把所有对 master 的更改命令在 slave 上回放 对 slave 来说, master 只是一个 client, slave 接收并执行这个特殊的 client 发送过来的命令. 并没有用什么特殊的命令和消息格式来做复制 每次 slave 链接到 master 都要做一个 master 的镜像, 然后把镜像发送到 slave 这边, 再 load 进来.采用这样的方式来实现同步主要是基于 Redis “简洁至上” 的原则, 也就是尽量少的引入复杂性来完成任务.采用全量同步来实现主从间的同步, 代码改动比较少, 而且实现逻辑简单, 很容易的就保证了整个同步过程的一致性, 提供了高可靠.但这种简单的实现方式也付出了代价: 在生产环境中, 如果主从间的网络不稳定, 会频繁的发生全量同步, 而全量同步时, 系统的性能会急剧下降 (从节点会阻塞, 主节点要用 COW 的方式来生成RDB文件)事实上很多公司在线上大规模的使用 Redis 时, 都对 Reids 的同步机制做了优化. 而 Redis 的作者在使用者抱怨很久以后, 也在 2.8 中加入了增量同步功能, 希望能解决这个问题下面介绍一下新增加的增量同步功能三, 增量同步 (PSYNC)在实现 PSYNC 的时候, Redis 的作者也先考虑怎样最简洁的来实现, 避免引入过多的复杂性:最直接的实现想法就是用一个 AOF 文件记录 master 所有的数据, 这样当 slave 需要做增量同步时, 只要根据 offset 位置从 AOF 文件中拿到需要增量同步的数据发送给客户端就能实现增量同步;但是这样的实现方式需要写 AOF 文件代价比较大. 作者最后实现的方法是用内存, 而不是用文件来解决问题!而想用内存来解决问题, 就不可能保存太多的数据, 所以做了两个需求上的妥协:slave 断开后需要尽早的连上来, 才能做增量同步, 如果间隔了太久才连上来, 只能做全量同步 (断开后很快就连上来, master 那边就不用保存太多的数据, 可以用内存来解决)断开的原因不是 master 重启了 (重启之后, 内存里面的数据就都丢失了, 只能做全量同步)做这两个妥协来实现增量同步还是划得来的, 因为我们之前讨论过, 增量同步主要时解决网络瞬断的问题重连发生时, 大概的过程:slave端:[60051] 17 Jan 16:52:54.979 * Caching the disconnected master state.[60051] 17 Jan 16:52:55.405 * Connecting to MASTER...[60051] 17 Jan 16:52:55.405 * MASTER &amp;lt;-&amp;gt; SLAVE sync started[60051] 17 Jan 16:52:55.405 * Non blocking connect for SYNC fired the event.[60051] 17 Jan 16:52:55.405 * Master replied to PING, replication can continue...[60051] 17 Jan 16:52:55.405 * Trying a partial resynchronization (request 6f0d582d3a23b65515644d7c61a10bf9b28094ca:30).[60051] 17 Jan 16:52:55.406 * Successful partial resynchronization with master.[60051] 17 Jan 16:52:55.406 * MASTER &amp;lt;-&amp;gt; SLAVE sync: Master accepted a Partial Resynchronization.master端:[59968] 17 Jan 16:52:55.406 * Slave asks for synchronization[59968] 17 Jan 16:52:55.406 * Partial resynchronization request accepted. Sending 0 bytes of backlog starting from offset 30整个过程就是先尝试做增量同步: 只要能从 master 端把增量同步offset点之后的数据拿到, 就能成功的做增量同步具有了增量同步, Redis 会更好更强大 (作者原话). 当前 2.8 已经开发完成, 正在测试阶段, 相信很快会 release.参考 http://antirez.com/news/47 http://antirez.com/news/49" }, { "title": "系统性能监控之 Swap Space", "url": "/posts/swap-space/", "categories": "notes", "tags": "swap", "date": "2012-09-23 00:00:00 +0800", "snippet": "对线上服务器进行性能监控时, 需要关注各种性能指标, 从各个方面来对系统性能进行监控. 例如, 系统负载, cpu 占用, 内存占用, 网络带宽等. 其中 Swap Space 的使用状况也是值得关注的一项, 本文对在 Linux 环境中监控 Swap Space 的相关内容进行了总结.一, 首先简单的介绍一下什么是 Swap Space用户进程内存空间中数据有两种: 从文件系统中读进来的数据 (主要有文件内容高速缓存, 程序代码和共享库) 程序使用的堆栈空间而Linux 是一个分页请求系统, 用户进程使用的所有内存需要映射到物理内存: 应用程序的地址空间是虚拟空间, 是按被分成固定大小的页 (page) 来管理的 物理内存也是按固定大小的页框 (page frames) 来组织 虚拟页面 (page) 需要由操作系统映射到物理内存中的页框 (page frames)为了更好的利用物理内存, Linux 会对在物理内存中的页面进行回收 如果存放的是程序代码或共享库, 可以直接回收 (以后需要的时候可以从文件里面直接读入) 如果缓存的是文件内容, 如果是脏页, 先写回磁盘后再回收; 如果不是脏页可以直接回收 而对于程序使用的堆栈空间, 由于没有对应的文件 (叫做 “匿名页”, anonymous pages), 只能是备份到磁盘上一块专门的分区, 这个专门的分区就是 Swap Space. 一旦回收算法既不能从文件缓存回收内存, 又不能从正在使用的匿名页中回收内存, 而系统需要满足更多的内存请求, 这个时候只能用最后一招了: OOM kill从 swap space 的原理得出以下结论: Swap Space 不会拖慢系统. 事实上, 不分配 swap space 并不代表就没有换进换出发生 (非匿名页还是可能会被换进换出).有了 Swap Space, 只是说在试图回收内存时, Linux 有了更多的选择 Swap Space 只是匿名页专用的. 在任何情况下, 程序代码, 共享库, 文件系统cache不会使用 Swap Space 由上面的 1, 2 两点可以知道: “尽量给 Swap Space 分配很小的空间” 唯一的好处就是不浪费磁盘空间 (也就是说最小化 Swap Space 大小并不能提升系统性能) 系统监控时, 要特别注意是否发生了 OOM 的情况, 一旦 OOM 发生, Linux 会自己挑选一个消耗内存较大, 又不 “重要” 的程序 kill 掉来回收内存 (但很多时候被 kill 掉的正好是某个应用程序)二, 对 Swap Space 进行监控先利用 free 命令来监控 swap space 的使用率. 例如:最后一行显示 swap space 的使用状况: total 显示 swap space 的总容量 (2G) used 显示已用空间大小 (43M 左右) free 显示空闲空间大小 (1.95G 左右)对系统进行监控时, 可以监控 used占用了total的比例.但是特别需要注意的是, 这个比值较高时, 并不等于系统内存紧张了. 只有当在 Swap Space 上有大量的换入换出操作时才说明出现了内存紧张.例如, 某个程序启动时使用了大量的内存, 造成系统的 Swap Space 消耗过大. 但启动完成之后, 进入平稳运行期时, 并不需要很多内存 (从这个例子也可以看出来, Swap Space 分配过小可能造成有时候本来可以启动起来的程序启动不起来).可以用 vmstat 或 sar -W 命令来监控换入换出操作的状况例如, 使用 sar -W:或者使用 vmstat:在这个例子里面, 可以看出, 虽然 Swap Space 被使用了一点, 但是没有在 Swap Space 上的换入换出, 所以系统性能还是不错的.总之, 系统性能可能出现以下几种情况: 极好: Swap Space 没有被使用. 物理内存的使用也很少. 好: Swap Space 用了一点, 但是在 swap space 上没有换进换出的操作 还行: Swap Space 用了一点. 有少量的在 Swap Space 上的 换进换出的操作. 不过系统的吞吐量还是不错 (CPU在 User 上的消耗还比较高, 也就是说明此时在 Swap Space 上的换入换出还没有造成系统瓶颈) 有问题: 在 Swap Space 上有大量的换入换出操作, 同时 CPU 在 Sys 和 Wait 上的消耗很高 (这种情况就是说明内存不够用了, 需要查看是否有程序发生了内存泄漏) 有问题: 系统运行良好, 但是启动新程序会由于内存不足而失败 (这种情况是 Swap Space 分配得太小了)三, 和 Swap Space 相关的系统参数的调节vm.swappiness (缺省值是60). 设置成0, Linux 会尽可能的避免把内存交换出去 (可以让系统有好的响应时间). 相反, 如果设置成100, Linux 会尽量的使用 swap space.vm.overcommit_memory (缺省值是0): 0: 允许 overcommit (当收到申请内存的请求时, Linux 用一套heuristic方法来决定是否要 overcommit), 但是就像前面提到的, overcommit 之后, 当真正需要用到这些内存时, 如果回收算法又不能回收到任何物理内存, 只好OOM kill 掉一个进程 1: 总是允许 overcommit (Redis 就建议这样设置) 2: 不允许进行 overcommit, 一旦申请的空间超出总空间 (总空间 = 物理空间 + swap space size x overcommit_ratio, overcommit_ratio 缺省值是 50%), 就报错参考 http://www.linuxjournal.com/article/10678 http://hi.baidu.com/_kouu/item/4c73532902a05299b73263d0" }, { "title": "Autotools 使用入门", "url": "/posts/autotools/", "categories": "工具", "tags": "autotools", "date": "2012-06-24 00:00:00 +0800", "snippet": "序. Intro介绍 Autotools 的相关文章网上已经太多了，但这些文章要么是把一大堆工具列举出来，要么就是来一个 step by step 的例子。但 Autotools 这套工具本身比较杂乱，罗列一堆工具，看了以后多半没有什么印象。而 step by step 的例子做完一遍之后，也多半不知其所以然。最好的入门材料还是 Autoconf 和 Automake 的官方文档： http://www.gnu.org/software/autoconf/ http://www.gnu.org/software/automake/可惜看初学者一上来就要面对几百页的英文文档，又有点下不了手。本文的目的就是使得初学者可以快速入门，马上能在自己的项目中使用 Autotools 工具。然后可以在以后的使用过程中，通过查询官方文档来解决实际遇到的问题，在实践中深入全面掌握其高阶用法。这个入门分两部分，第一部分介绍基础概念；第二部分展现了一个完整的例子，可以参照这个例子把已有项目改成用 Autotools 工具来 build。GNU Autotools 一般指的是3个 GNU 工具包：Autoconf，Automake 和 Libtool (本文先介绍前两个工具，Libtool留到今后介绍)它们能解决什么问题，要先从 GNU 开源软件的 Build 系统说起。一般来说。GNU 软件的安装过程都是： 解压源代码包 ./configure make make install这个过程中， 需要有一个 configure 脚本，同时也需要一个 Makefile 文件。而 Autoconf 和 Automake 就是一套自动生成 configure 脚本和 Makefile 文件的工具。一. Autoconf 解决什么问题最早的时候，程序员完成源代码开发以后，发布代码包时，一般会附带相应的 Makefile 文件。然后就可以 make &amp;amp;&amp;amp; make install 来编译工程。当时并不需要这个运行 configure 的步骤。但是如果一个程序被广泛使用以后 (特别是成功的开源软件)，可能需要被安装到不同的平台上使用。这个时候，在不同的平台做 build 的时，一方面可能需要对 Makefile 文件进行调整 (最常见的例子就是：编译器的名字在不同的平台可能不同)。另外一方面，可能需要用一个替代函数来替换当前平台所不支持的函数 (例如：有的平台上不支持strdup这个调用)，需要在程序里面给每个平台写#define。为了避免手工做这些调整，人们开始写 configure 脚本来自动做这些调整工作 (现在在 make 之前先运行 configure 是 GNU Code Style 标准所规定的)。configure 脚本一般会先检查目前的环境，然后生成一个config.h 文件 (里面带了各种各样的#define) ，同时会生成一个 针对当前平台的 Makefile 文件，之后，make 命令就会使用到这个 Makefile文件。另外，GNU的 build 系统还有一些”乱七八糟”的功能，用户在使用 configure 这个脚本的时候，可能会使用到这些功能 (最常见的就是用 –prefix 来指定安装路径，用configure –help来查看说明等等)。但是，过了一段时间以后，人们发现靠人手工写这个 configure 脚本工作量太巨大了，而且以后维护这个 configure也比较麻烦：一旦发现项目在某个平台的移植性有问题，就需要更新这个 configure 脚本，比较繁琐。于是，人们就开发了 Autoconf 这个工具集来自动生成 configure 脚本。总之，相对于手工维护 configure 脚本，用 Autoconf 工具来生成 configure 有以下好处： 不需要手工写configure来支持 GNU Build System 规定的一些必须的功能。例如：在运行configure的时候对目录参数进行设置 (最常见的就是 –prefix=…)。或者可以设置 build 选项, 例如, 设置 CFLAGS 什么的 用 Autoconf 自动生成的 configure 能完美的支持各种不同的平台上 (手工写几乎是不可能的)。当 GNU Coding Style 更新的时候, Autoconf 会统一做相应的更新, 不需要所有的项目都自己去手工调整 configure 脚本。二. Automake 解决什么问题上面介绍了 Autoconf 的功能，接下来介绍 Automake。有了 Autoconf 工具以后，人们能自动生成 configure 脚本，configure 脚本一般会做以下工作： 检查用户的环境是否满足 GNU 程序的 Build 环境 (Autoconf 提供了很多macro来做各种各样的检查) 替换模版文件，生成最后 Build 工程所需要的 Makefile，config.h 等文件 (其中 Makefile 的模版文件是 Makefile.in, 而 config.h 的模版文件是 config.h.in)这里要特别说明的是 configure 是通过替换模版文件中的一些平台相关变量和 Build 选项来生成最后的 Makefile 文件 (和当前平台相关的 Makefile 文件)。虽然 configure 文件是 Autoconf 自动生成的，但是模版文件 Makefile.in 还是要程序员自己写。下面是一个简单的 Makefile.in 的片段 (基本上就是普通 Makefile 文件的样子，除了 configure 脚本会替换掉那些用@包含的变量)：# Package-specific substitution variablespackage = @PACKAGE_NAME@version = @PACKAGE_VERSION@tarname = @PACKAGE_TARNAME@distdir = $(tarname)-$(version)# Prefix-specific substitution variablesprefix = @prefix@exec_prefix = @exec_prefix@bindir = @bindir@# Tool-specific substitution variablesCFLAGS ?= -g -O2# VPATH-specific substitution variablessrcdir = @srcdir@VPATH = @srcdir@all: jupiterhello: main.c $(CC) $(CFLAGS) $(CPPFLAGS) -I. -I$(srcdir) -I.. -o $@ $(srcdir)/main.c而写这个 Makefile.in 估计每个程序员都会一开始自己写一个，之后做其他项目的时候，就在已有 Makefile.in 的基础上改一下直接用了。这种做法有一些弊端，一方面都是手工操作，自动化不够；另外一方面在 copy 修改过程中可能会出现一些问题。而 Automake 的目的就是可以帮助程序员自动生成模版文件 Makefile.in。而程序员只需要提供一个简单的 Makefile.am 文件来描述依赖关系就行了。例如, 只要在 Makefile.am 中写两行：# 目标bin_PROGRAMS = hello# 所依赖的源文件hello_SOURCES = main.c然后 Automake 就可以根据 Makefile.am 的描述自动生成 Makefile.in 模版文件。三. 例子提供一个简单利用 Autotools 来 build 项目的例子。这个例子重点展示的是怎么把一个目录结构有点怪的已有项目改成用 Autotools 工具来 build 系统。先说明一下，本文所使用的各个工具的版本 (因为 Autotools 工具集经常更新，老的版本可能和新版本有细微的差异) autoconf 2.67 版 automake 1.11.1 版然后介绍一下项目 (假设项目名称为 earth) 的目录结构： earth / \\ src test / \\fun.h fun.c funtest \\ funtest1.c funtest2.c项目根目录下有两个子目录 src 和 test。其中 src 目录放源代码，而 test 目录下面放测试代码；而测试代码又被分成很多组，每组是一个测试目录。例如, test 目录下的 funtest 目录是一个测试组 (这个例子中只有一组测试)，用来对 fun.h/fun.c 进行测试，其中包含了两个测试用例，两个测试程序都可以链接出可执行文件 (funtest1.c 包含了一个 main 函数，funtest2.c 也包含了一个 main 函数)。使用 Autotools 就是要利用 Autoconf 来生成 configure 脚本，同时利用 Automake 来生成 Makefile.in 和 config.h (因为这个例子程序比较简单, 没有用到什么特殊的调用, config.h 中也没有什么特殊的定义)所以首先要提供这两个工具所需的输入文件： configure.ac：Autoconf 所需 (就是一组宏定义，Autoconf 使用这些宏进行平台相关的检查, 并生成 configure 脚本) Makefile.am： Automake 所需 (特别注意，Automake 不单独使用，会和 Autoconf 配合使用。因此 configure.ac 里面也有 Automake 相关的宏定义)1. configure.ac一般来说，尽量直接使用 Autoconf 工具提供的 macro 来做各种各样的检查，最好不要自己写 macro 来做这些检查。因为Autoconf 提供的这些 macro 已经经过千锤百炼，适用不同的平台不同的系统，自己写多半不能做到像它这样全面。earth 项目的 configure.ac：AC_INIT([earth], [1.0], [bug-report@earth.com])AM_INIT_AUTOMAKE([foreign -Werror])AC_PROG_CCAC_CONFIG_SRCDIR([test/funtest/funtest1.c])AC_CONFIG_HEADERS([config.h])AC_CONFIG_FILES([Makefile test/Makefile test/funtest/Makefile])AC_OUTPUT这里只特别说明一下 AC_CONFIG_FILES 这个宏的作用 (其他宏是必须的, 但是含义比较简单, 先直接照葫芦画瓢就行了, 具体细节可以查询 Autoconf 的手册)AC_CONFIG_FILES 指明了需要根据模版生成的 Makefile 文件. 在这个例子中, 需要生成 3 个 Makefile 文件, 每个 Mafile 文件都需要一个模版: 根目录下需要有模版文件 Makefile.in test 目录下需要有模版文件 Makefile.in test/funtest 目录下需要有模版文件 Makefile.in而这3个 Makefile.in 需要用 Automake 通过各自的 Makefile.am 来生成。特别注意，这个例子比较特殊，不需要在 src 目录下面进行 build， 因为这个例子只需要在测试用例目录下面生成测试程序的可执行文件即可。2. Makefile.am在介绍如何编写 Makefile.am 之前，先说明一下 earth 项目为什么需要 3 个 Makefile 文件。一般来说，都是在项目根目录下面运行 make &amp;amp;&amp;amp; make install 的。而具体到 earth 这个项目，希望运行 make 以后，会在 test/funtest 下生成测试程序，因此需要make递归运行：先在根目录运行make，再在 test 目录下运行 make，最后才在test/funtest 下运行 make，生成两个测试程序的可执行文件。可以通过配置 Makefile.am 来控制这个递归过程。其中：根目录下的 Makefile.am (SUBDIRS 指明需要在哪些子目录递归生成模版文件 Makefile.in) 只需要一行：SUBDIRS=testtest目录下的 Makefile.am 只需要一行，指明在子目录 funtest 下执行 make：SUBDIRS = funtesttest/funtest 目录下的 Makefile.am（test/funtest 目录下不需要再往下递归，所以不用设置SUBDIRS）。由于需要描述出依赖关系，这个 Makefile.am 会比较复杂一点，有5行：bin_PROGRAMS = funtest1 funtest2funtest1_SOURCES = funtest1.c ../../src/fun.h ../../src/fun.cfuntest1_CPPFLAGS = -I$(top_srcdir)/srcfuntest2_SOURCES = funtest2.c ../../src/fun.h ../../src/fun.cfuntest2_CPPFLAGS = -I$(top_srcdir)/src其中： bin_PROGRAMS 指明要生成那些可执行文件 funtest1_SOURCES 指明 funtest1 所依赖的源文件 (特别注意, 利用相对路径指明了所依赖的src目录下的源文件) funtest1_CPPFLAGS = -I$(top_srcdir)/src 指明了编译时需要指定的头文件路径3. 利用 Autotools 生成configure脚本和Makefile.in模版在configure.ac 和 Makefile.am 做好以后。在项目根目录运行：autoreconf --install就可以生成 configure 脚本和所有的模版文件 Makefile.in然后再运行./configure &amp;amp;&amp;amp; make就可以得到测试程序的可执行文件。这里特别说明的是，Autoconf 和 Automake 是两个工具包。其中包含了多个工具软件，而这些工具软件要互相配合才能工作。而这些工具软件间的关系比较复杂，单独运行各个工具软件也可以得到最后的 configure 脚本和模版文件 Makefile.in。不过使用 autoreconf 这个命令可以自动安排各个工具软件的执行顺，所以一般推荐直接运行 autoreconf，而不是单独运行各个单独的工具软件。参考文档 autoconf 官方手册 automake 官方手册 比较流行的一个入门文档 (这个人做过一段时间的维护工作) 最新的一本关于 Autotools 的书: Autotools: A Practioner’s Guide to GNU Autoconf, Automake, and Libtool" } ]
